{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Exploration\n",
    "### Adapted concepts from [HW1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw1/hw1.ipynb) and [HW5 Part1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw5/hw5part1.ipynb)\n",
    "\n",
    "**This notebook should be locally run by issuing `vagrant up` from project root, then locating the notebook at \"http:\\\\localhost:4545\". You may also need to issue `vagrant provision` to update any required resources.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Provided Data Into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the provided lyrics\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/provided/all billboard top 100 songs from 1970-2014.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cull excess columns swept up on read\n",
    "lyrics_pd_df = lyrics_pd_df[['position','year','title.href','title','artist','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary feeling small When tears are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Woman_(...</td>\n",
       "      <td>American Woman</td>\n",
       "      <td>The Guess Who</td>\n",
       "      <td>American woman, stay away from me American wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Raindrops_Keep_F...</td>\n",
       "      <td>Raindrops Keep Fallin' on My Head</td>\n",
       "      <td>B.J. Thomas</td>\n",
       "      <td>Raindrops keep falling on my head Just like th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/War_(Edwin_Starr...</td>\n",
       "      <td>War</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>War huh Yeah! Absolutely uh-huh, uh-huh huh Ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position  year                                         title.href                              title               artist                                             lyrics\n",
       "0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...         Bridge over Troubled Water  Simon and Garfunkel  When you're weary feeling small When tears are...\n",
       "1         2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...     (They Long to Be) Close to You       The Carpenters                                                  x\n",
       "2         3  1970  https://en.wikipedia.org/wiki/American_Woman_(...                     American Woman        The Guess Who  American woman, stay away from me American wom...\n",
       "3         4  1970  https://en.wikipedia.org/wiki/Raindrops_Keep_F...  Raindrops Keep Fallin' on My Head          B.J. Thomas  Raindrops keep falling on my head Just like th...\n",
       "4         5  1970  https://en.wikipedia.org/wiki/War_(Edwin_Starr...                                War          Edwin Starr  War huh Yeah! Absolutely uh-huh, uh-huh huh Ye..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add `decade` column to df\n",
    "lyrics_pd_df['decade'] = lyrics_pd_df.year.apply(lambda y : y - y%10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a `song_key` column by joining `year` and `position` for better identity \n",
    "# adapted from:\n",
    "# http://stackoverflow.com/questions/29983946/concatenate-cells-into-a-string-with-separator-pandas-python\n",
    "lyrics_pd_df['song_key'] = lyrics_pd_df[['year','position']].apply(lambda row: '-'.join(row.astype(str).values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>66</td>\n",
       "      <td>1991</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cream_(song)</td>\n",
       "      <td>Cream</td>\n",
       "      <td>Prince</td>\n",
       "      <td>This is it Its time for you to go to the wire ...</td>\n",
       "      <td>1990</td>\n",
       "      <td>1991-66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>26</td>\n",
       "      <td>1990</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Power_(Snap!...</td>\n",
       "      <td>The Power</td>\n",
       "      <td>Snap!</td>\n",
       "      <td>x</td>\n",
       "      <td>1990</td>\n",
       "      <td>1990-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>59</td>\n",
       "      <td>2003</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Underneath_It_All</td>\n",
       "      <td>Underneath It All</td>\n",
       "      <td>No Doubt</td>\n",
       "      <td>There's times where I want something more Some...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2003-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>61</td>\n",
       "      <td>1972</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Day_Dreaming_(Ar...</td>\n",
       "      <td>Day Dreaming</td>\n",
       "      <td>Aretha Franklin</td>\n",
       "      <td>He's the kind of guy that would say 'Hey baby ...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1972-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>41</td>\n",
       "      <td>2006</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I%27m_%27n_Luv_(...</td>\n",
       "      <td>I'm 'n Luv (Wit a Stripper)</td>\n",
       "      <td>T-Pain</td>\n",
       "      <td>Goddamn Lil Mama You know you thick as hell yo...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2006-41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  year                                         title.href                        title           artist                                             lyrics  decade song_key\n",
       "2165        66  1991         https://en.wikipedia.org/wiki/Cream_(song)                        Cream           Prince  This is it Its time for you to go to the wire ...    1990  1991-66\n",
       "2025        26  1990  https://en.wikipedia.org/wiki/The_Power_(Snap!...                    The Power            Snap!                                                  x    1990  1990-26\n",
       "3358        59  2003    https://en.wikipedia.org/wiki/Underneath_It_All            Underneath It All         No Doubt  There's times where I want something more Some...    2000  2003-59\n",
       "260         61  1972  https://en.wikipedia.org/wiki/Day_Dreaming_(Ar...                 Day Dreaming  Aretha Franklin  He's the kind of guy that would say 'Hey baby ...    1970  1972-61\n",
       "3640        41  2006  https://en.wikipedia.org/wiki/I%27m_%27n_Luv_(...  I'm 'n Luv (Wit a Stripper)           T-Pain  Goddamn Lil Mama You know you thick as hell yo...    2000  2006-41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyrics_pd_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Which Lyrics Are Missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics == 'x' shape -->  (863, 8)\n",
      "check lyrics len < 10 -->  (863, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I%27ll_Be_There_...</td>\n",
       "      <td>I'll Be There</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Let_It_Be_(song)</td>\n",
       "      <td>Let It Be</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/ABC_(song)</td>\n",
       "      <td>ABC</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Love_You_Save</td>\n",
       "      <td>The Love You Save</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    position  year                                         title.href                           title          artist lyrics  decade song_key\n",
       "1          2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...  (They Long to Be) Close to You  The Carpenters      x    1970   1970-2\n",
       "6          7  1970  https://en.wikipedia.org/wiki/I%27ll_Be_There_...                   I'll Be There   The Jackson 5      x    1970   1970-7\n",
       "8          9  1970     https://en.wikipedia.org/wiki/Let_It_Be_(song)                       Let It Be     The Beatles      x    1970   1970-9\n",
       "14        15  1970           https://en.wikipedia.org/wiki/ABC_(song)                             ABC   The Jackson 5      x    1970  1970-15\n",
       "15        16  1970    https://en.wikipedia.org/wiki/The_Love_You_Save               The Love You Save   The Jackson 5      x    1970  1970-16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing lyrics have a value of 'x', these will need to be back-filled in the Harvest section\n",
    "missing_lyricsdf = lyrics_pd_df[lyrics_pd_df['lyrics'] == 'x']\n",
    "print \"lyrics == 'x' shape --> \", missing_lyricsdf.shape\n",
    "print \"check lyrics len < 10 --> \", lyrics_pd_df[lyrics_pd_df['lyrics'].str.len() <= 10].shape\n",
    "missing_lyricsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join Provided Artist Info with Song \n",
    "* **TODO**: Use artist info provided to have a joined dataframe, reference HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Harvest Missing Data\n",
    "* **TODO**: consider missing lyrics from `missing_lyrics` \n",
    "* **TODO**: consider additional years, prior to 1970 and 2015 \n",
    "* **TODO**: harvest artist info for additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join Harvested Artist Info with Song \n",
    "* **TODO**: Use artist info harvested to have a joined dataframe, reference HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Build Decade Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000, 4499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1970: [0, 999],\n",
       " 1980: [1000, 1999],\n",
       " 1990: [2000, 2999],\n",
       " 2000: [3000, 3999],\n",
       " 2010: [4000, 4499]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary holding indexes for decades, useful for filtering in corpus, vocabs, etc.\n",
    "def buildDecadeDict(df=lyrics_pd_df):\n",
    "    \"\"\"\n",
    "    assumes df is sorted by year.\n",
    "    Note: saved value as list to be consistent with json persistence.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    dyear = None\n",
    "    didx = 0\n",
    "    idx = -1\n",
    "    for row in lyrics_pd_df.iterrows():\n",
    "        idx +=1    \n",
    "        year = row[1].year\n",
    "        # initial conditions\n",
    "        if not dyear:\n",
    "            dyear = year\n",
    "        # year change    \n",
    "        elif year - year%10 != dyear:\n",
    "            # add the last year to the dictionary\n",
    "            d[dyear] = [didx,idx-1]           \n",
    "            dyear = year\n",
    "            didx = idx\n",
    "    # handle the last entry in the loop\n",
    "    d[dyear] = [didx,idx]\n",
    "    print d[dyear]\n",
    "    \n",
    "    return d\n",
    "decade_dict = buildDecadeDict()\n",
    "decade_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Pandas Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save lyrics_pd_df\n",
    "lyrics_pd_df.to_csv(\"../../data/conditioned/lyrics_pd_df.csv\",index=False) #note: ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save decade dict\n",
    "with open('../../data/conditioned/decade-dict.json', 'w') as fp:\n",
    "    json.dump(decade_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save missing lyrics df\n",
    "missing_lyricsdf.to_csv(\"../../data/conditioned/missing_lyricsdf.csv\",index=False) #note: ascii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Manipulate With Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|  1970|  1970-2|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|  1970|  1970-3|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer need the pandas version, clear it out\n",
    "del lyrics_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|  1970|  1970-2|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|  1970|  1970-3|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many songs do we have? 4500\n"
     ]
    }
   ],
   "source": [
    "#We cache the data to make sure it is only read once from disk\n",
    "lyricsdf.cache()\n",
    "print \"How many songs do we have?\", lyricsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the schema? root\n",
      " |-- position: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- title.href: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- decade: long (nullable = true)\n",
      " |-- song_key: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"What is the schema?\", lyricsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sample Lyrics (or Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial sampling to take from each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether or not to sample lyrics, and how many to sample per year\n",
    "sample_lyrics = False\n",
    "PER_YEAR_SAMPLES=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "def randomSubSampleLyrics(sparkdf,take=PER_YEAR_SAMPLES):    \n",
    "    # generate spark pairs as a tuple\n",
    "    br_pairs = sparkdf.map(lambda r: (r.year, r.song_key))\n",
    "    \n",
    "    # group by key for a list of reviews per business and collect\n",
    "    br_grouped = br_pairs.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "        \n",
    "    #sample after collect\n",
    "    br_sample = [np.random.choice(v, size=take, replace=False) for k,v in br_grouped]    \n",
    "    \n",
    "    #flatten into a list\n",
    "    return list(itertools.chain.from_iterable(br_sample))\n",
    "    \n",
    "small_song_keys = randomSubSampleLyrics(lyricsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\n"
     ]
    }
   ],
   "source": [
    "if sample_lyrics:\n",
    "    print \"How many small_song_keys? \", len(small_song_keys)\n",
    "    small_song_keys[:5]\n",
    "else:\n",
    "    print \"No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Sat, 21 Nov 2015 23:11:29\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 µs, sys: 4 µs, total: 19 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#(your code here)\n",
    "if sample_lyrics:\n",
    "    ldf=lyricsdf[lyricsdf.song_key.isin(small_song_keys)]#creates new dataframe\n",
    "else:\n",
    "    ldf=lyricsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[position: bigint, year: bigint, title.href: string, title: string, artist: string, lyrics: string, decade: bigint, song_key: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "ldf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lyrics are in ldf?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many lyrics are in ldf? \", ldf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Test of parse...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The/DT/B-NP/O/the world/NN/I-NP/O/world is/VBZ/B-VP/O/be the/DT/B-NP/O/the craziest/JJ/I-NP/O/craziest place/NN/I-NP/O/place ././O/O/.\\nI/PRP/B-NP/O/i am/VBP/B-VP/O/be working/VBG/I-VP/O/work hard/RB/B-ADVP/O/hard ././O/O/.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick Test of parse...\"\n",
    "parse(\"The world is the craziest place. I am working hard.\", tokenize=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of get_parts ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[u'patio', u'job'], [u'lunch', u'egg']], [[u'perfect'], [u'good', u'great']])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick check of get_parts ...\"\n",
    "get_parts(\"Have had many other items and just love the food. The patio...job was and...perfect. Lunch is good, and the only egg is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run Get Parts on Provided Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "lyric_parts = ldf.map(lambda r : get_parts(r.lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'feeling',\n",
       "    u'tear',\n",
       "    u'eye',\n",
       "    u'time',\n",
       "    u'friend',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'street',\n",
       "    u'evening',\n",
       "    u'comfort',\n",
       "    u'darkness',\n",
       "    u'pain',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'time',\n",
       "    u'dream',\n",
       "    u'way',\n",
       "    u'friend',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'mind',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'mind']],\n",
       "  [[u'weary',\n",
       "    u'small',\n",
       "    u'rough',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled']]),\n",
       " ([], [])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyric_parts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Sat, 21 Nov 2015 23:11:31\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 18 ms, total: 138 ms\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parseout=lyric_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vocab\n",
    "###Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many parseout entries?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many parseout entries? \", len(parseout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten parseout to create initial noun rdd\n",
    "nounrdd=sc.parallelize([ele[0] for ele in parseout]).flatMap(lambda l: l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'feeling',\n",
       "  u'tear',\n",
       "  u'eye',\n",
       "  u'time',\n",
       "  u'friend',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'street',\n",
       "  u'evening',\n",
       "  u'comfort',\n",
       "  u'darkness',\n",
       "  u'pain',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'time',\n",
       "  u'dream',\n",
       "  u'way',\n",
       "  u'friend',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'mind',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'mind']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounrdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "nwordsrdd = (nounrdd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 3),\n",
       " (u'sleet', 6),\n",
       " (u'sleep', 77),\n",
       " (u'mansion', 10),\n",
       " (u'integrity', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nwordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'love', 4969),\n",
       " (u'baby', 3852),\n",
       " (u'time', 3801),\n",
       " (u'way', 2994),\n",
       " (u'girl', 2825),\n",
       " (u'night', 2406),\n",
       " (u'heart', 2006),\n",
       " (u'thing', 1996),\n",
       " (u'day', 1947),\n",
       " (u'life', 1806)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "nwordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[41] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect all the words and cache\n",
    "nounvocabtups = (nwordsrdd\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 0), (u'sleet', 1), (u'sleep', 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[44] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "nounvocab=nounvocabtups.collectAsMap()\n",
    "nounid2word=nounvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'jockin', u'woody', 2302)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `nounvocab['dance']`\n",
    "nounid2word[0], nounvocab.keys()[5], nounvocab[nounvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the noun vocabulary?  8757\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the noun vocabulary? \", len(nounvocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create initial adj rdd from parseout\n",
    "adjrdd=sc.parallelize([ele[1] for ele in parseout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'weary',\n",
       "   u'small',\n",
       "   u'rough',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled']],\n",
       " [],\n",
       " [[u'american',\n",
       "   u'american',\n",
       "   u'hanging',\n",
       "   u'important',\n",
       "   u'old',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'warm',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'good',\n",
       "   u'good',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american']]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[46] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "awordsrdd = (adjrdd\n",
    "             .flatMap(lambda l: l)\n",
    "             .flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ooh-oh', 1),\n",
       " (u'good-night', 1),\n",
       " (u'suicidal', 2),\n",
       " (u'b-b-b-be', 1),\n",
       " (u'crucial', 5)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "awordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'good', 1934),\n",
       " (u'little', 1560),\n",
       " (u'bad', 858),\n",
       " (u'real', 842),\n",
       " (u'true', 705),\n",
       " (u'wrong', 703),\n",
       " (u'right', 661),\n",
       " (u'long', 632),\n",
       " (u'crazy', 626),\n",
       " (u'new', 620)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "awordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "awordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "adjvocabtups = (awordsrdd\n",
    "              .map(lambda (x,y): x)\n",
    "              .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ooh-oh', 0), (u'good-night', 1), (u'suicidal', 2)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[57] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "adjvocab=adjvocabtups.collectAsMap()\n",
    "adjid2word=adjvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'ooh-oh', u'dynamic', 32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `adjvocab['exotic']`\n",
    "adjid2word[0], adjvocab.keys()[5], adjvocab[adjvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the adjective vocabulary?  3486\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the adjective vocabulary? \", len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# CITATION - Use of counter for reduce within each word list from:\n",
    "# http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "##################################################################################################\n",
    "from collections import Counter\n",
    "\n",
    "# for each sentence, reduct into a list of tuple k,v where k=vocab index and v=count, \n",
    "# each word list is sorted by occurence\n",
    "documents = nounrdd.map(lambda words: Counter([nounvocab[word] for word in words]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(610, 6),\n",
       "  (3123, 6),\n",
       "  (8417, 2),\n",
       "  (5210, 2),\n",
       "  (232, 2),\n",
       "  (5211, 1),\n",
       "  (580, 1),\n",
       "  (840, 1),\n",
       "  (828, 1),\n",
       "  (8241, 1),\n",
       "  (6868, 1),\n",
       "  (1206, 1),\n",
       "  (7987, 1),\n",
       "  (7003, 1),\n",
       "  (3420, 1)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify output\n",
    "documents.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather spark results\n",
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Spark Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save lyricsdf / ldf\n",
    "ldf.toPandas().to_csv(\"../../data/conditioned/sample_or_not_lyricsdf.csv\",index=False,encoding='utf-8') #note: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun n-gram\n",
    "with open('../../data/conditioned/noun-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(nwordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adjective n-gram\n",
    "with open('../../data/conditioned/adj-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(awordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun vocab and id2word\n",
    "with open('../../data/conditioned/nounvocab.json', 'w') as fp:\n",
    "    json.dump(nounvocab, fp)\n",
    "    \n",
    "with open('../../data/conditioned/nounid2word.json', 'w') as fp:\n",
    "    json.dump(nounid2word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adj vocab and id2word\n",
    "with open('../../data/conditioned/adjvocab.json', 'w') as fp:\n",
    "    json.dump(adjvocab, fp)\n",
    "    \n",
    "with open('../../data/conditioned/adjid2word.json', 'w') as fp:\n",
    "    json.dump(adjid2word, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save corpus\n",
    "pickle.dump( corpus, open( \"../../data/conditioned/corpus.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Synonym Lookups\n",
    "Focus on WordNet python package within [nltk](http://www.nltk.org) via [textblob](https://textblob.readthedocs.org/en/dev/)\n",
    "The main idea is to lookup all words in the noun and adj vocab dictionaries and attempt to collapse down -- where possible -- to synonyms. The synonyms can be used for common_support also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.wordnet import Synset\n",
    "from textblob.wordnet import NOUN\n",
    "from textblob.wordnet import ADJ\n",
    "\n",
    "SIM_THRESHOLD = 1.0 # Only act on values at/above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## COMMON METHODS FOR SYNSETS\n",
    "def synsetStr(syn):\n",
    "    \"\"\"\n",
    "    attempt to parse the string from a Synset, e.g. Synset('dog.n.01') would return 'dog'\n",
    "    return String or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return syn.name().split('.')[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def flattenSynsetValues(syn_dict, skip_invalid=True, replace_invalid=None):\n",
    "    \"\"\"\n",
    "    flatten synset values in dictionary using params\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for k,v in syn_dict.iteritems():\n",
    "        if v:\n",
    "            d[k] = synsetStr(v)\n",
    "        elif not skip_invalid:\n",
    "            d[k] = replace_invalid\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CORE FUNCTIONS FOR BUILDING SIMILARITY MATRIX\n",
    "\n",
    "def posToSingle(pos):\n",
    "    \"\"\"\n",
    "    Keep up with which pos values are implemented.\n",
    "    \"\"\"\n",
    "    if pos == NOUN:\n",
    "        return \"n\"\n",
    "    elif pos == ADJ:\n",
    "        return \"a\"\n",
    "    return None # essentially, else clause\n",
    "\n",
    "\n",
    "def cachedSynsetOrBuild(idx, syns, p, id_lookup):\n",
    "    \"\"\"\n",
    "    Build Synset for given `idx`, using the `id_lookup`.\n",
    "    Facilitate O(n) computational complexity by caching results.\n",
    "    \n",
    "    --- Input ---\n",
    "    idx: id to build and cache\n",
    "    syns: existing dictionary of synsets, with k: id, v: Synset or None\n",
    "    p: String pos value in the form needed for Synset generation, see `posToSingle`\n",
    "    id_lookup: dictionary for noun / adj to build n x n matrix of similarity.\n",
    "    \n",
    "    --- Return ---\n",
    "    Synset or None\n",
    "    \"\"\"\n",
    "    if idx in syns:\n",
    "        return syns[idx] \n",
    "        \n",
    "    # focus on `.01` only\n",
    "    try:                      \n",
    "        syn = Synset(\"{}.{}.01\".format(id_lookup[idx],p))\n",
    "        syns[idx] = syn\n",
    "        return syn\n",
    "    except Exception:\n",
    "        syns[idx] = None\n",
    "        return None\n",
    "\n",
    "def similarityMatrix(id2word, pos, take_n=None):\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    Build matrix of synsets for given id2word dictionary.    \n",
    "    Optionally, only build a similarity matrix for the first n values.\n",
    "    \n",
    "    --- Input ---    \n",
    "    id2word: dictionary for noun / adj to build n x n matrix of similarity.\n",
    "    pos: WordNet position, `NOUN` or `ADJ` imported based on needs\n",
    "    take_n: whether take the first n values for testing, default=None\n",
    "    \n",
    "    --- Return ---\n",
    "    return a tuple, t where\n",
    "    t[0]: n x n matrix with raw similarity score or zero\n",
    "    t[1]: dictionary of synsets with k: id, v: Synset or None\n",
    "    ##############################################################    \n",
    "    \"\"\"    \n",
    "    syns = {} # obtain O(n)\n",
    "    p = posToSingle(pos)\n",
    "    \n",
    "    # determine n\n",
    "    n = len(id2word)\n",
    "    if take_n:\n",
    "        n = take_n\n",
    "    \n",
    "    # n x n matrix, initialized with zeros \n",
    "    matrix = np.zeros((n,n))\n",
    "    \n",
    "    # populate\n",
    "    ns = range(n)\n",
    "    for i in ns:  \n",
    "        isyn = cachedSynsetOrBuild(i,syns,p,id2word)       \n",
    "        for j in ns:\n",
    "            # find j in synset\n",
    "            jsyn = None\n",
    "            if isyn:\n",
    "                jsyn = cachedSynsetOrBuild(j,syns,p,id2word) # no reason unless isyn is ok\n",
    "        \n",
    "            # update matrix with path_similarity between i and j words\n",
    "            if isyn and jsyn:            \n",
    "                ps = isyn.path_similarity(jsyn)            \n",
    "                if ps:\n",
    "                    matrix[i][j] = ps\n",
    "            \n",
    "    return matrix, syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS FOR EVALUATING SIMILARITY MATRIX RESULTS\n",
    "\n",
    "def printSimilarityPairs(matrix, show_n=None, id_lookup=None, sim_threshold=SIM_THRESHOLD): \n",
    "    \"\"\"\n",
    "    print non zero similarities, ignoring diagonals.\n",
    "    Optionally, show only first n non zeros then return.\n",
    "    Optionally, lookup ids with words.\n",
    "    Optionally, only evaluate values at/above a threshold.\n",
    "    \"\"\"\n",
    "    ns = range(len(matrix))      \n",
    "    c = 0\n",
    "    for i in ns:\n",
    "        for j in ns:\n",
    "            v = matrix[i][j] \n",
    "            \n",
    "            # handle sim_threshold\n",
    "            met_threshold = True\n",
    "            if sim_threshold and v < sim_threshold:\n",
    "                met_threshold = False\n",
    "            elif not v:\n",
    "                met_threshold = False\n",
    "                    \n",
    "            if (i != j) and met_threshold:                \n",
    "                if not show_n or c < show_n:\n",
    "                    c += 1\n",
    "                    s_i = i\n",
    "                    s_j = j\n",
    "                    if id_lookup:\n",
    "                        s_i = id_lookup[i]\n",
    "                        s_j = id_lookup[j]\n",
    "                    print \"{},{} --> {}\".format(s_i,s_j,v)\n",
    "                elif show_n:\n",
    "                    return\n",
    "                \n",
    "def countSimilarityPairs(matrix, sim_threshold=SIM_THRESHOLD):\n",
    "    \"\"\"\n",
    "    count non zero similarities, ignoring diagonals.\n",
    "    Optionally, only evaluate values at/above a threshold.    \n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    ns = range(len(matrix))         \n",
    "    for i in ns:\n",
    "        for j in ns:\n",
    "            v = matrix[i][j]\n",
    "            \n",
    "            # handle sim_threshold\n",
    "            met_threshold = True\n",
    "            if sim_threshold and v < sim_threshold:\n",
    "                met_threshold = False\n",
    "            elif not v:\n",
    "                met_threshold = False\n",
    "            \n",
    "            if (i != j) and met_threshold:                \n",
    "                c += 1                    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Sat, 21 Nov 2015 23:12:31\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 43.2 ms, total: 25.9 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build adj similarity matrix\n",
    "asimatrix, asyns = similarityMatrix(adjid2word, ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count non-zero similarities for adjectivies at/above SIM_THRESHOLD, ignoring diagonal\n",
    "countSimilarityPairs(asimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephemeral,passing --> 1.0\n",
      "yummy,scrumptious --> 1.0\n",
      "small,little --> 1.0\n",
      "handicapped,disabled --> 1.0\n",
      "blue,bluish --> 1.0\n",
      "slippery,slippy --> 1.0\n",
      "ferocious,furious --> 1.0\n",
      "ferocious,fierce --> 1.0\n",
      "eternal,ageless --> 1.0\n",
      "eternal,everlasting --> 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check adj similarity results, are they any good?\n",
    "printSimilarityPairs(asimatrix, show_n=10, id_lookup=adjid2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Sat, 21 Nov 2015 23:13:02\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 20s, sys: 14.8 s, total: 34min 35s\n",
      "Wall time: 34min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build noun similarity matrix (can take 30+ minutes!!!)\n",
    "nsimatrix, nsyns = similarityMatrix(nounid2word, NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count non-zero similarities for nouns at/above SIM_THRESHOLD, ignoring diagonal\n",
    "countSimilarityPairs(nsimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep,slumber --> 1.0\n",
      "integrity,unity --> 1.0\n",
      "yack,chatter --> 1.0\n",
      "yack,yak --> 1.0\n",
      "hide,fell --> 1.0\n",
      "hate,hatred --> 1.0\n",
      "swaggie,swagger --> 1.0\n",
      "tornado,twister --> 1.0\n",
      "dorm,dormitory --> 1.0\n",
      "limousine,limo --> 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check noun similarity results, are they any good?\n",
    "printSimilarityPairs(nsimatrix, show_n = 10, id_lookup=nounid2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Similarity Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save asimatrix\n",
    "pickle.dump( asimatrix, open( \"../../data/conditioned/asimatrix.p\", \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten and save asyns\n",
    "with open('../../data/conditioned/asyns.json', 'w') as fp:\n",
    "    json.dump(flattenSynsetValues(asyns), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save nsimatrix\n",
    "pickle.dump( nsimatrix, open( \"../../data/conditioned/nsimatrix.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten and save nsyns\n",
    "with open('../../data/conditioned/nsyns.json', 'w') as fp:\n",
    "    json.dump(flattenSynsetValues(nsyns), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Hypernyms\n",
    "find the lowest common [hypernym](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy) between similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('carnivore.n.01')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick Test\n",
    "Synset('dog.n.01').lowest_common_hypernyms(Synset('cat.n.01'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CORE FUNCTIONS FOR BUILDING HYPERNYM\n",
    "\n",
    "def makeOrderedTuple(idx1, idx2):\n",
    "    if idx1 > idx2:\n",
    "        return (idx2,idx1) \n",
    "    return (idx1,idx2) \n",
    "\n",
    "def cachedHypernymOrBuild(idx1, idx2, syn_lookup, hypes, hype_as_str=True):\n",
    "    \"\"\"\n",
    "    Build Hypernym for given `idxtuple`, using the `syns_lookup`.\n",
    "    Facilitate O(n) computational complexity by caching results\n",
    "    Will internally manage hypernym keys as ordered tuple.\n",
    "    \n",
    "    --- Input ---\n",
    "    idx: tuple of id to build and cache\n",
    "    syn_lookup: existing dictionary of synsets, with k: id, v: Synset or None    \n",
    "    hypes: dictionary for hypernyms with k: ordered tuple, v: hypernym.\n",
    "    hype_as_str: optional build map with string values, default = True\n",
    "    --- Return ---\n",
    "    a hypernym Synset or None\n",
    "    \"\"\"\n",
    "    ituple = makeOrderedTuple(idx1,idx2)    \n",
    "    if ituple in hypes: \n",
    "        return hypes[ituple] \n",
    "    \n",
    "    try:    \n",
    "        s1 = syn_lookup[ituple[0]]\n",
    "        s2 = syn_lookup[ituple[1]]\n",
    "        h = s1.lowest_common_hypernyms(s2)[0]\n",
    "        \n",
    "        if hype_as_str:\n",
    "            h = synsetStr(h)\n",
    "            \n",
    "        hypes[ituple] = h\n",
    "        return h\n",
    "    except Exception:\n",
    "        hypes[ituple] = None\n",
    "        return None\n",
    "\n",
    "def lowestCommonHypernyms(simatrix, syn_lookup, sim_threshold=SIM_THRESHOLD, hype_as_str=True):\n",
    "    \"\"\"\n",
    "    Build a matrix with hypernym where found.\n",
    "    Optionally, only evaluate values at/above a threshold.\n",
    "    \n",
    "    --- Input ---\n",
    "    simatrix: tuple of id to build and cache\n",
    "    syn_lookup: existing dictionary of synsets, with k: id, v: Synset or None    \n",
    "    sim_threshold: optional threshold to use for establishing hypernyms, default = SIM_THRESHOLD\n",
    "    hype_as_str: optional build map with string values, default = True\n",
    "    \n",
    "    --- Return ---\n",
    "    dictionary for hypernyms with k: ordered tuple, v: Synset.    \n",
    "    \"\"\"\n",
    "    \n",
    "    hypes = {} # dictionary to build up.\n",
    "    \n",
    "    n = len(simatrix)\n",
    "    ns = range(n)          \n",
    "    for i in ns:\n",
    "        for j in ns:\n",
    "            v = simatrix[i][j] \n",
    "            \n",
    "            # handle sim_threshold\n",
    "            met_threshold = True\n",
    "            if sim_threshold and v < sim_threshold:\n",
    "                met_threshold = False\n",
    "            elif not v:\n",
    "                met_threshold = False\n",
    "                    \n",
    "            if (i != j) and met_threshold:                                \n",
    "                cachedHypernymOrBuild(i,j, syn_lookup, hypes, hype_as_str)\n",
    "                \n",
    "    return hypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS FOR EVALUATING HYPERNYMS\n",
    "\n",
    "def countHypernyms(hypes, count_valid=True, count_invalid=True):\n",
    "    \"\"\"\n",
    "    Count  hypernyms, ignoring None\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    for k,v in hypes.iteritems():\n",
    "        if count_valid and v:\n",
    "            c += 1\n",
    "        elif count_invalid and not v:\n",
    "            c += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adjective Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find adj hypernyms, defaulting to only the string value\n",
    "ahypes = lowestCommonHypernyms(asimatrix, asyns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many adj hypernyms?  164\n",
      "how many valid adj hypernyms?  164\n",
      "how many invalid adj hypernyms?  0\n",
      "example key: (1116, 1618), value: nightlong\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"how many adj hypernyms? \", countHypernyms(ahypes)\n",
    "print \"how many valid adj hypernyms? \", countHypernyms(ahypes, count_valid=True, count_invalid=False)\n",
    "print \"how many invalid adj hypernyms? \", countHypernyms(ahypes, count_valid=False, count_invalid=True)\n",
    "print \"example key: {}, value: {}\".format(ahypes.keys()[0],ahypes[ahypes.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5, 2312): u'ephemeral',\n",
       " (18, 1199): u'delectable',\n",
       " (51, 1798): u'small',\n",
       " (60, 228): u'disabled',\n",
       " (68, 2896): u'blue',\n",
       " (82, 1698): u'slippery',\n",
       " (86, 219): u'ferocious',\n",
       " (86, 574): u'ferocious',\n",
       " (88, 1737): u'ageless',\n",
       " (88, 2727): u'ageless',\n",
       " (88, 3317): u'ageless',\n",
       " (133, 3227): u'cryptic',\n",
       " (185, 971): u'casual',\n",
       " (197, 468): u'all_right',\n",
       " (197, 777): u'all_right',\n",
       " (212, 2273): u'devilish',\n",
       " (219, 574): u'ferocious',\n",
       " (223, 1499): u'chopped',\n",
       " (225, 549): u'boggy',\n",
       " (225, 2335): u'boggy',\n",
       " (257, 1294): u'charming',\n",
       " (275, 1378): u'diffident',\n",
       " (293, 3226): u'dizzy',\n",
       " (303, 1196): u'hairy',\n",
       " (314, 405): u'seventh',\n",
       " (316, 2946): u'electrifying',\n",
       " (343, 2884): u'alone',\n",
       " (358, 1763): u'disgusting',\n",
       " (366, 2738): u'extreme',\n",
       " (391, 2135): u'cockamamie',\n",
       " (391, 2808): u'cockamamie',\n",
       " (450, 3041): u'bare',\n",
       " (465, 1201): u'besotted',\n",
       " (468, 777): u'all_right',\n",
       " (480, 1074): u'bang-up',\n",
       " (480, 2408): u'bang-up',\n",
       " (480, 2880): u'bang-up',\n",
       " (485, 2349): u'cheery',\n",
       " (493, 1227): u'awful',\n",
       " (500, 512): u'bare',\n",
       " (500, 2477): u'bare',\n",
       " (512, 2477): u'bare',\n",
       " (540, 1818): u'religious',\n",
       " (544, 565): u'spare',\n",
       " (549, 2335): u'boggy',\n",
       " (550, 1429): u'blasted',\n",
       " (582, 2075): u'enormous',\n",
       " (605, 1518): u'red',\n",
       " (605, 1555): u'red',\n",
       " (605, 1810): u'red',\n",
       " (626, 1482): u'crafty',\n",
       " (635, 1568): u'large',\n",
       " (650, 3054): u'brokenhearted',\n",
       " (694, 2472): u'color',\n",
       " (718, 1588): u'crisp',\n",
       " (721, 1432): u'bigheaded',\n",
       " (722, 2294): u'cardinal',\n",
       " (734, 3270): u'bally',\n",
       " (743, 2348): u'beloved',\n",
       " (772, 1406): u'ace',\n",
       " (772, 2997): u'ace',\n",
       " (781, 883): u'favorite',\n",
       " (895, 3338): u'ineluctable',\n",
       " (899, 1087): u'icky',\n",
       " (899, 1571): u'icky',\n",
       " (899, 2052): u'icky',\n",
       " (899, 3201): u'icky',\n",
       " (934, 2144): u'deserving',\n",
       " (937, 3459): u'talented',\n",
       " (989, 1012): u'aureate',\n",
       " (996, 2760): u'entire',\n",
       " (1033, 2492): u'calm',\n",
       " (1034, 1545): u'unfair',\n",
       " (1074, 2408): u'bang-up',\n",
       " (1074, 2880): u'bang-up',\n",
       " (1087, 1571): u'icky',\n",
       " (1087, 2052): u'icky',\n",
       " (1087, 3201): u'icky',\n",
       " (1101, 1599): u'barbarous',\n",
       " (1101, 2593): u'barbarous',\n",
       " (1116, 1618): u'nightlong',\n",
       " (1125, 3237): u'cunning',\n",
       " (1129, 2444): u'bitty',\n",
       " (1129, 2952): u'bitty',\n",
       " (1129, 3280): u'bitty',\n",
       " (1138, 3444): u'aroused',\n",
       " (1154, 1281): u'grey',\n",
       " (1165, 2219): u'rocky',\n",
       " (1202, 1579): u'fantastic',\n",
       " (1202, 2810): u'fantastic',\n",
       " (1207, 2670): u'brassy',\n",
       " (1240, 2784): u'extremist',\n",
       " (1248, 1598): u'bum',\n",
       " (1274, 1596): u'curious',\n",
       " (1275, 2073): u'obscure',\n",
       " (1284, 3147): u'exclusive',\n",
       " (1285, 1617): u'sixth',\n",
       " (1297, 2865): u'comfortable',\n",
       " (1332, 3470): u'hurt',\n",
       " (1334, 3065): u'fifty',\n",
       " (1388, 1574): u'grim',\n",
       " (1395, 2831): u'bouffant',\n",
       " (1406, 2997): u'ace',\n",
       " (1408, 1754): u'wide',\n",
       " (1418, 2445): u'antique',\n",
       " (1449, 2507): u'frigid',\n",
       " (1465, 2002): u'huge',\n",
       " (1484, 1848): u'apparent',\n",
       " (1484, 2781): u'apparent',\n",
       " (1503, 1668): u'everyday',\n",
       " (1503, 3139): u'everyday',\n",
       " (1518, 1555): u'red',\n",
       " (1518, 1810): u'red',\n",
       " (1548, 1752): u'bogus',\n",
       " (1548, 3325): u'bogus',\n",
       " (1555, 1810): u'red',\n",
       " (1571, 2052): u'icky',\n",
       " (1571, 3201): u'icky',\n",
       " (1579, 2810): u'fantastic',\n",
       " (1584, 1938): u'ill-famed',\n",
       " (1599, 2593): u'barbarous',\n",
       " (1613, 2321): u'conceited',\n",
       " (1613, 3364): u'conceited',\n",
       " (1636, 2914): u'eighteenth',\n",
       " (1647, 1766): u'grateful',\n",
       " (1668, 3139): u'everyday',\n",
       " (1685, 1835): u'brumous',\n",
       " (1693, 3225): u'dirty',\n",
       " (1737, 2727): u'ageless',\n",
       " (1737, 3317): u'ageless',\n",
       " (1752, 3325): u'bogus',\n",
       " (1789, 3230): u'null',\n",
       " (1848, 2781): u'apparent',\n",
       " (1955, 2809): u'incredible',\n",
       " (1974, 2533): u'fifteenth',\n",
       " (2045, 2978): u'excellent',\n",
       " (2052, 3201): u'icky',\n",
       " (2089, 2213): u'fifth',\n",
       " (2118, 2942): u'bushy',\n",
       " (2135, 2808): u'cockamamie',\n",
       " (2181, 3464): u'debonair',\n",
       " (2305, 2622): u'ill',\n",
       " (2319, 2925): u'gay',\n",
       " (2321, 3364): u'conceited',\n",
       " (2328, 2374): u'blond',\n",
       " (2336, 2832): u'bantam',\n",
       " (2405, 2907): u'average',\n",
       " (2408, 2880): u'bang-up',\n",
       " (2444, 2952): u'bitty',\n",
       " (2444, 3280): u'bitty',\n",
       " (2450, 2743): u'bigger',\n",
       " (2521, 2824): u'edgy',\n",
       " (2532, 2803): u'slender',\n",
       " (2536, 2750): u'deluxe',\n",
       " (2569, 2883): u'immaculate',\n",
       " (2578, 3256): u'aged',\n",
       " (2621, 2656): u'particular',\n",
       " (2727, 3317): u'ageless',\n",
       " (2836, 3371): u'permanent',\n",
       " (2845, 3451): u'ill-fed',\n",
       " (2913, 3243): u'difficult',\n",
       " (2952, 3280): u'bitty',\n",
       " (3191, 3454): u'coincident',\n",
       " (3257, 3288): u'coarse'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Noun Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find noun hypernyms\n",
    "nhypes = lowestCommonHypernyms(nsimatrix, nsyns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many noun hypernyms?  522\n",
      "how many valid noun hypernyms?  522\n",
      "how many invalid noun hypernyms?  0\n",
      "example key: (94, 123), value: millimeter\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"how many noun hypernyms? \", countHypernyms(nhypes)\n",
    "print \"how many valid noun hypernyms? \", countHypernyms(nhypes, count_valid=True, count_invalid=False)\n",
    "print \"how many invalid noun hypernyms? \", countHypernyms(nhypes, count_valid=False, count_invalid=True)\n",
    "print \"example key: {}, value: {}\".format(nhypes.keys()[0],nhypes[nhypes.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 3404): u'sleep',\n",
       " (4, 6485): u'integrity',\n",
       " (42, 1502): u'yak',\n",
       " (42, 6448): u'yak',\n",
       " (46, 8733): u'hide',\n",
       " (48, 5999): u'hate',\n",
       " (54, 8182): u'swagman',\n",
       " (60, 7630): u'tornado',\n",
       " (61, 893): u'dormitory',\n",
       " (86, 3980): u'limousine',\n",
       " (91, 3623): u'craze',\n",
       " (94, 123): u'millimeter',\n",
       " (125, 858): u'chump',\n",
       " (125, 3534): u'chump',\n",
       " (127, 651): u'attempt',\n",
       " (127, 1721): u'attempt',\n",
       " (155, 7909): u'lease',\n",
       " (156, 4856): u'rumor',\n",
       " (170, 909): u'ace',\n",
       " (170, 7764): u'ace',\n",
       " (234, 511): u'three',\n",
       " (239, 1007): u'kingdom',\n",
       " (241, 1354): u'ma',\n",
       " (241, 1593): u'ma',\n",
       " (241, 5928): u'ma',\n",
       " (241, 6003): u'ma',\n",
       " (241, 6007): u'ma',\n",
       " (249, 8299): u'phase',\n",
       " (251, 567): u'panic',\n",
       " (258, 1618): u'aunt',\n",
       " (262, 4155): u'bait',\n",
       " (263, 2725): u'ecstasy',\n",
       " (293, 5804): u'barroom',\n",
       " (331, 3502): u'adventure',\n",
       " (340, 7606): u'lotto',\n",
       " (341, 5724): u'assurance',\n",
       " (343, 3239): u'sludge',\n",
       " (358, 536): u'purpose',\n",
       " (358, 2500): u'purpose',\n",
       " (358, 3643): u'purpose',\n",
       " (377, 5355): u'brow',\n",
       " (385, 8077): u'bent',\n",
       " (391, 6637): u'opportunity',\n",
       " (395, 1674): u'dad',\n",
       " (395, 1867): u'dad',\n",
       " (395, 5323): u'dad',\n",
       " (395, 7138): u'dad',\n",
       " (395, 8032): u'dad',\n",
       " (402, 3863): u'revolver',\n",
       " (417, 5245): u'theater',\n",
       " (419, 2719): u'prisoner',\n",
       " (446, 3568): u'asshole',\n",
       " (446, 4821): u'asshole',\n",
       " (446, 6346): u'asshole',\n",
       " (471, 2514): u'asset',\n",
       " (500, 5398): u'spirit',\n",
       " (501, 4363): u'slope',\n",
       " (518, 1486): u'cast',\n",
       " (520, 8090): u'breast',\n",
       " (530, 2131): u'eternity',\n",
       " (532, 6168): u'frog',\n",
       " (536, 2500): u'purpose',\n",
       " (536, 3643): u'purpose',\n",
       " (540, 2193): u'grief',\n",
       " (540, 7539): u'grief',\n",
       " (552, 5760): u'shop',\n",
       " (558, 2249): u'adieu',\n",
       " (558, 4559): u'adieu',\n",
       " (559, 7116): u'battle',\n",
       " (594, 5171): u'dawn',\n",
       " (594, 5506): u'dawn',\n",
       " (617, 8724): u'referee',\n",
       " (639, 4236): u'fellow',\n",
       " (642, 4368): u'black',\n",
       " (651, 1721): u'attempt',\n",
       " (660, 5431): u'laugh',\n",
       " (662, 4709): u'subject',\n",
       " (662, 8448): u'subject',\n",
       " (670, 3812): u'sofa',\n",
       " (670, 7919): u'sofa',\n",
       " (672, 7084): u'rhinoceros',\n",
       " (673, 1355): u'maestro',\n",
       " (678, 768): u'topographic_point',\n",
       " (691, 5753): u'narrative',\n",
       " (693, 3669): u'jail',\n",
       " (703, 4596): u'arrow',\n",
       " (722, 2868): u'answer',\n",
       " (726, 7866): u'twilight',\n",
       " (729, 7128): u'resignation',\n",
       " (749, 4116): u'chap',\n",
       " (749, 6045): u'chap',\n",
       " (749, 7675): u'chap',\n",
       " (750, 4255): u'maid',\n",
       " (762, 5396): u'remainder',\n",
       " (765, 1234): u'hood',\n",
       " (765, 4638): u'hood',\n",
       " (765, 8739): u'hood',\n",
       " (794, 4163): u'anteroom',\n",
       " (794, 4618): u'anteroom',\n",
       " (805, 4496): u'curse',\n",
       " (805, 7214): u'curse',\n",
       " (821, 1695): u'foreigner',\n",
       " (824, 5290): u'inside',\n",
       " (825, 987): u'duty',\n",
       " (825, 5184): u'duty',\n",
       " (838, 5984): u'glamor',\n",
       " (841, 6181): u'dunce',\n",
       " (842, 4149): u'sunset',\n",
       " (848, 8728): u'shooting',\n",
       " (854, 6743): u'mister',\n",
       " (858, 3534): u'chump',\n",
       " (874, 1095): u'idiot',\n",
       " (874, 6036): u'idiot',\n",
       " (882, 2892): u'dinner_jacket',\n",
       " (905, 4086): u'semen',\n",
       " (905, 5945): u'semen',\n",
       " (909, 7764): u'ace',\n",
       " (914, 2322): u'expression',\n",
       " (941, 2630): u'aftermath',\n",
       " (950, 7305): u'career',\n",
       " (950, 8504): u'career',\n",
       " (954, 6116): u'lookout',\n",
       " (954, 7723): u'lookout',\n",
       " (960, 3778): u'person',\n",
       " (968, 1599): u'buddy',\n",
       " (968, 2306): u'buddy',\n",
       " (968, 7790): u'buddy',\n",
       " (971, 2736): u'thunderbolt',\n",
       " (975, 4990): u'director',\n",
       " (987, 5184): u'duty',\n",
       " (1000, 5298): u'palette',\n",
       " (1002, 4938): u'pile',\n",
       " (1030, 1654): u'freak',\n",
       " (1036, 2268): u'pit',\n",
       " (1050, 2340): u'boom',\n",
       " (1050, 4419): u'boom',\n",
       " (1050, 7229): u'boom',\n",
       " (1072, 8079): u'undertaking',\n",
       " (1073, 3205): u'dirt',\n",
       " (1073, 7883): u'dirt',\n",
       " (1081, 3606): u'appreciation',\n",
       " (1095, 6036): u'idiot',\n",
       " (1110, 7115): u'sneeze',\n",
       " (1112, 5458): u'neighbor',\n",
       " (1134, 5218): u'television',\n",
       " (1136, 8527): u'pistol',\n",
       " (1140, 2696): u'case',\n",
       " (1158, 3936): u'buttocks',\n",
       " (1159, 4246): u'vicinity',\n",
       " (1159, 5992): u'vicinity',\n",
       " (1162, 7400): u'past',\n",
       " (1179, 8343): u'component',\n",
       " (1186, 5166): u'sphere',\n",
       " (1209, 7461): u'morning',\n",
       " (1212, 6588): u'adolescent',\n",
       " (1233, 7724): u'fall',\n",
       " (1234, 4638): u'hood',\n",
       " (1234, 8739): u'hood',\n",
       " (1272, 3201): u'limelight',\n",
       " (1292, 3056): u'mystery',\n",
       " (1314, 4117): u'glitter',\n",
       " (1323, 4096): u'beep',\n",
       " (1337, 2595): u'consequence',\n",
       " (1337, 3100): u'consequence',\n",
       " (1347, 1937): u'girl',\n",
       " (1347, 5170): u'girl',\n",
       " (1354, 1593): u'ma',\n",
       " (1354, 5928): u'ma',\n",
       " (1354, 6003): u'ma',\n",
       " (1354, 6007): u'ma',\n",
       " (1375, 6658): u'pornography',\n",
       " (1375, 7207): u'pornography',\n",
       " (1397, 6287): u'macintosh',\n",
       " (1398, 1564): u'wage',\n",
       " (1398, 3513): u'wage',\n",
       " (1437, 3251): u'ballyhoo',\n",
       " (1445, 7321): u'touch',\n",
       " (1446, 4473): u'second',\n",
       " (1501, 3199): u'joke',\n",
       " (1502, 6448): u'yak',\n",
       " (1513, 4337): u'clasp',\n",
       " (1513, 6737): u'clasp',\n",
       " (1522, 6099): u'anguish',\n",
       " (1543, 3420): u'dark',\n",
       " (1549, 4905): u'cheep',\n",
       " (1564, 3513): u'wage',\n",
       " (1593, 5928): u'ma',\n",
       " (1593, 6003): u'ma',\n",
       " (1593, 6007): u'ma',\n",
       " (1599, 2306): u'buddy',\n",
       " (1599, 7790): u'buddy',\n",
       " (1610, 5790): u'play',\n",
       " (1620, 4192): u'sweetheart',\n",
       " (1620, 5883): u'sweetheart',\n",
       " (1634, 6949): u'reporter',\n",
       " (1663, 2453): u'expressway',\n",
       " (1666, 7962): u'seller',\n",
       " (1674, 1867): u'dad',\n",
       " (1674, 5323): u'dad',\n",
       " (1674, 7138): u'dad',\n",
       " (1674, 8032): u'dad',\n",
       " (1675, 1742): u'lens',\n",
       " (1705, 3161): u'residence',\n",
       " (1728, 5896): u'loot',\n",
       " (1735, 4085): u'scribble',\n",
       " (1757, 8569): u'amour_propre',\n",
       " (1762, 5488): u'cheapness',\n",
       " (1768, 6550): u'lumberman',\n",
       " (1773, 4961): u'crap',\n",
       " (1781, 5772): u'kind',\n",
       " (1783, 1788): u'baby',\n",
       " (1790, 6118): u'crop',\n",
       " (1829, 6560): u'sister',\n",
       " (1845, 7003): u'evening',\n",
       " (1851, 6056): u'parlor',\n",
       " (1860, 6717): u'pound',\n",
       " (1867, 5323): u'dad',\n",
       " (1867, 7138): u'dad',\n",
       " (1867, 8032): u'dad',\n",
       " (1868, 4891): u'clang',\n",
       " (1882, 2663): u'violin',\n",
       " (1883, 2424): u'disregard',\n",
       " (1889, 6360): u'atmosphere',\n",
       " (1911, 2172): u'bartender',\n",
       " (1911, 7477): u'bartender',\n",
       " (1922, 5887): u'fathead',\n",
       " (1937, 5170): u'girl',\n",
       " (1944, 3106): u'grandfather',\n",
       " (1944, 7730): u'grandfather',\n",
       " (1954, 3488): u'information',\n",
       " (1968, 3631): u'physique',\n",
       " (1990, 6777): u'misdemeanor',\n",
       " (2001, 4060): u'lunatic',\n",
       " (2001, 6829): u'lunatic',\n",
       " (2003, 8026): u'satan',\n",
       " (2004, 8523): u'bias',\n",
       " (2006, 7748): u'marriage',\n",
       " (2017, 3397): u'fagot',\n",
       " (2017, 6424): u'fagot',\n",
       " (2033, 3984): u'hippie',\n",
       " (2057, 4326): u'blink_of_an_eye',\n",
       " (2060, 6140): u'remark',\n",
       " (2072, 2708): u'rustle',\n",
       " (2087, 4181): u'nickel',\n",
       " (2126, 2861): u'speaker',\n",
       " (2136, 6202): u'animal',\n",
       " (2136, 7331): u'animal',\n",
       " (2142, 5759): u'very_important_person',\n",
       " (2143, 3553): u'idea',\n",
       " (2145, 6320): u'command',\n",
       " (2150, 5771): u'percentage',\n",
       " (2160, 5610): u'aroma',\n",
       " (2160, 7974): u'aroma',\n",
       " (2172, 7477): u'bartender',\n",
       " (2178, 4998): u'bulge',\n",
       " (2187, 3672): u'banquet',\n",
       " (2188, 4833): u'hush',\n",
       " (2193, 7539): u'grief',\n",
       " (2204, 5167): u'stairway',\n",
       " (2215, 5777): u'motivation',\n",
       " (2229, 8660): u'cocoa',\n",
       " (2249, 4559): u'adieu',\n",
       " (2270, 3872): u'moonlight',\n",
       " (2272, 5087): u'jesus',\n",
       " (2279, 2446): u'grandma',\n",
       " (2279, 6714): u'grandma',\n",
       " (2279, 7184): u'grandma',\n",
       " (2306, 7790): u'buddy',\n",
       " (2316, 2554): u'doctor',\n",
       " (2316, 7173): u'doctor',\n",
       " (2340, 4419): u'boom',\n",
       " (2340, 7229): u'boom',\n",
       " (2383, 4934): u'hypocrite',\n",
       " (2383, 6679): u'hypocrite',\n",
       " (2384, 3206): u'telephone',\n",
       " (2403, 5925): u'affair',\n",
       " (2419, 5612): u'ballad',\n",
       " (2446, 6714): u'grandma',\n",
       " (2446, 7184): u'grandma',\n",
       " (2451, 6706): u'baggage',\n",
       " (2460, 4466): u'victory',\n",
       " (2466, 2984): u'hallway',\n",
       " (2471, 4550): u'lab',\n",
       " (2496, 2679): u'suburb',\n",
       " (2500, 3643): u'purpose',\n",
       " (2530, 7062): u'miniskirt',\n",
       " (2536, 4075): u'manner',\n",
       " (2536, 5211): u'manner',\n",
       " (2536, 5308): u'manner',\n",
       " (2536, 6871): u'manner',\n",
       " (2554, 7173): u'doctor',\n",
       " (2557, 4939): u'swamp',\n",
       " (2566, 3009): u'lurch',\n",
       " (2577, 4270): u'plain',\n",
       " (2594, 6437): u'joint',\n",
       " (2595, 3100): u'consequence',\n",
       " (2609, 7576): u'check',\n",
       " (2618, 7320): u'state',\n",
       " (2639, 3382): u'calamity',\n",
       " (2639, 6325): u'calamity',\n",
       " (2689, 6319): u'photograph',\n",
       " (2729, 8376): u'luster',\n",
       " (2746, 3032): u'turf',\n",
       " (2762, 3025): u'fabric',\n",
       " (2782, 8308): u'whiskey',\n",
       " (2801, 3208): u'center',\n",
       " (2807, 6011): u'jewelry',\n",
       " (2818, 3905): u'impression',\n",
       " (2823, 4807): u'sting',\n",
       " (2840, 4628): u'bustle',\n",
       " (2844, 6153): u'blizzard',\n",
       " (2846, 3114): u'die',\n",
       " (2850, 5177): u'metro',\n",
       " (2871, 3654): u'defender',\n",
       " (2871, 8694): u'defender',\n",
       " (2902, 8323): u'native',\n",
       " (2904, 5997): u'scope',\n",
       " (2911, 7718): u'titanium',\n",
       " (2912, 3578): u'car',\n",
       " (2912, 4061): u'car',\n",
       " (2920, 7213): u'thief',\n",
       " (2930, 5805): u'winter',\n",
       " (2944, 8714): u'gelatin',\n",
       " (2958, 3134): u'feature',\n",
       " (3031, 4962): u'view',\n",
       " (3046, 7390): u'waist',\n",
       " (3059, 8218): u'rival',\n",
       " (3083, 8019): u'drunkard',\n",
       " (3106, 7730): u'grandfather',\n",
       " (3119, 6478): u'base',\n",
       " (3140, 3526): u'gangster',\n",
       " (3152, 4311): u'boodle',\n",
       " (3181, 7089): u'vacation',\n",
       " (3205, 7883): u'dirt',\n",
       " (3226, 5552): u'captivity',\n",
       " (3230, 4210): u'longing',\n",
       " (3236, 6018): u'summer',\n",
       " (3254, 4274): u'virginia',\n",
       " (3273, 5432): u'chow',\n",
       " (3318, 3842): u'dainty',\n",
       " (3321, 4108): u'repose',\n",
       " (3340, 6848): u'toilet',\n",
       " (3343, 6491): u'aluminum',\n",
       " (3382, 6325): u'calamity',\n",
       " (3387, 5417): u'smile',\n",
       " (3393, 5533): u'detention',\n",
       " (3397, 6424): u'fagot',\n",
       " (3415, 4306): u'monotony',\n",
       " (3416, 8509): u'position',\n",
       " (3417, 7440): u'therapist',\n",
       " (3419, 8530): u'bully',\n",
       " (3419, 8649): u'bully',\n",
       " (3443, 6197): u'tan',\n",
       " (3443, 6495): u'tan',\n",
       " (3446, 4727): u'doughnut',\n",
       " (3452, 8465): u'rotter',\n",
       " (3452, 8708): u'rotter',\n",
       " (3523, 4910): u'hell',\n",
       " (3568, 4821): u'asshole',\n",
       " (3568, 6346): u'asshole',\n",
       " (3578, 4061): u'car',\n",
       " (3628, 7978): u'enchantment',\n",
       " (3637, 4095): u'piglet',\n",
       " (3639, 7526): u'plan',\n",
       " (3654, 8694): u'defender',\n",
       " (3661, 4491): u'smoke',\n",
       " (3688, 4171): u'matter',\n",
       " (3689, 5507): u'movie',\n",
       " (3689, 6733): u'movie',\n",
       " (3732, 7304): u'shingle',\n",
       " (3812, 7919): u'sofa',\n",
       " (3817, 4822): u'spring',\n",
       " (3826, 3839): u'religion',\n",
       " (3834, 7709): u'rug',\n",
       " (3875, 8014): u'haunt',\n",
       " (3899, 6249): u'temper',\n",
       " (3920, 8518): u'pioneer',\n",
       " (3946, 5345): u'criminal',\n",
       " (3946, 7948): u'criminal',\n",
       " (3946, 8359): u'criminal',\n",
       " (3968, 8700): u'resentment',\n",
       " (3970, 7095): u'coffin',\n",
       " (3972, 6245): u'kiss',\n",
       " (4014, 4151): u'bend',\n",
       " (4034, 7966): u'strut',\n",
       " (4049, 6660): u'motorcycle',\n",
       " (4055, 6996): u'child',\n",
       " (4055, 7030): u'child',\n",
       " (4060, 6829): u'lunatic',\n",
       " (4075, 5211): u'manner',\n",
       " (4075, 5308): u'manner',\n",
       " (4075, 6871): u'manner',\n",
       " (4086, 5945): u'semen',\n",
       " (4116, 6045): u'chap',\n",
       " (4116, 7675): u'chap',\n",
       " (4124, 5451): u'starship',\n",
       " (4163, 4618): u'anteroom',\n",
       " (4180, 5067): u'brink',\n",
       " (4192, 5883): u'sweetheart',\n",
       " (4195, 4370): u'drip',\n",
       " (4197, 4584): u'knock',\n",
       " (4197, 5732): u'knock',\n",
       " (4206, 6579): u'dame',\n",
       " (4214, 5904): u'destiny',\n",
       " (4246, 5992): u'vicinity',\n",
       " (4273, 7007): u'mistake',\n",
       " (4273, 8053): u'mistake',\n",
       " (4280, 7691): u'spouse',\n",
       " (4303, 4529): u'snake',\n",
       " (4304, 8064): u'pursuit',\n",
       " (4335, 4817): u'recoil',\n",
       " (4337, 6737): u'clasp',\n",
       " (4340, 7570): u'death',\n",
       " (4360, 5487): u'bullet',\n",
       " (4373, 8419): u'cunt',\n",
       " (4419, 7229): u'boom',\n",
       " (4445, 7857): u'daze',\n",
       " (4452, 4794): u'slap',\n",
       " (4461, 8656): u'nigger',\n",
       " (4480, 7815): u'internet',\n",
       " (4496, 7214): u'curse',\n",
       " (4504, 7306): u'athlete',\n",
       " (4505, 5545): u'glance',\n",
       " (4514, 5182): u'ball',\n",
       " (4558, 7021): u'ammunition',\n",
       " (4571, 6426): u'fit',\n",
       " (4576, 5109): u'fad',\n",
       " (4584, 5732): u'knock',\n",
       " (4604, 5128): u'magazine',\n",
       " (4638, 8739): u'hood',\n",
       " (4646, 5813): u'tune',\n",
       " (4670, 5742): u'palace',\n",
       " (4709, 8448): u'subject',\n",
       " (4726, 8418): u'bad',\n",
       " (4757, 5947): u'ailment',\n",
       " (4774, 7508): u'prototype',\n",
       " (4780, 6647): u'cry',\n",
       " (4821, 6346): u'asshole',\n",
       " (4830, 5440): u'runaway',\n",
       " (4836, 6230): u'jean',\n",
       " (4880, 5574): u'lawyer',\n",
       " (4924, 6971): u'sunlight',\n",
       " (4934, 6679): u'hypocrite',\n",
       " (4950, 8027): u'couple',\n",
       " (4989, 8547): u'walk',\n",
       " (5019, 7286): u'guy',\n",
       " (5162, 5462): u'whitey',\n",
       " (5171, 5506): u'dawn',\n",
       " (5211, 5308): u'manner',\n",
       " (5211, 6871): u'manner',\n",
       " (5276, 6017): u'refrigerator',\n",
       " (5291, 7088): u'husband',\n",
       " (5308, 6871): u'manner',\n",
       " (5318, 5416): u'railway',\n",
       " (5323, 7138): u'dad',\n",
       " (5323, 8032): u'dad',\n",
       " (5326, 7945): u'flicker',\n",
       " (5328, 6383): u'pot',\n",
       " (5338, 7502): u'thousand',\n",
       " (5345, 7948): u'criminal',\n",
       " (5345, 8359): u'criminal',\n",
       " (5362, 7896): u'concern',\n",
       " (5374, 6738): u'hideout',\n",
       " (5437, 6173): u'noon',\n",
       " (5452, 7330): u'rock',\n",
       " (5507, 6733): u'movie',\n",
       " (5602, 6698): u'family',\n",
       " (5610, 7974): u'aroma',\n",
       " (5645, 7816): u'airplane',\n",
       " (5704, 6734): u'injury',\n",
       " (5741, 8222): u'menace',\n",
       " (5764, 7202): u'intestine',\n",
       " (5811, 6601): u'liquor',\n",
       " (5863, 6670): u'reverie',\n",
       " (5863, 7389): u'reverie',\n",
       " (5866, 6576): u'pancake',\n",
       " (5928, 6003): u'ma',\n",
       " (5928, 6007): u'ma',\n",
       " (5964, 7746): u'aggravation',\n",
       " (5988, 8384): u'measure',\n",
       " (6003, 6007): u'ma',\n",
       " (6004, 6170): u'crying',\n",
       " (6045, 7675): u'chap',\n",
       " (6116, 7723): u'lookout',\n",
       " (6132, 7198): u'degree',\n",
       " (6197, 6495): u'tan',\n",
       " (6202, 7331): u'animal',\n",
       " (6203, 7357): u'bandanna',\n",
       " (6339, 6452): u'microphone',\n",
       " (6412, 8551): u'sadness',\n",
       " (6420, 7451): u'occupation',\n",
       " (6458, 6591): u'accountant',\n",
       " (6479, 6890): u'prostitute',\n",
       " (6538, 6675): u'debris',\n",
       " (6658, 7207): u'pornography',\n",
       " (6670, 7389): u'reverie',\n",
       " (6714, 7184): u'grandma',\n",
       " (6746, 7183): u'passage',\n",
       " (6843, 7252): u'plaything',\n",
       " (6904, 7191): u'choice',\n",
       " (6996, 7030): u'child',\n",
       " (7007, 8053): u'mistake',\n",
       " (7019, 8543): u'writer',\n",
       " (7081, 7091): u'discord',\n",
       " (7103, 7805): u'despair',\n",
       " (7138, 8032): u'dad',\n",
       " (7167, 8584): u'potato',\n",
       " (7269, 7554): u'supporter',\n",
       " (7305, 8504): u'career',\n",
       " (7339, 7849): u'year',\n",
       " (7505, 7773): u'mark',\n",
       " (7728, 7867): u'eden',\n",
       " (7743, 8321): u'rudiment',\n",
       " (7895, 8695): u'scheme',\n",
       " (7948, 8359): u'criminal',\n",
       " (8228, 8565): u'finale',\n",
       " (8361, 8505): u'material',\n",
       " (8396, 8710): u'cover_girl',\n",
       " (8441, 8749): u'basement',\n",
       " (8465, 8708): u'rotter',\n",
       " (8521, 8576): u'braid',\n",
       " (8530, 8649): u'bully'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save adj hypernyms\n",
    "pickle.dump( ahypes, open( \"../../data/conditioned/ahypes.p\", \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save noun hypernyms\n",
    "pickle.dump( nhypes, open( \"../../data/conditioned/nhypes.p\", \"wb\" ) )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
