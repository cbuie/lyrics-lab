{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vector Ensemble\n",
    "Ensemble approach using Spark. This notebook leverages the consolidated vector CSV which includes normal, synonym, and hypernym vectors, see [master-lyricsdf-word_syn_hype_vectors.csv](../../data/conditioned/master-lyricsdf-word_syn_hype_vectors.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(2),2)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Setup Data For Pipeline\n",
    "###Load Dataframe into Pandas for initial manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the lyrics from the approved \"master\" dataframe\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/conditioned/master-lyricsdf-word_syn_hype_vectors.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "      <th>lyrics_url</th>\n",
       "      <th>lyrics_abstract</th>\n",
       "      <th>noun_vector</th>\n",
       "      <th>adj_vector</th>\n",
       "      <th>noun_syn_vector</th>\n",
       "      <th>adj_syn_vector</th>\n",
       "      <th>noun_syn_hype_vector</th>\n",
       "      <th>adj_syn_hype_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-1</td>\n",
       "      <td>http://lyrics.wikia.com/Simon_And_Garfunkel:Br...</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "      <td>time bridge water</td>\n",
       "      <td>rough troubled</td>\n",
       "      <td>bridge time water</td>\n",
       "      <td>rough troubled</td>\n",
       "      <td>bridge time water</td>\n",
       "      <td>rough troubled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  position  year                                         title.href                       title               artist                                             lyrics  decade song_key                                         lyrics_url                                    lyrics_abstract        noun_vector      adj_vector    noun_syn_vector  adj_syn_vector noun_syn_hype_vector adj_syn_hype_vector\n",
       "0      0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...  Bridge over Troubled Water  Simon and Garfunkel  When you're weary. Feeling small. When tears a...    1970   1970-1  http://lyrics.wikia.com/Simon_And_Garfunkel:Br...  When you're weary. Feeling small. When tears a...  time bridge water  rough troubled  bridge time water  rough troubled    bridge time water      rough troubled"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Add Labels for Data based on position\n",
    "This will change based upon the current run. A straight-forward usage is to see how well top and bottom 50 can be predicted.\n",
    "**Note: Spark ML seems picky about `label` being the column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use positions for labeling\n",
    "pcols = ['bin_10_percent','bin_25_percent','is_top_10_percent','is_top_25_percent','is_top_50_percent']\n",
    "\n",
    "# linear regression only supports binary topics\n",
    "pbinarycols = ['is_top_10_percent','is_top_25_percent','is_top_50_percent'] \n",
    "\n",
    "pos_dict = {\n",
    "    'bin_10_percent': {\n",
    "      10.0:range(1,11),\n",
    "      20.0:range(11,21),\n",
    "      30.0:range(21,31),\n",
    "      40.0:range(31,41),\n",
    "      50.0:range(41,51),\n",
    "      60.0:range(51,61), \n",
    "      70.0:range(61,71),\n",
    "      80.0:range(71,81),\n",
    "      90.0:range(81,91),\n",
    "      100.0:range(91,101)  \n",
    "    }, 'bin_25_percent': {\n",
    "      25.0:range(1,26),\n",
    "      50.0:range(26,51),\n",
    "      75.0:range(51,76),\n",
    "      100.0:range(76,101)\n",
    "    }, 'is_top_10_percent': {\n",
    "      1.0:range(1,11),\n",
    "      0.0:range(11,101)            \n",
    "    }, 'is_top_25_percent': {\n",
    "      1.0:range(1,26),\n",
    "      0.0:range(26,101)\n",
    "    }, 'is_top_50_percent': {\n",
    "      1.0:range(1,51),\n",
    "      0.0:range(51,101)\n",
    "    }\n",
    "}\n",
    "\n",
    "pos_dict_descrips = {\n",
    "    'bin_10_percent': \"10 Percent Splits (10 Topics)\",\n",
    "    'bin_25_percent': \"25 Percent Splits (4 Topics)\",\n",
    "    'is_top_10_percent': \"Top 10 versus Bottom 90 (2 Topics, 1 means 'yes')\",\n",
    "    'is_top_25_percent': \"Top 25 versus Bottom 75 (2 Topics, 1 means 'yes')\",      \n",
    "    'is_top_50_percent': \"Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "      <th>lyrics_url</th>\n",
       "      <th>lyrics_abstract</th>\n",
       "      <th>noun_vector</th>\n",
       "      <th>adj_vector</th>\n",
       "      <th>noun_syn_vector</th>\n",
       "      <th>adj_syn_vector</th>\n",
       "      <th>noun_syn_hype_vector</th>\n",
       "      <th>adj_syn_hype_vector</th>\n",
       "      <th>bin_10_percent</th>\n",
       "      <th>is_top_50_percent</th>\n",
       "      <th>is_top_25_percent</th>\n",
       "      <th>bin_25_percent</th>\n",
       "      <th>is_top_10_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>3671</td>\n",
       "      <td>72</td>\n",
       "      <td>2006</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Too_Little_Too_Late</td>\n",
       "      <td>Too Little Too Late</td>\n",
       "      <td>JoJo</td>\n",
       "      <td>Come with me. Stay the night. Just say the wor...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2006-72</td>\n",
       "      <td>http://lyrics.wikia.com/JoJo:Too_Little_Too_Late</td>\n",
       "      <td>Come with me. Stay the night. Just say the wor...</td>\n",
       "      <td>ya game time thing love chase</td>\n",
       "      <td>little late strong right young chase</td>\n",
       "      <td>game love pursuit thing time</td>\n",
       "      <td>late right small strong young</td>\n",
       "      <td>game love pursuit thing time</td>\n",
       "      <td>late right small strong young</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>18</td>\n",
       "      <td>1980</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Upside_Down</td>\n",
       "      <td>Upside Down</td>\n",
       "      <td>Diana Ross</td>\n",
       "      <td>I said upside down. You're turning me. You're ...</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980-18</td>\n",
       "      <td>http://lyrics.wikia.com/Diana_Ross:Upside_Down</td>\n",
       "      <td>I said upside down. You're turning me. You're ...</td>\n",
       "      <td>cheatin</td>\n",
       "      <td>aware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aware</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>3471</td>\n",
       "      <td>72</td>\n",
       "      <td>2004</td>\n",
       "      <td>https://en.wikipedia.org/wiki/U_Should%27ve_Kn...</td>\n",
       "      <td>U Should've Known Better</td>\n",
       "      <td>Monica</td>\n",
       "      <td>Oh, oh. Lalalalala. I didn't ask to go with yo...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2004-72</td>\n",
       "      <td>http://lyrics.wikia.com/Monica:U_Should%27ve_K...</td>\n",
       "      <td>Oh, oh. Lalalalala. I didn't ask to go with yo...</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  position  year                                         title.href                     title      artist                                             lyrics  decade song_key                                         lyrics_url                                    lyrics_abstract                    noun_vector                            adj_vector               noun_syn_vector                 adj_syn_vector          noun_syn_hype_vector            adj_syn_hype_vector  bin_10_percent  \\\n",
       "3671   3671        72  2006  https://en.wikipedia.org/wiki/Too_Little_Too_Late       Too Little Too Late        JoJo  Come with me. Stay the night. Just say the wor...    2000  2006-72   http://lyrics.wikia.com/JoJo:Too_Little_Too_Late  Come with me. Stay the night. Just say the wor...  ya game time thing love chase  little late strong right young chase  game love pursuit thing time  late right small strong young  game love pursuit thing time  late right small strong young              80   \n",
       "1017   1017        18  1980          https://en.wikipedia.org/wiki/Upside_Down               Upside Down  Diana Ross  I said upside down. You're turning me. You're ...    1980  1980-18     http://lyrics.wikia.com/Diana_Ross:Upside_Down  I said upside down. You're turning me. You're ...                        cheatin                                 aware                           NaN                          aware                           NaN                          aware              20   \n",
       "3471   3471        72  2004  https://en.wikipedia.org/wiki/U_Should%27ve_Kn...  U Should've Known Better      Monica  Oh, oh. Lalalalala. I didn't ask to go with yo...    2000  2004-72  http://lyrics.wikia.com/Monica:U_Should%27ve_K...  Oh, oh. Lalalalala. I didn't ask to go with yo...                      interlude                               musical                     interlude                        musical                     interlude                        musical              80   \n",
       "\n",
       "      is_top_50_percent  is_top_25_percent  bin_25_percent  is_top_10_percent  \n",
       "3671                  0                  0              75                  0  \n",
       "1017                  1                  1              25                  0  \n",
       "3471                  0                  0              75                  0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def labelForPosition(pos,pos_dict_key):\n",
    "    for k,p in pos_dict[pos_dict_key].iteritems():\n",
    "        if pos in p:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "#label is position\n",
    "for pos_dict_key in pos_dict.keys():\n",
    "    lyrics_pd_df[pos_dict_key] = lyrics_pd_df.position.apply(lambda p : labelForPosition(p,pos_dict_key))\n",
    "\n",
    "#sanity check\n",
    "lyrics_pd_df.sample(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filter out Non-Lyric Records\n",
    "**Non-Lyrics due to:**\n",
    "* Instrumentals\n",
    "* Licensing restrictions on lyrics.wikia\n",
    "* No lyrics added to lyrics.wikia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the noun cols\n",
    "ncols = ['noun_vector','noun_syn_vector','noun_syn_hype_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many empty lyrics are there? 159\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls (which may include instrumentals or otherwise unavailable )\n",
    "print \"How many empty lyrics are there? {}\".format(len(np.where(pd.isnull(lyrics_pd_df[['lyrics']]))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out null lyrics\n",
    "lyrics_pd_df = lyrics_pd_df.dropna(axis=0, how='any', thresh=None, subset=['lyrics'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4341, 22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after removing noun_vector empties --> (4121, 22)\n",
      "shape after removing noun_syn_vector empties --> (4105, 22)\n",
      "shape after removing noun_syn_hype_vector empties --> (4105, 22)\n"
     ]
    }
   ],
   "source": [
    "# ALSO NEED TO REMOVE EMPTY NCOL ROWS RESULTING FROM VOCAB SHRINKAGE OPERATIONS\n",
    "for col in ncols:\n",
    "    lyrics_pd_df = lyrics_pd_df.dropna(axis=0, how='any', thresh=None, subset=[col], inplace=False)\n",
    "    print \"shape after removing {} empties --> {}\".format(col,lyrics_pd_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ROWS after removing empties --> 4105\n"
     ]
    }
   ],
   "source": [
    "print \"Final ROWS after removing empties -->\", lyrics_pd_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up Master Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUniqueWordsSorted(df,word_col):\n",
    "    u = []\n",
    "    for r in df.iterrows():\n",
    "        ws = r[1][word_col]\n",
    "        if not isinstance(ws,float):\n",
    "            vs = ws.split()          \n",
    "            for v in vs:\n",
    "                if not v in u:\n",
    "                    u.append(v)\n",
    "        \n",
    "    return sorted(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- noun_vector ---\n",
      "\n",
      "\tHow long is dict? 5138\n",
      "\tWhat are the first 5 entries? [['60', '8-bit', '>jeep', '>mayback', '\\\\n|officialsite']]\n",
      "\n",
      "--- noun_syn_vector ---\n",
      "\n",
      "\tHow long is dict? 3230\n",
      "\tWhat are the first 5 entries? [['abdomen', 'ability', 'abnormality', 'abortion', 'abrasion']]\n",
      "\n",
      "--- noun_syn_hype_vector ---\n",
      "\n",
      "\tHow long is dict? 3215\n",
      "\tWhat are the first 5 entries? [['abdomen', 'ability', 'abnormality', 'abortion', 'abrasion']]\n"
     ]
    }
   ],
   "source": [
    "# this will be the master columns to populate for the matrix\n",
    "all_words_dict = {}\n",
    "\n",
    "for col in ncols:\n",
    "    print \"\\n--- {} ---\\n\".format(col)\n",
    "    all_words_dict[col] = getUniqueWordsSorted(lyrics_pd_df, col)\n",
    "    print \"\\tHow long is dict? {}\".format(len(all_words_dict[col]))\n",
    "    print \"\\tWhat are the first 5 entries? [{}]\".format(all_words_dict[col][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareWordLists(alist,blist):\n",
    "    same = []\n",
    "    ina = []\n",
    "    inb = []\n",
    "    \n",
    "    for a in alist:\n",
    "        if a in blist:\n",
    "            same.append(a)\n",
    "        else:\n",
    "            ina.append(a)\n",
    "    \n",
    "    for b in blist:\n",
    "        if b not in same:\n",
    "            inb.append(b)\n",
    "    return sorted(same), sorted(ina), sorted(inb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For syn versus syn-hype words...\n",
      "How many are same?  3215\n",
      "How many are only in syn?  15\n",
      "How many are only in hype?  0\n"
     ]
    }
   ],
   "source": [
    "tcomp = compareWordLists(all_words_dict[ncols[1]],all_words_dict[ncols[2]])\n",
    "print \"For syn versus syn-hype words...\"\n",
    "print \"How many are same? \", len(tcomp[0])\n",
    "print \"How many are only in syn? \", len(tcomp[1])\n",
    "print \"How many are only in hype? \", len(tcomp[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up dictionaries for string and vectorized data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert array of strings to array of arrays\n",
    "def stringsToUniqueVectors(strings):\n",
    "    vectors = []\n",
    "    for s in strings:\n",
    "        # test for NaN\n",
    "        if not isinstance(s,float):\n",
    "            tmp = s.split()\n",
    "            cs = []\n",
    "            for t in tmp:\n",
    "                if not t in cs:\n",
    "                    cs.append(t)\n",
    "            vectors.append(sorted(cs))\n",
    "        # just in case, handle empty    \n",
    "        else:\n",
    "            vectors.append([])\n",
    "            \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict for k=col, v=array of strings\n",
    "nstrings = {}\n",
    "\n",
    "# dict for k=col, v=array of arrays\n",
    "nvectors = {}\n",
    "\n",
    "# initialize\n",
    "for col in ncols:\n",
    "    nstrings[col] = []\n",
    "    nvectors[col] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# populate\n",
    "for col in ncols:\n",
    "    nstrings[col] = lyrics_pd_df[col].values\n",
    "    nvectors[col] = stringsToUniqueVectors(nstrings[col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`noun_vector`[32]: folk patch papa man wonder time money crop daddy rule chicken wood day rain family angel mama brand home -->\n",
      "\t['angel', 'brand', 'chicken', 'crop', 'daddy', 'day', 'family', 'folk', 'home', 'mama', 'man', 'money', 'papa', 'patch', 'rain', 'rule', 'time', 'wonder', 'wood']\n",
      "`noun_syn_vector`[32]: angel chicken crop dad day family folk home ma man money rain rule spot time trade_name wonder wood -->\n",
      "\t['angel', 'chicken', 'crop', 'dad', 'day', 'family', 'folk', 'home', 'ma', 'man', 'money', 'rain', 'rule', 'spot', 'time', 'trade_name', 'wonder', 'wood']\n",
      "`noun_syn_hype_vector`[32]: angel chicken crop dad day family folk home ma man money rain rule time topographic_point trade_name wonder wood -->\n",
      "\t['angel', 'chicken', 'crop', 'dad', 'day', 'family', 'folk', 'home', 'ma', 'man', 'money', 'rain', 'rule', 'time', 'topographic_point', 'trade_name', 'wonder', 'wood']\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "for col in ncols:\n",
    "    idx = 32\n",
    "    print \"`{}`[{}]: {} -->\\n\\t{}\".format(col,idx,nstrings[col][idx],nvectors[col][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Convert and manipulate with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "|index|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|          lyrics_url|     lyrics_abstract|         noun_vector|          adj_vector|     noun_syn_vector|      adj_syn_vector|noun_syn_hype_vector| adj_syn_hype_vector|bin_10_percent|is_top_50_percent|is_top_25_percent|bin_25_percent|is_top_10_percent|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "|    0|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|http://lyrics.wik...|When you're weary...|   time bridge water|      rough troubled|   bridge time water|      rough troubled|   bridge time water|      rough troubled|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "|    1|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|Why do birds sudd...|  1970|  1970-2|http://lyrics.wik...|Why do birds sudd...| dream starlight eye|           true blue| dream eye starlight|           blue true| dream eye starlight|           blue true|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "|    2|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|Mmm, da da da. Mm...|  1970|  1970-3|http://lyrics.wik...|Mmm, da da da. Mm...|woman mess mind m...|american importan...|crap light ma mes...|american colored ...|crap light ma mes...|american colored ...|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pipeline Using Spark\n",
    "Reference [combine all features into a single feature vector](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)\n",
    "![Ensemble Pipeline Overview](https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png)\n",
    "* Tokenizer\n",
    "* HashingTF\n",
    "* Word2Vec\n",
    "* OneHotEncoder\n",
    "* Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline adapted from:\n",
    "# http://spark.apache.org/docs/latest/ml-guide.html\n",
    "# https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used in common for assessing and printing predictions versus actuals\n",
    "def assessPredicts(predictsdf,pred_hits,ncol,pcol,pipeline_name=\"Pipeline\",reg_param=None, max_iter=None, verbose=False):\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    print \"For reg_param: {} & max_iter: {}, how did pipeline '{}' do predicting pcol '{}' for label '{}'?\".format(\n",
    "        (reg_param if reg_param else \"<unspecified>\"),\n",
    "        (max_iter if max_iter else \"<unspecified>\"),\n",
    "        pipeline_name,pos_dict_descrips[pcol],ncol)\n",
    "    \n",
    "    for r in predictsdf.iterrows():\n",
    "        song_key = r[1].song_key\n",
    "        pred = pred_hits[song_key]  \n",
    "        result = labelForPosition(r[1].position, pcol)\n",
    "        correct = result == pred\n",
    "        if correct:\n",
    "            hits +=1\n",
    "            if verbose:\n",
    "                print \"Correct ::: song_key --> {}, predicted {}\".format(song_key, pred)\n",
    "        else:\n",
    "            misses +=1\n",
    "            if verbose:\n",
    "                print \"Incorrect ::: song_key --> {}, predicted {}\".format(song_key, pred)\n",
    "    hitperc = float(hits)/float(hits+misses)*100.                \n",
    "    print \"'{}' {}% success --> hits: {}, misses: {}\".format(pipeline_name,hitperc,hits,misses)\n",
    "    \n",
    "    return hitperc,hits,misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fit Linear Regression Models to songs prior to 2013 and predict on 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## establish training and test Spark Dataframes (to be filtered further as needed)\n",
    "## e.g. traindf.select(['song_key',ncol,pcol])\n",
    "\n",
    "# training on songs to 2013 (uses nstrings)\n",
    "traindf = lyricsdf.filter(lyricsdf['year'] != 2014)\n",
    "\n",
    "# test year 2014 (uses nstrings)\n",
    "testdf = lyricsdf.filter(lyricsdf['year'] == 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pipeline  :  Linear Regression\n",
    "**Here is the output of** \n",
    "```python\n",
    "LogisticRegression().explainParams()\n",
    "```\n",
    "* __elasticNetParam__: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
    "* __featuresCol__: features column name (default: features)\n",
    "* __fitIntercept__: whether to fit an intercept term. (default: True)\n",
    "* __labelCol__: label column name (default: label)\n",
    "* __maxIter__: max number of iterations (>= 0) (default: 100)\n",
    "* __predictionCol__: prediction column name (default: prediction)\n",
    "* __probabilityCol__: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
    "* __rawPredictionCol__: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
    "* __regParam__: regularization parameter (>= 0) (default: 0.1)\n",
    "* __threshold__: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match. (default: 0.5)\n",
    "* __thresholds__: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
    "* __tol__: the convergence tolerance for iterative algorithms (default: 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these will be tuned in quick test\n",
    "breg_param = None\n",
    "bmax_iter = None\n",
    "\n",
    "# used to verify results\n",
    "assessdf = lyrics_pd_df[lyrics_pd_df['year'] == 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLinearRegressionPipelineModel(ncol, pcol, num_features=200, max_iter=10, reg_param=0.01):  \n",
    "    tok = Tokenizer(inputCol=ncol, outputCol=\"words\")\n",
    "    htf = HashingTF(inputCol=tok.getOutputCol(), outputCol=\"features\", numFeatures=num_features)\n",
    "    lr = LogisticRegression(labelCol=pcol, maxIter=max_iter, regParam=reg_param)\n",
    "    return Pipeline(stages=[tok, htf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLinearRegressionModelAndPredict(traindf, testdf, assessdf, ncol, pcol, pipeline_name=\"\", num_features=200,\n",
    "                                         max_iter=bmax_iter, reg_param=breg_param, verbose=False):\n",
    "    # build and fit the model\n",
    "    model = buildLinearRegressionPipelineModel(ncol, pcol, num_features=num_features, \n",
    "        max_iter=max_iter, reg_param=reg_param).fit(traindf.select(['song_key',ncol, pcol]))\n",
    "    \n",
    "    # Make predictions on test documents and print columns of interest.\n",
    "    prediction = model.transform(testdf.select(['song_key',ncol,pcol]))\n",
    "    selected = prediction.select(\"song_key\", ncol, \"prediction\")\n",
    "    \n",
    "    # build up predictions\n",
    "    pred_hits = {}\n",
    "    for row in selected.collect():\n",
    "        pred_hits[row[0]] = row[2]\n",
    "    \n",
    "    # return predictions assessment\n",
    "    return assessPredicts(assessdf, pred_hits, ncol, pcol, reg_param=reg_param, max_iter=max_iter, \n",
    "        pipeline_name=pipeline_name, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tuning / Quick Test\n",
    "**See how our most likely best performing regression will do, using synonyms with hypernym replacement as the vocab and 'top 50 versus bottom 50' as the labels. The outcome of this will be used as the tuned hyperparameters for `reg_param` and `max_iter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with\n",
      "\tncol: noun_syn_hype_vector\n",
      "\tpcol: is_top_50_percent\n",
      "\ttnum_features: 3215\n",
      "\treg_params: [0.001, 0.01, 0.1, 1.0, 10.0]\n",
      "\ttmax_iters: [10, 20, 30]\n"
     ]
    }
   ],
   "source": [
    "# quick test \n",
    "tpname = \"LRPipeline Predict 2014\"\n",
    "tncol = ncols[2] # most 'shrunken' vocab, synonyms with hypernym replacement\n",
    "tpcol = pbinarycols[2] # top 50 versus bottom 50\n",
    "\n",
    "tnum_features = len(all_words_dict[tncol]) #THIS IS THE KEY, GET THE FEATURES UP TO LEN DICT!!!\n",
    "treg_params = [.001, .01, .1, 1., 10.] #no value derived above 10 based on exploration\n",
    "tmax_iters = [10,20,30] # NO REAL GAINS BEYOND 10\n",
    "\n",
    "print \"testing with\\n\\tncol: {}\\n\\tpcol: {}\\n\\ttnum_features: {}\\n\\treg_params: {}\\n\\ttmax_iters: {}\".format(\n",
    "    tncol,tpcol,tnum_features,treg_params,tmax_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- reg_param: 0.001 ---\n",
      "For reg_param: 0.001 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 54.7368421053% success --> hits: 52, misses: 43\n",
      "For reg_param: 0.001 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 50.5263157895% success --> hits: 48, misses: 47\n",
      "For reg_param: 0.001 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 51.5789473684% success --> hits: 49, misses: 46\n",
      "\n",
      "--- reg_param: 0.01 ---\n",
      "For reg_param: 0.01 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 54.7368421053% success --> hits: 52, misses: 43\n",
      "For reg_param: 0.01 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 52.6315789474% success --> hits: 50, misses: 45\n",
      "For reg_param: 0.01 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 53.6842105263% success --> hits: 51, misses: 44\n",
      "\n",
      "--- reg_param: 0.1 ---\n",
      "For reg_param: 0.1 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "For reg_param: 0.1 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "For reg_param: 0.1 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "\n",
      "--- reg_param: 1.0 ---\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "For reg_param: 1.0 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "For reg_param: 1.0 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "\n",
      "--- reg_param: 10.0 ---\n",
      "For reg_param: 10.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n",
      "For reg_param: 10.0 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n",
      "For reg_param: 10.0 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n"
     ]
    }
   ],
   "source": [
    "# reset everything on subsequent runs\n",
    "breg_param = None\n",
    "bmax_iter = None\n",
    "tbpredicts = None\n",
    "\n",
    "# loop to tune hyper params for the best success rate\n",
    "for reg_param in treg_params:\n",
    "    print \"\\n--- reg_param: {} ---\".format(reg_param)\n",
    "    for max_iter in tmax_iters:\n",
    "      \n",
    "        ptuple = buildLinearRegressionModelAndPredict(traindf, testdf, assessdf, tncol, tpcol, pipeline_name=tpname,\n",
    "                         num_features=tnum_features, max_iter=max_iter, reg_param=reg_param, verbose=False)\n",
    "        \n",
    "        if not tbpredicts or ptuple[0] > tbpredicts[0]:\n",
    "            tbpredicts = ptuple\n",
    "            breg_param = reg_param\n",
    "            bmax_iter = max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Winners For Quick Test ---\n",
      "How did well did 'LRPipeline Predict 2014' predict 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label/col 'noun_syn_hype_vector'? best success rate: 61.0526315789% --> hits: 58, misses: 37\n",
      "\n",
      "Tuned best reg_param: 1.0, max_iter: 10, to be used in full processing\n"
     ]
    }
   ],
   "source": [
    "print \"--- Winners For Quick Test ---\"\n",
    "print \"How did well did '{}' predict '{}' for label/col '{}'? best success rate: {}% --> hits: {}, misses: {}\".format(        \n",
    "        tpname,pos_dict_descrips[tpcol],tncol,tbpredicts[0],tbpredicts[1],tbpredicts[2])\n",
    "print\n",
    "print \"Tuned best reg_param: {}, max_iter: {}, to be used in full processing\".format(breg_param,bmax_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Full Run Using Tuned Hyper-Params\n",
    "**Could have tuned each one separately, but the gains would have been minimal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
