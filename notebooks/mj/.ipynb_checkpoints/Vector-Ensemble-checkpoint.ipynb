{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vector Ensemble\n",
    "Ensemble approach using Spark. This notebook leverages the consolidated vector CSV which includes normal, synonym, and hypernym vectors, see [master-lyricsdf-word_syn_hype_vectors.csv](../../data/conditioned/master-lyricsdf-word_syn_hype_vectors.csv)\n",
    "\n",
    "While we understand that a song’s lyrics are not generally causal to where a given song might land on the charts, we had a hunch that there might at least be a correlation between a song’s position and its lyric content that we could model, train, and ultimately do some prediction. Having already explored Topic Modeling with Latent Factors as shown in the previous section, we wanted to separately focus on supervised learning techniques that would be more readily interpretable. Ultimately, we landed on predicting the ‘Top 50 versus Bottom 50’ positions on the 2014 charts, a balanced set of positives and negatives, also offering a binary prediction, where positives values indicate ‘In the Top 50’ and negative values indicate ‘In the Bottom 50’. We also wanted to again leverage the cluster computing framework Spark as we had used previously in establishing the Vocabularies. For prediction, we were interested in its Machine Learning APIs. After some trial, error, and tuning, we used Spark’s Pipeline API to apply Logistic Regression learning algorithm, or estimator, which turned out to be a solid choice for this exploration as it favors binary, balanced data. More in-depth information for our approach and results can be found at Vector Ensemble Notebook in addition to more cursory information in the project Master Process Notebook.\n",
    "\n",
    "First, we tuned and fit models against noun vectors derived from all 3 Vocabularies — Initial, Shrunken-1, and Shrunken-2 — training on all data from 1970-2013 using the tuned hyperparameters. Then we predicted ‘Top 50 versus Bottom 50’ exclusively for 2014 using the same models derived from the 3 vocabularies.\n",
    "\n",
    "At least to the degree that lyrics and position are correlated, the results of predicting the top / bottom splits are consistent with our intuitions. Initial vocabulary (vectorized to allow only each unique noun 1x max per song) was the least performant, slightly improved upon by model fit to Shrunken-1 (nouns with synonym replacement, also vectorized using same reduction rules as Initial), then a significant performance gain realized by Shrunken-2 noun vocabulary (synonyms with hypernym replacement, vectorized using same rules). Shrunken-2 correctly predicted 58 of 95 non-empty lyrics in 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(2),2)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Setup Data For Pipeline\n",
    "###Load Dataframe into Pandas for initial manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the lyrics from the approved \"master\" dataframe\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/conditioned/master-lyricsdf-word_syn_hype_vectors.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "      <th>lyrics_url</th>\n",
       "      <th>lyrics_abstract</th>\n",
       "      <th>noun_vector</th>\n",
       "      <th>adj_vector</th>\n",
       "      <th>noun_syn_vector</th>\n",
       "      <th>adj_syn_vector</th>\n",
       "      <th>noun_syn_hype_vector</th>\n",
       "      <th>adj_syn_hype_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-1</td>\n",
       "      <td>http://lyrics.wikia.com/Simon_And_Garfunkel:Br...</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "      <td>time bridge water</td>\n",
       "      <td>rough troubled</td>\n",
       "      <td>bridge time water</td>\n",
       "      <td>rough troubled</td>\n",
       "      <td>bridge time water</td>\n",
       "      <td>rough troubled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  position  year                                         title.href                       title               artist                                             lyrics  decade song_key                                         lyrics_url                                    lyrics_abstract        noun_vector      adj_vector    noun_syn_vector  adj_syn_vector noun_syn_hype_vector adj_syn_hype_vector\n",
       "0      0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...  Bridge over Troubled Water  Simon and Garfunkel  When you're weary. Feeling small. When tears a...    1970   1970-1  http://lyrics.wikia.com/Simon_And_Garfunkel:Br...  When you're weary. Feeling small. When tears a...  time bridge water  rough troubled  bridge time water  rough troubled    bridge time water      rough troubled"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Add Labels for Data based on position\n",
    "This will change based upon the current run. A straight-forward usage is to see how well top and bottom 50 can be predicted.\n",
    "**Note: Spark ML seems picky about `label` being the column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use positions for labeling\n",
    "pcols = ['bin_10_percent','bin_25_percent','is_top_10_percent','is_top_25_percent','is_top_50_percent']\n",
    "\n",
    "# linear regression only supports binary topics\n",
    "pbinarycols = ['is_top_10_percent','is_top_25_percent','is_top_50_percent'] \n",
    "\n",
    "pos_dict = {\n",
    "    'bin_10_percent': {\n",
    "      10.0:range(1,11),\n",
    "      20.0:range(11,21),\n",
    "      30.0:range(21,31),\n",
    "      40.0:range(31,41),\n",
    "      50.0:range(41,51),\n",
    "      60.0:range(51,61), \n",
    "      70.0:range(61,71),\n",
    "      80.0:range(71,81),\n",
    "      90.0:range(81,91),\n",
    "      100.0:range(91,101)  \n",
    "    }, 'bin_25_percent': {\n",
    "      25.0:range(1,26),\n",
    "      50.0:range(26,51),\n",
    "      75.0:range(51,76),\n",
    "      100.0:range(76,101)\n",
    "    }, 'is_top_10_percent': {\n",
    "      1.0:range(1,11),\n",
    "      0.0:range(11,101)            \n",
    "    }, 'is_top_25_percent': {\n",
    "      1.0:range(1,26),\n",
    "      0.0:range(26,101)\n",
    "    }, 'is_top_50_percent': {\n",
    "      1.0:range(1,51),\n",
    "      0.0:range(51,101)\n",
    "    }\n",
    "}\n",
    "\n",
    "pos_dict_descrips = {\n",
    "    'bin_10_percent': \"10 Percent Splits (10 Topics)\",\n",
    "    'bin_25_percent': \"25 Percent Splits (4 Topics)\",\n",
    "    'is_top_10_percent': \"Top 10 versus Bottom 90 (2 Topics, 1 means 'yes')\",\n",
    "    'is_top_25_percent': \"Top 25 versus Bottom 75 (2 Topics, 1 means 'yes')\",      \n",
    "    'is_top_50_percent': \"Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "      <th>lyrics_url</th>\n",
       "      <th>lyrics_abstract</th>\n",
       "      <th>noun_vector</th>\n",
       "      <th>adj_vector</th>\n",
       "      <th>noun_syn_vector</th>\n",
       "      <th>adj_syn_vector</th>\n",
       "      <th>noun_syn_hype_vector</th>\n",
       "      <th>adj_syn_hype_vector</th>\n",
       "      <th>bin_10_percent</th>\n",
       "      <th>is_top_50_percent</th>\n",
       "      <th>is_top_25_percent</th>\n",
       "      <th>bin_25_percent</th>\n",
       "      <th>is_top_10_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>3671</td>\n",
       "      <td>72</td>\n",
       "      <td>2006</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Too_Little_Too_Late</td>\n",
       "      <td>Too Little Too Late</td>\n",
       "      <td>JoJo</td>\n",
       "      <td>Come with me. Stay the night. Just say the wor...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2006-72</td>\n",
       "      <td>http://lyrics.wikia.com/JoJo:Too_Little_Too_Late</td>\n",
       "      <td>Come with me. Stay the night. Just say the wor...</td>\n",
       "      <td>ya game time thing love chase</td>\n",
       "      <td>little late strong right young chase</td>\n",
       "      <td>game love pursuit thing time</td>\n",
       "      <td>late right small strong young</td>\n",
       "      <td>game love pursuit thing time</td>\n",
       "      <td>late right small strong young</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>18</td>\n",
       "      <td>1980</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Upside_Down</td>\n",
       "      <td>Upside Down</td>\n",
       "      <td>Diana Ross</td>\n",
       "      <td>I said upside down. You're turning me. You're ...</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980-18</td>\n",
       "      <td>http://lyrics.wikia.com/Diana_Ross:Upside_Down</td>\n",
       "      <td>I said upside down. You're turning me. You're ...</td>\n",
       "      <td>cheatin</td>\n",
       "      <td>aware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aware</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>3471</td>\n",
       "      <td>72</td>\n",
       "      <td>2004</td>\n",
       "      <td>https://en.wikipedia.org/wiki/U_Should%27ve_Kn...</td>\n",
       "      <td>U Should've Known Better</td>\n",
       "      <td>Monica</td>\n",
       "      <td>Oh, oh. Lalalalala. I didn't ask to go with yo...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2004-72</td>\n",
       "      <td>http://lyrics.wikia.com/Monica:U_Should%27ve_K...</td>\n",
       "      <td>Oh, oh. Lalalalala. I didn't ask to go with yo...</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>interlude</td>\n",
       "      <td>musical</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  position  year                                         title.href                     title      artist                                             lyrics  decade song_key                                         lyrics_url                                    lyrics_abstract                    noun_vector                            adj_vector               noun_syn_vector                 adj_syn_vector          noun_syn_hype_vector            adj_syn_hype_vector  bin_10_percent  \\\n",
       "3671   3671        72  2006  https://en.wikipedia.org/wiki/Too_Little_Too_Late       Too Little Too Late        JoJo  Come with me. Stay the night. Just say the wor...    2000  2006-72   http://lyrics.wikia.com/JoJo:Too_Little_Too_Late  Come with me. Stay the night. Just say the wor...  ya game time thing love chase  little late strong right young chase  game love pursuit thing time  late right small strong young  game love pursuit thing time  late right small strong young              80   \n",
       "1017   1017        18  1980          https://en.wikipedia.org/wiki/Upside_Down               Upside Down  Diana Ross  I said upside down. You're turning me. You're ...    1980  1980-18     http://lyrics.wikia.com/Diana_Ross:Upside_Down  I said upside down. You're turning me. You're ...                        cheatin                                 aware                           NaN                          aware                           NaN                          aware              20   \n",
       "3471   3471        72  2004  https://en.wikipedia.org/wiki/U_Should%27ve_Kn...  U Should've Known Better      Monica  Oh, oh. Lalalalala. I didn't ask to go with yo...    2000  2004-72  http://lyrics.wikia.com/Monica:U_Should%27ve_K...  Oh, oh. Lalalalala. I didn't ask to go with yo...                      interlude                               musical                     interlude                        musical                     interlude                        musical              80   \n",
       "\n",
       "      is_top_50_percent  is_top_25_percent  bin_25_percent  is_top_10_percent  \n",
       "3671                  0                  0              75                  0  \n",
       "1017                  1                  1              25                  0  \n",
       "3471                  0                  0              75                  0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def labelForPosition(pos,pos_dict_key):\n",
    "    for k,p in pos_dict[pos_dict_key].iteritems():\n",
    "        if pos in p:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "#label is position\n",
    "for pos_dict_key in pos_dict.keys():\n",
    "    lyrics_pd_df[pos_dict_key] = lyrics_pd_df.position.apply(lambda p : labelForPosition(p,pos_dict_key))\n",
    "\n",
    "#sanity check\n",
    "lyrics_pd_df.sample(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filter out Non-Lyric Records\n",
    "**Non-Lyrics due to:**\n",
    "* Instrumentals\n",
    "* Licensing restrictions on lyrics.wikia\n",
    "* No lyrics added to lyrics.wikia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the noun cols\n",
    "ncols = ['noun_vector','noun_syn_vector','noun_syn_hype_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many empty lyrics are there? 159\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls (which may include instrumentals or otherwise unavailable )\n",
    "print \"How many empty lyrics are there? {}\".format(len(np.where(pd.isnull(lyrics_pd_df[['lyrics']]))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out null lyrics\n",
    "lyrics_pd_df = lyrics_pd_df.dropna(axis=0, how='any', thresh=None, subset=['lyrics'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4341, 22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after removing noun_vector empties --> (4121, 22)\n",
      "shape after removing noun_syn_vector empties --> (4105, 22)\n",
      "shape after removing noun_syn_hype_vector empties --> (4105, 22)\n"
     ]
    }
   ],
   "source": [
    "# ALSO NEED TO REMOVE EMPTY NCOL ROWS RESULTING FROM VOCAB SHRINKAGE OPERATIONS\n",
    "for col in ncols:\n",
    "    lyrics_pd_df = lyrics_pd_df.dropna(axis=0, how='any', thresh=None, subset=[col], inplace=False)\n",
    "    print \"shape after removing {} empties --> {}\".format(col,lyrics_pd_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ROWS after removing empties --> 4105\n"
     ]
    }
   ],
   "source": [
    "print \"Final ROWS after removing empties -->\", lyrics_pd_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up Master Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUniqueWordsSorted(df,word_col):\n",
    "    u = []\n",
    "    for r in df.iterrows():\n",
    "        ws = r[1][word_col]\n",
    "        if not isinstance(ws,float):\n",
    "            vs = ws.split()          \n",
    "            for v in vs:\n",
    "                if not v in u:\n",
    "                    u.append(v)\n",
    "        \n",
    "    return sorted(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- noun_vector ---\n",
      "\n",
      "\tHow long is dict? 5138\n",
      "\tWhat are the first 5 entries? [['60', '8-bit', '>jeep', '>mayback', '\\\\n|officialsite']]\n",
      "\n",
      "--- noun_syn_vector ---\n",
      "\n",
      "\tHow long is dict? 3230\n",
      "\tWhat are the first 5 entries? [['abdomen', 'ability', 'abnormality', 'abortion', 'abrasion']]\n",
      "\n",
      "--- noun_syn_hype_vector ---\n",
      "\n",
      "\tHow long is dict? 3215\n",
      "\tWhat are the first 5 entries? [['abdomen', 'ability', 'abnormality', 'abortion', 'abrasion']]\n"
     ]
    }
   ],
   "source": [
    "# this will be the master columns to populate for the matrix\n",
    "all_words_dict = {}\n",
    "\n",
    "for col in ncols:\n",
    "    print \"\\n--- {} ---\\n\".format(col)\n",
    "    all_words_dict[col] = getUniqueWordsSorted(lyrics_pd_df, col)\n",
    "    print \"\\tHow long is dict? {}\".format(len(all_words_dict[col]))\n",
    "    print \"\\tWhat are the first 5 entries? [{}]\".format(all_words_dict[col][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareWordLists(alist,blist):\n",
    "    same = []\n",
    "    ina = []\n",
    "    inb = []\n",
    "    \n",
    "    for a in alist:\n",
    "        if a in blist:\n",
    "            same.append(a)\n",
    "        else:\n",
    "            ina.append(a)\n",
    "    \n",
    "    for b in blist:\n",
    "        if b not in same:\n",
    "            inb.append(b)\n",
    "    return sorted(same), sorted(ina), sorted(inb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For syn versus syn-hype words...\n",
      "How many are same?  3215\n",
      "How many are only in syn?  15\n",
      "How many are only in hype?  0\n"
     ]
    }
   ],
   "source": [
    "tcomp = compareWordLists(all_words_dict[ncols[1]],all_words_dict[ncols[2]])\n",
    "print \"For syn versus syn-hype words...\"\n",
    "print \"How many are same? \", len(tcomp[0])\n",
    "print \"How many are only in syn? \", len(tcomp[1])\n",
    "print \"How many are only in hype? \", len(tcomp[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up dictionaries for string and vectorized data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert array of strings to array of arrays\n",
    "def stringsToUniqueVectors(strings):\n",
    "    vectors = []\n",
    "    for s in strings:\n",
    "        # test for NaN\n",
    "        if not isinstance(s,float):\n",
    "            tmp = s.split()\n",
    "            cs = []\n",
    "            for t in tmp:\n",
    "                if not t in cs:\n",
    "                    cs.append(t)\n",
    "            vectors.append(sorted(cs))\n",
    "        # just in case, handle empty    \n",
    "        else:\n",
    "            vectors.append([])\n",
    "            \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict for k=col, v=array of strings\n",
    "nstrings = {}\n",
    "\n",
    "# dict for k=col, v=array of arrays\n",
    "nvectors = {}\n",
    "\n",
    "# initialize\n",
    "for col in ncols:\n",
    "    nstrings[col] = []\n",
    "    nvectors[col] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# populate\n",
    "for col in ncols:\n",
    "    nstrings[col] = lyrics_pd_df[col].values\n",
    "    nvectors[col] = stringsToUniqueVectors(nstrings[col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`noun_vector`[32]: folk patch papa man wonder time money crop daddy rule chicken wood day rain family angel mama brand home -->\n",
      "\t['angel', 'brand', 'chicken', 'crop', 'daddy', 'day', 'family', 'folk', 'home', 'mama', 'man', 'money', 'papa', 'patch', 'rain', 'rule', 'time', 'wonder', 'wood']\n",
      "`noun_syn_vector`[32]: angel chicken crop dad day family folk home ma man money rain rule spot time trade_name wonder wood -->\n",
      "\t['angel', 'chicken', 'crop', 'dad', 'day', 'family', 'folk', 'home', 'ma', 'man', 'money', 'rain', 'rule', 'spot', 'time', 'trade_name', 'wonder', 'wood']\n",
      "`noun_syn_hype_vector`[32]: angel chicken crop dad day family folk home ma man money rain rule time topographic_point trade_name wonder wood -->\n",
      "\t['angel', 'chicken', 'crop', 'dad', 'day', 'family', 'folk', 'home', 'ma', 'man', 'money', 'rain', 'rule', 'time', 'topographic_point', 'trade_name', 'wonder', 'wood']\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "for col in ncols:\n",
    "    idx = 32\n",
    "    print \"`{}`[{}]: {} -->\\n\\t{}\".format(col,idx,nstrings[col][idx],nvectors[col][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Convert and manipulate with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "|index|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|          lyrics_url|     lyrics_abstract|         noun_vector|          adj_vector|     noun_syn_vector|      adj_syn_vector|noun_syn_hype_vector| adj_syn_hype_vector|bin_10_percent|is_top_50_percent|is_top_25_percent|bin_25_percent|is_top_10_percent|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "|    0|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|http://lyrics.wik...|When you're weary...|   time bridge water|      rough troubled|   bridge time water|      rough troubled|   bridge time water|      rough troubled|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "|    1|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|Why do birds sudd...|  1970|  1970-2|http://lyrics.wik...|Why do birds sudd...| dream starlight eye|           true blue| dream eye starlight|           blue true| dream eye starlight|           blue true|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "|    2|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|Mmm, da da da. Mm...|  1970|  1970-3|http://lyrics.wik...|Mmm, da da da. Mm...|woman mess mind m...|american importan...|crap light ma mes...|american colored ...|crap light ma mes...|american colored ...|          10.0|              1.0|              1.0|          25.0|              1.0|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----------------+-----------------+--------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pipeline Using Spark\n",
    "Reference [combine all features into a single feature vector](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)\n",
    "![Ensemble Pipeline Overview](https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png)\n",
    "* Tokenizer\n",
    "* HashingTF\n",
    "* Word2Vec\n",
    "* OneHotEncoder\n",
    "* Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline adapted from:\n",
    "# http://spark.apache.org/docs/latest/ml-guide.html\n",
    "# https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used in common for assessing and printing predictions versus actuals\n",
    "def assessPredicts(predictsdf,pred_hits,ncol,pcol,pipeline_name=\"Pipeline\",reg_param=None, max_iter=None, verbose=False):\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    print \"For reg_param: {} & max_iter: {}, how did pipeline '{}' do predicting pcol '{}' for label '{}'?\".format(\n",
    "        (reg_param if reg_param else \"<unspecified>\"),\n",
    "        (max_iter if max_iter else \"<unspecified>\"),\n",
    "        pipeline_name,pos_dict_descrips[pcol],ncol)\n",
    "    \n",
    "    for r in predictsdf.iterrows():\n",
    "        song_key = r[1].song_key\n",
    "        pred = pred_hits[song_key]  \n",
    "        result = labelForPosition(r[1].position, pcol)\n",
    "        correct = result == pred\n",
    "        if correct:\n",
    "            hits +=1\n",
    "            if verbose:\n",
    "                print \"Correct ::: song_key --> {}, predicted {}\".format(song_key, pred)\n",
    "        else:\n",
    "            misses +=1\n",
    "            if verbose:\n",
    "                print \"Incorrect ::: song_key --> {}, predicted {}\".format(song_key, pred)\n",
    "    hitperc = float(hits)/float(hits+misses)*100.                \n",
    "    print \"'{}' {}% success --> hits: {}, misses: {}\".format(pipeline_name,hitperc,hits,misses)\n",
    "    \n",
    "    return hitperc,hits,misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fit Linear Regression Models to songs prior to 2013 and predict on 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## establish training and test Spark Dataframes (to be filtered further as needed)\n",
    "## e.g. traindf.select(['song_key',ncol,pcol])\n",
    "\n",
    "# training on songs to 2013 (uses nstrings)\n",
    "traindf = lyricsdf.filter(lyricsdf['year'] != 2014)\n",
    "\n",
    "# test year 2014 (uses nstrings)\n",
    "testdf = lyricsdf.filter(lyricsdf['year'] == 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pipeline  :  Linear Regression\n",
    "**Here is the output of** \n",
    "```python\n",
    "LogisticRegression().explainParams()\n",
    "```\n",
    "* __elasticNetParam__: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
    "* __featuresCol__: features column name (default: features)\n",
    "* __fitIntercept__: whether to fit an intercept term. (default: True)\n",
    "* __labelCol__: label column name (default: label)\n",
    "* __maxIter__: max number of iterations (>= 0) (default: 100)\n",
    "* __predictionCol__: prediction column name (default: prediction)\n",
    "* __probabilityCol__: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
    "* __rawPredictionCol__: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
    "* __regParam__: regularization parameter (>= 0) (default: 0.1)\n",
    "* __threshold__: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match. (default: 0.5)\n",
    "* __thresholds__: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
    "* __tol__: the convergence tolerance for iterative algorithms (default: 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these will be tuned in quick test\n",
    "breg_param = None\n",
    "bmax_iter = None\n",
    "\n",
    "# used to verify results\n",
    "assessdf = lyrics_pd_df[lyrics_pd_df['year'] == 2014]\n",
    "\n",
    "#used for the pipeline name\n",
    "pipelinename = \"LRPipeline Predict 2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLinearRegressionPipelineModel(ncol, pcol, num_features=200, max_iter=10, reg_param=0.01):  \n",
    "    tok = Tokenizer(inputCol=ncol, outputCol=\"words\")\n",
    "    htf = HashingTF(inputCol=tok.getOutputCol(), outputCol=\"features\", numFeatures=num_features)\n",
    "    lr = LogisticRegression(labelCol=pcol, maxIter=max_iter, regParam=reg_param)\n",
    "    return Pipeline(stages=[tok, htf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model not provided, it will be built and fit\n",
    "def predictLinearRegressionModel(ncol, pcol,\n",
    "                                 traindf=traindf, testdf=testdf, assessdf=assessdf,\n",
    "                                 pipeline_name=pipelinename, verbose=False, provided_model=None,\n",
    "                                 num_features=200, max_iter=bmax_iter, reg_param=breg_param):\n",
    "    # use provided or build and fit a new model\n",
    "    model = provided_model\n",
    "    if not model:\n",
    "        model = buildLinearRegressionPipelineModel(ncol, pcol, num_features=num_features, \n",
    "            max_iter=max_iter, reg_param=reg_param).fit(traindf.select(['song_key',ncol, pcol]))\n",
    "    \n",
    "    # Make predictions on test documents and print columns of interest.\n",
    "    prediction = model.transform(testdf.select(['song_key',ncol,pcol]))\n",
    "    selected = prediction.select(\"song_key\", ncol, \"prediction\")\n",
    "    \n",
    "    # build up predictions\n",
    "    pred_hits = {}\n",
    "    for row in selected.collect():\n",
    "        pred_hits[row[0]] = row[2]\n",
    "    \n",
    "    # return 2tuple t: t[0]=(model,prediction,selection) and t[1]= (hitperc,hits,misses)    \n",
    "    return (model, prediction, selected), assessPredicts(assessdf, pred_hits, ncol, pcol, reg_param=reg_param, max_iter=max_iter, \n",
    "        pipeline_name=pipeline_name, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tuning / Quick Test\n",
    "**See how our most likely best performing regression will do, using synonyms with hypernym replacement as the vocab and 'top 50 versus bottom 50' as the labels. The outcome of this will be used as the tuned hyperparameters for `reg_param` and `max_iter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with\n",
      "\tncol: noun_syn_hype_vector\n",
      "\tpcol: is_top_50_percent\n",
      "\ttnum_features: 3215\n",
      "\treg_params: [0.001, 0.01, 0.1, 1.0, 10.0]\n",
      "\ttmax_iters: [10, 20, 30]\n"
     ]
    }
   ],
   "source": [
    "# quick test \n",
    "tncol = ncols[2] # most 'shrunken' vocab, synonyms with hypernym replacement\n",
    "tpcol = pbinarycols[2] # top 50 versus bottom 50\n",
    "\n",
    "tnum_features = len(all_words_dict[tncol]) #THIS IS THE KEY, GET THE FEATURES UP TO LEN DICT!!!\n",
    "treg_params = [.001, .01, .1, 1., 10.] #no value derived above 10 based on exploration\n",
    "tmax_iters = [10,20,30] # NO REAL GAINS BEYOND 10\n",
    "\n",
    "print \"testing with\\n\\tncol: {}\\n\\tpcol: {}\\n\\ttnum_features: {}\\n\\treg_params: {}\\n\\ttmax_iters: {}\".format(\n",
    "    tncol,tpcol,tnum_features,treg_params,tmax_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- reg_param: 0.001 ---\n",
      "For reg_param: 0.001 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 54.7368421053% success --> hits: 52, misses: 43\n",
      "For reg_param: 0.001 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 50.5263157895% success --> hits: 48, misses: 47\n",
      "For reg_param: 0.001 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 51.5789473684% success --> hits: 49, misses: 46\n",
      "\n",
      "--- reg_param: 0.01 ---\n",
      "For reg_param: 0.01 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 54.7368421053% success --> hits: 52, misses: 43\n",
      "For reg_param: 0.01 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 52.6315789474% success --> hits: 50, misses: 45\n",
      "For reg_param: 0.01 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 53.6842105263% success --> hits: 51, misses: 44\n",
      "\n",
      "--- reg_param: 0.1 ---\n",
      "For reg_param: 0.1 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "For reg_param: 0.1 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "For reg_param: 0.1 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 58.9473684211% success --> hits: 56, misses: 39\n",
      "\n",
      "--- reg_param: 1.0 ---\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "For reg_param: 1.0 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "For reg_param: 1.0 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "\n",
      "--- reg_param: 10.0 ---\n",
      "For reg_param: 10.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n",
      "For reg_param: 10.0 & max_iter: 20, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n",
      "For reg_param: 10.0 & max_iter: 30, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 56.8421052632% success --> hits: 54, misses: 41\n"
     ]
    }
   ],
   "source": [
    "# reset everything on subsequent runs\n",
    "breg_param = None\n",
    "bmax_iter = None\n",
    "tbpredicts = None\n",
    "\n",
    "# loop to tune hyper params for the best success rate\n",
    "for reg_param in treg_params:\n",
    "    print \"\\n--- reg_param: {} ---\".format(reg_param)\n",
    "    for max_iter in tmax_iters:\n",
    "      \n",
    "        t = predictLinearRegressionModel(tncol, tpcol, num_features=tnum_features,\n",
    "                                         max_iter=max_iter, reg_param=reg_param)\n",
    "        \n",
    "        ptuple = t[1] # just want the prediction tuple        \n",
    "        if not tbpredicts or ptuple[0] > tbpredicts[0]:\n",
    "            tbpredicts = ptuple\n",
    "            breg_param = reg_param\n",
    "            bmax_iter = max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Winners For Quick Test ---\n",
      "How did well did 'LRPipeline Predict 2014' predict 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label/col 'noun_syn_hype_vector'? best success rate: 61.0526315789% --> hits: 58, misses: 37\n",
      "\n",
      "Tuned best reg_param: 1.0, max_iter: 10, to be used in full processing\n"
     ]
    }
   ],
   "source": [
    "print \"--- Winners For Quick Test ---\"\n",
    "print \"How did well did '{}' predict '{}' for label/col '{}'? best success rate: {}% --> hits: {}, misses: {}\".format(        \n",
    "        tpname,pos_dict_descrips[tpcol],tncol,tbpredicts[0],tbpredicts[1],tbpredicts[2])\n",
    "print\n",
    "print \"Tuned best reg_param: {}, max_iter: {}, to be used in full processing\".format(breg_param,bmax_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Full Run Using Tuned Hyper-Params\n",
    "**Could have tuned each one separately, but the gains would have been minimal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Tue, 08 Dec 2015 17:33:17\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ncol: noun_vector ---\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 10 versus Bottom 90 (2 Topics, 1 means 'yes')' for label 'noun_vector'?\n",
      "'LRPipeline Predict 2014' 89.4736842105% success --> hits: 85, misses: 10\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 25 versus Bottom 75 (2 Topics, 1 means 'yes')' for label 'noun_vector'?\n",
      "'LRPipeline Predict 2014' 76.8421052632% success --> hits: 73, misses: 22\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_vector'?\n",
      "'LRPipeline Predict 2014' 49.4736842105% success --> hits: 47, misses: 48\n",
      "\n",
      "--- ncol: noun_syn_vector ---\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 10 versus Bottom 90 (2 Topics, 1 means 'yes')' for label 'noun_syn_vector'?\n",
      "'LRPipeline Predict 2014' 89.4736842105% success --> hits: 85, misses: 10\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 25 versus Bottom 75 (2 Topics, 1 means 'yes')' for label 'noun_syn_vector'?\n",
      "'LRPipeline Predict 2014' 76.8421052632% success --> hits: 73, misses: 22\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_vector'?\n",
      "'LRPipeline Predict 2014' 50.5263157895% success --> hits: 48, misses: 47\n",
      "\n",
      "--- ncol: noun_syn_hype_vector ---\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 10 versus Bottom 90 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 89.4736842105% success --> hits: 85, misses: 10\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 25 versus Bottom 75 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 77.8947368421% success --> hits: 74, misses: 21\n",
      "For reg_param: 1.0 & max_iter: 10, how did pipeline 'LRPipeline Predict 2014' do predicting pcol 'Top 50 versus Bottom 50 (2 Topics, 1 means 'yes')' for label 'noun_syn_hype_vector'?\n",
      "'LRPipeline Predict 2014' 61.0526315789% success --> hits: 58, misses: 37\n",
      "CPU times: user 254 ms, sys: 396 ms, total: 651 ms\n",
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_results = {}\n",
    "\n",
    "# loop over ncols and pcols to build and predict using tuned hyper params\n",
    "for ncol in ncols:\n",
    "    print \"\\n--- ncol: {} ---\".format(ncol)\n",
    "    full_results[ncol] = {}\n",
    "    for pcol in pbinarycols:            \n",
    "        full_results[ncol][pcol] = predictLinearRegressionModel(ncol, pcol, num_features=len(all_words_dict[ncol]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Results are as anticipated:\n",
    "1. `syn_hype_vector`\n",
    "1. `syn_vector`\n",
    "1. `noun_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(song_key=u'2014-1', noun_syn_hype_vector=u'air balloon love news space turn', prediction=1.0),\n",
       " Row(song_key=u'2014-2', noun_syn_hype_vector=u'bedroom deep-freeze heart horse love ma male_child narrative pie pot storm', prediction=0.0),\n",
       " Row(song_key=u'2014-3', noun_syn_hype_vector=u'blue distraction drive head imperfection mind mouth mystery rhythm', prediction=0.0),\n",
       " Row(song_key=u'2014-4', noun_syn_hype_vector=u'bitch department drop flow girl gold hand hater lane star swagman taste thing truth', prediction=1.0),\n",
       " Row(song_key=u'2014-5', noun_syn_hype_vector=u'dollar thing word', prediction=1.0),\n",
       " Row(song_key=u'2014-6', noun_syn_hype_vector=u'cast class conversation cunt damn flight girl lap lipstick loot pass penis pet seat sexual_activity sphere talk telephone', prediction=0.0),\n",
       " Row(song_key=u'2014-7', noun_syn_hype_vector=u'answer fortune friend man suit', prediction=1.0),\n",
       " Row(song_key=u'2014-8', noun_syn_hype_vector=u'bass bitch boom crap debris doll figure inch loot male_child night playing silicone size topographic_point turn', prediction=0.0),\n",
       " Row(song_key=u'2014-9', noun_syn_hype_vector=u'baby life male_child money thing time topographic_point', prediction=1.0),\n",
       " Row(song_key=u'2014-10', noun_syn_hype_vector=u'base control expression self', prediction=1.0),\n",
       " Row(song_key=u'2014-11', noun_syn_hype_vector=u'airplane dance hotel lumber swing thing', prediction=1.0),\n",
       " Row(song_key=u'2014-13', noun_syn_hype_vector=u'beat foot girlfriend hair heartbreaker imposter interruption liar universe', prediction=0.0),\n",
       " Row(song_key=u'2014-14', noun_syn_hype_vector=u'box manner', prediction=1.0),\n",
       " Row(song_key=u'2014-16', noun_syn_hype_vector=u'bed cause child civilian day fathead friend leaf lottery monster nut path struggle vision', prediction=1.0),\n",
       " Row(song_key=u'2014-18', noun_syn_hype_vector=u'eye', prediction=1.0),\n",
       " Row(song_key=u'2014-19', noun_syn_hype_vector=u'ceiling feeling heart love', prediction=1.0),\n",
       " Row(song_key=u'2014-20', noun_syn_hype_vector=u'address ball buzz care envy gown hotel kind room stain town zip_code', prediction=1.0),\n",
       " Row(song_key=u'2014-21', noun_syn_hype_vector=u'blast distance fractal girl idea limit past right rule soul', prediction=1.0),\n",
       " Row(song_key=u'2014-22', noun_syn_hype_vector=u'eye', prediction=1.0),\n",
       " Row(song_key=u'2014-23', noun_syn_hype_vector=u'blood card day eye fold inside light', prediction=0.0),\n",
       " Row(song_key=u'2014-24', noun_syn_hype_vector=u'cage day foot heart inside land love narrative night time', prediction=1.0),\n",
       " Row(song_key=u'2014-25', noun_syn_hype_vector=u'life time', prediction=1.0),\n",
       " Row(song_key=u'2014-26', noun_syn_hype_vector=u'attraction baby cat game liking mirror mouse time universe widow', prediction=0.0),\n",
       " Row(song_key=u'2014-27', noun_syn_hype_vector=u'bottle cause girl kitten mind night pot wood', prediction=0.0),\n",
       " Row(song_key=u'2014-28', noun_syn_hype_vector=u'clasp', prediction=1.0),\n",
       " Row(song_key=u'2014-29', noun_syn_hype_vector=u'life map night taste time topographic_point', prediction=0.0),\n",
       " Row(song_key=u'2014-30', noun_syn_hype_vector=u'baby bitch bone breast chain change dick drug girl holmium karat loot nigger weed', prediction=0.0),\n",
       " Row(song_key=u'2014-31', noun_syn_hype_vector=u'cloud control day dream life star window', prediction=1.0),\n",
       " Row(song_key=u'2014-32', noun_syn_hype_vector=u'brand dad end fun high home life make-believe manner mind money pain person play resort_area time', prediction=0.0),\n",
       " Row(song_key=u'2014-33', noun_syn_hype_vector=u'leaf sky', prediction=1.0),\n",
       " Row(song_key=u'2014-34', noun_syn_hype_vector=u'blue gold knock moon picture silver', prediction=1.0),\n",
       " Row(song_key=u'2014-35', noun_syn_hype_vector=u'ailment baseball_club bitch body car dad eye height light liquor love male_child night rag sheet surfing turn voice wash', prediction=1.0),\n",
       " Row(song_key=u'2014-36', noun_syn_hype_vector=u'baseball_club bitch buttocks crap hell little money nigger pot tower trader', prediction=1.0),\n",
       " Row(song_key=u'2014-37', noun_syn_hype_vector=u'baby body fever hand heart hell highway', prediction=1.0),\n",
       " Row(song_key=u'2014-38', noun_syn_hype_vector=u'en tequila', prediction=1.0),\n",
       " Row(song_key=u'2014-39', noun_syn_hype_vector=u'light match race sleeping space star', prediction=1.0),\n",
       " Row(song_key=u'2014-40', noun_syn_hype_vector=u'baby bath butt cake damn dip future topographic_point working', prediction=0.0),\n",
       " Row(song_key=u'2014-41', noun_syn_hype_vector=u'brand heart mission peace scene shame shooting simplicity', prediction=1.0),\n",
       " Row(song_key=u'2014-42', noun_syn_hype_vector=u'body cause dick fool friend game girl minute nigger number shower summer think', prediction=0.0),\n",
       " Row(song_key=u'2014-43', noun_syn_hype_vector=u'attempt buttocks detention ex-husband land man minute night set tomorrow type', prediction=0.0),\n",
       " Row(song_key=u'2014-44', noun_syn_hype_vector=u'heart', prediction=1.0),\n",
       " Row(song_key=u'2014-45', noun_syn_hype_vector=u'beating concern day dream feel heart knife love mind money pair shoulder sky star stop thing time topographic_point universe waste wing', prediction=1.0),\n",
       " Row(song_key=u'2014-46', noun_syn_hype_vector=u'band bee cause lion point', prediction=1.0),\n",
       " Row(song_key=u'2014-47', noun_syn_hype_vector=u'fish fun pond time trouble universe', prediction=1.0),\n",
       " Row(song_key=u'2014-48', noun_syn_hype_vector=u'cast cause relief remainder universe', prediction=0.0),\n",
       " Row(song_key=u'2014-49', noun_syn_hype_vector=u'baby bend drink drive kiss mix seat shotgun thump truck universe wheel window', prediction=0.0),\n",
       " Row(song_key=u'2014-50', noun_syn_hype_vector=u'baby class diamond girl life manner plastic school time universe', prediction=0.0),\n",
       " Row(song_key=u'2014-51', noun_syn_hype_vector=u'position', prediction=1.0),\n",
       " Row(song_key=u'2014-52', noun_syn_hype_vector=u'floor friend fun heart hotel sheet topographic_point trouble', prediction=1.0),\n",
       " Row(song_key=u'2014-53', noun_syn_hype_vector=u'air check hand tonight waitress', prediction=0.0),\n",
       " Row(song_key=u'2014-54', noun_syn_hype_vector=u'argon asshole ball bark bash bitch block body bull bullet bully cash cat check class crack crap cunt dad day difference dog drop gun hole house kennel kink man manner money neck nigger pound problem run shooting squad taw thing trap week', prediction=0.0),\n",
       " Row(song_key=u'2014-55', noun_syn_hype_vector=u'emotion girl love thing', prediction=1.0),\n",
       " Row(song_key=u'2014-56', noun_syn_hype_vector=u'act evening kindness lady love sofa time', prediction=0.0),\n",
       " Row(song_key=u'2014-57', noun_syn_hype_vector=u'age right', prediction=1.0),\n",
       " Row(song_key=u'2014-58', noun_syn_hype_vector=u'act bean biscuit bitch blame block bucket business buttocks case crack crap day drink drive figure finger fink gravy gun_trigger home_plate house kick lake love loyalty manner necktie nigger one-half pas reason room smoke suit thing wall weight word', prediction=1.0),\n",
       " Row(song_key=u'2014-59', noun_syn_hype_vector=u'car child lane life line smile', prediction=0.0),\n",
       " Row(song_key=u'2014-60', noun_syn_hype_vector=u'blood brother road sister water', prediction=1.0),\n",
       " Row(song_key=u'2014-61', noun_syn_hype_vector=u'brassiere buttocks drum eye', prediction=1.0),\n",
       " Row(song_key=u'2014-63', noun_syn_hype_vector=u'baby bed crawl girl line mind opportunity writing', prediction=0.0),\n",
       " Row(song_key=u'2014-64', noun_syn_hype_vector=u'cup dance groove male_child song sugar', prediction=1.0),\n",
       " Row(song_key=u'2014-65', noun_syn_hype_vector=u'baby backseat bellow bitch brand choice cunt drink fast gun knock male_child manner mouth nigger noise ring tree', prediction=0.0),\n",
       " Row(song_key=u'2014-66', noun_syn_hype_vector=u'clay day elm fortune road shade smile tax_return time', prediction=0.0),\n",
       " Row(song_key=u'2014-67', noun_syn_hype_vector=u'cause soul', prediction=1.0),\n",
       " Row(song_key=u'2014-68', noun_syn_hype_vector=u'burning drink girl gun ma south state thing time toast town', prediction=0.0),\n",
       " Row(song_key=u'2014-69', noun_syn_hype_vector=u'oven time', prediction=0.0),\n",
       " Row(song_key=u'2014-70', noun_syn_hype_vector=u'heart life love start tonight', prediction=1.0),\n",
       " Row(song_key=u'2014-71', noun_syn_hype_vector=u'animal', prediction=0.0),\n",
       " Row(song_key=u'2014-72', noun_syn_hype_vector=u'baby bitch cause coffin college cunt day doctor gas league ma nigger night red satan shark time white', prediction=0.0),\n",
       " Row(song_key=u'2014-73', noun_syn_hype_vector=u'baby base bug bus child dog dust fence home house jean light manner map midnight park point room school seat sofa stop truck uptown wire zapper', prediction=0.0),\n",
       " Row(song_key=u'2014-74', noun_syn_hype_vector=u'brave', prediction=0.0),\n",
       " Row(song_key=u'2014-75', noun_syn_hype_vector=u'hand minute short topographic_point waist', prediction=0.0),\n",
       " Row(song_key=u'2014-76', noun_syn_hype_vector=u'baby gold lane light right road sky streetlight train wind window', prediction=1.0),\n",
       " Row(song_key=u'2014-77', noun_syn_hype_vector=u'baby cause diamond fire nigger series thing universe watch', prediction=1.0),\n",
       " Row(song_key=u'2014-78', noun_syn_hype_vector=u'baby life tonight', prediction=1.0),\n",
       " Row(song_key=u'2014-79', noun_syn_hype_vector=u'airplane airport aisle attention champagne chap couple drive flight glass idea limousine stewardess ticket', prediction=0.0),\n",
       " Row(song_key=u'2014-80', noun_syn_hype_vector=u'appetite balloon birthday time', prediction=0.0),\n",
       " Row(song_key=u'2014-81', noun_syn_hype_vector=u'adieu bartender boot dress leather rock shooting sky tonight', prediction=0.0),\n",
       " Row(song_key=u'2014-82', noun_syn_hype_vector=u'messiah theory word', prediction=1.0),\n",
       " Row(song_key=u'2014-83', noun_syn_hype_vector=u'bitch buttocks girl guy topographic_point', prediction=1.0),\n",
       " Row(song_key=u'2014-84', noun_syn_hype_vector=u'agenda baseball_club crap flight invitation life morning night party shooting trip', prediction=1.0),\n",
       " Row(song_key=u'2014-85', noun_syn_hype_vector=u'memory', prediction=1.0),\n",
       " Row(song_key=u'2014-86', noun_syn_hype_vector=u'dream idaho thing wish', prediction=0.0),\n",
       " Row(song_key=u'2014-87', noun_syn_hype_vector=u'asshole bitch buttocks dick dime drop face hoe key life male_child shoe shrub stop stunt tennis think', prediction=0.0),\n",
       " Row(song_key=u'2014-88', noun_syn_hype_vector=u'bite control', prediction=1.0),\n",
       " Row(song_key=u'2014-89', noun_syn_hype_vector=u'award bitch check damn day drop girl icicle imperativeness king man mean nigger number path perspiration queen right road team time', prediction=0.0),\n",
       " Row(song_key=u'2014-90', noun_syn_hype_vector=u'animal aspirant athlete ballyhoo baseball_club batch bitch blow bottle cause check chop cup dryer floor foot game grape hand head house hurrah kick miniskirt model nature night pair paramedic player respect right shade shame skirt sky sofa spirit standing thing tongue touch wave', prediction=0.0),\n",
       " Row(song_key=u'2014-91', noun_syn_hype_vector=u'apprehension barroom burn cap corner eye hair hand head line margarita reggae set sun sunlight tide turn', prediction=0.0),\n",
       " Row(song_key=u'2014-93', noun_syn_hype_vector=u'diamond floor jean ring standing town underwear', prediction=0.0),\n",
       " Row(song_key=u'2014-94', noun_syn_hype_vector=u'eye sparkle', prediction=0.0),\n",
       " Row(song_key=u'2014-95', noun_syn_hype_vector=u'baseball_club beat brim buttocks clasp drip eye filth footprint girl glass lipstick male_child mascara music partition rubbish talk wine', prediction=0.0),\n",
       " Row(song_key=u'2014-96', noun_syn_hype_vector=u'attitude buttocks game girl nigger poetry pot quality song swag tongue topographic_point', prediction=0.0),\n",
       " Row(song_key=u'2014-97', noun_syn_hype_vector=u'crap feedback foot lesson male_child man matter nigger night person position problem season shoe squad studio television topographic_point watch work', prediction=0.0),\n",
       " Row(song_key=u'2014-98', noun_syn_hype_vector=u'foot', prediction=1.0),\n",
       " Row(song_key=u'2014-99', noun_syn_hype_vector=u'church dress girl life', prediction=0.0),\n",
       " Row(song_key=u'2014-100', noun_syn_hype_vector=u'marriage', prediction=1.0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# santity check for `selection` output of predicts for syn_hype col, considering top/bottom 50 predictions.\n",
    "full_results[ncols[2]][pbinarycols[2]][0][2].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Save Data suitable for plotting in Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_top_50_percent\n"
     ]
    }
   ],
   "source": [
    "#where to save the dataframe\n",
    "root_out = \"../../viz/data/\"\n",
    "\n",
    "#only outputting the more interesting prediction of top 50 versus bottom 50 (which is also balanced / binary friendly)\n",
    "prediction_col = pbinarycols[2]\n",
    "print prediction_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for saving dataframe to csv\n",
    "def dataframeToCsv(df, csv_name, root_out=root_out, index=False):\n",
    "    df.to_csv(root_out+csv_name,index=index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manipulate results dict to single dataframe\n",
    "def predictionToDataframe(ncol,prediction_col=prediction_col,pdict=full_results):\n",
    "    tidx = 0 # this is the tuple holding the model, prediction, and selection info\n",
    "    dfidx = 2 # this is the selection object within the results.    \n",
    "    selection = pdict[ncol][prediction_col][tidx][dfidx] # this is a Spark Dataframe\n",
    "    return selection.toPandas()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to go from current `full_results_dict` to a single DataFrame.\n",
    "pdfs = {}\n",
    "for ncol in ncols:\n",
    "    df = predictionToDataframe(ncol)\n",
    "    pdfs[ncol] = df \n",
    "    dataframeToCsv(df,ncol+'_'+prediction_col+'.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inline verification\n",
    "pdfs[ncols[2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_key</th>\n",
       "      <th>noun_syn_hype_vector</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-1</td>\n",
       "      <td>air balloon love news space turn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-2</td>\n",
       "      <td>bedroom deep-freeze heart horse love ma male_c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-3</td>\n",
       "      <td>blue distraction drive head imperfection mind ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-4</td>\n",
       "      <td>bitch department drop flow girl gold hand hate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-5</td>\n",
       "      <td>dollar thing word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  song_key                               noun_syn_hype_vector  prediction\n",
       "0   2014-1                   air balloon love news space turn           1\n",
       "1   2014-2  bedroom deep-freeze heart horse love ma male_c...           0\n",
       "2   2014-3  blue distraction drive head imperfection mind ...           0\n",
       "3   2014-4  bitch department drop flow girl gold hand hate...           1\n",
       "4   2014-5                                  dollar thing word           1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs[ncols[2]].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
