{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Exploration\n",
    "### Adapted concepts from [HW1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw1/hw1.ipynb) and [HW5 Part1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw5/hw5part1.ipynb)\n",
    "\n",
    "**This notebook should be locally run by issuing `vagrant up` from project root, then locating the notebook at \"http:\\\\localhost:4545\". You may also need to issue `vagrant provision` to update any required resources.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Provided Data Into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the provided lyrics\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/provided/all billboard top 100 songs from 1970-2014.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cull excess columns swept up on read\n",
    "lyrics_pd_df = lyrics_pd_df[['position','year','title.href','title','artist','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary feeling small When tears are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Woman_(...</td>\n",
       "      <td>American Woman</td>\n",
       "      <td>The Guess Who</td>\n",
       "      <td>American woman, stay away from me American wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Raindrops_Keep_F...</td>\n",
       "      <td>Raindrops Keep Fallin' on My Head</td>\n",
       "      <td>B.J. Thomas</td>\n",
       "      <td>Raindrops keep falling on my head Just like th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/War_(Edwin_Starr...</td>\n",
       "      <td>War</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>War huh Yeah! Absolutely uh-huh, uh-huh huh Ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position  year                                         title.href                              title               artist                                             lyrics\n",
       "0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...         Bridge over Troubled Water  Simon and Garfunkel  When you're weary feeling small When tears are...\n",
       "1         2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...     (They Long to Be) Close to You       The Carpenters                                                  x\n",
       "2         3  1970  https://en.wikipedia.org/wiki/American_Woman_(...                     American Woman        The Guess Who  American woman, stay away from me American wom...\n",
       "3         4  1970  https://en.wikipedia.org/wiki/Raindrops_Keep_F...  Raindrops Keep Fallin' on My Head          B.J. Thomas  Raindrops keep falling on my head Just like th...\n",
       "4         5  1970  https://en.wikipedia.org/wiki/War_(Edwin_Starr...                                War          Edwin Starr  War huh Yeah! Absolutely uh-huh, uh-huh huh Ye..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add `decade` column to df\n",
    "lyrics_pd_df['decade'] = lyrics_pd_df.year.apply(lambda y : y - y%10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a `song_key` column by joining `year` and `position` for better identity \n",
    "# adapted from:\n",
    "# http://stackoverflow.com/questions/29983946/concatenate-cells-into-a-string-with-separator-pandas-python\n",
    "lyrics_pd_df['song_key'] = lyrics_pd_df[['year','position']].apply(lambda row: '-'.join(row.astype(str).values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>95</td>\n",
       "      <td>1972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baby Let Me Take You (In My Arms)</td>\n",
       "      <td>The Detroit Emeralds</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1972-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>8</td>\n",
       "      <td>1997</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Return_of_the_Mack</td>\n",
       "      <td>Return of the Mack</td>\n",
       "      <td>Mark Morrison</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>1997-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>42</td>\n",
       "      <td>1985</td>\n",
       "      <td>https://en.wikipedia.org/wiki/All_I_Need_(Jack...</td>\n",
       "      <td>All I Need</td>\n",
       "      <td>Jack Wagner</td>\n",
       "      <td>Kissing you is not what I had planned And now ...</td>\n",
       "      <td>1980</td>\n",
       "      <td>1985-42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>32</td>\n",
       "      <td>1992</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Smells_Like_Teen...</td>\n",
       "      <td>Smells Like Teen Spirit</td>\n",
       "      <td>Nirvana</td>\n",
       "      <td>Load up on guns, bring your friends, It's fun ...</td>\n",
       "      <td>1990</td>\n",
       "      <td>1992-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>43</td>\n",
       "      <td>1993</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I%27d_Die_Withou...</td>\n",
       "      <td>I'd Die Without You</td>\n",
       "      <td>P.M. Dawn</td>\n",
       "      <td>x</td>\n",
       "      <td>1990</td>\n",
       "      <td>1993-43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  year                                         title.href                              title                artist                                             lyrics  decade song_key\n",
       "294         95  1972                                                NaN  Baby Let Me Take You (In My Arms)  The Detroit Emeralds                                                  x    1970  1972-95\n",
       "2707         8  1997   https://en.wikipedia.org/wiki/Return_of_the_Mack                 Return of the Mack         Mark Morrison                                                NaN    1990   1997-8\n",
       "1541        42  1985  https://en.wikipedia.org/wiki/All_I_Need_(Jack...                         All I Need           Jack Wagner  Kissing you is not what I had planned And now ...    1980  1985-42\n",
       "2231        32  1992  https://en.wikipedia.org/wiki/Smells_Like_Teen...            Smells Like Teen Spirit               Nirvana  Load up on guns, bring your friends, It's fun ...    1990  1992-32\n",
       "2342        43  1993  https://en.wikipedia.org/wiki/I%27d_Die_Withou...                I'd Die Without You             P.M. Dawn                                                  x    1990  1993-43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyrics_pd_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Which Lyrics Are Missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics == 'x' shape -->  (863, 8)\n",
      "check lyrics len < 10 -->  (863, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I%27ll_Be_There_...</td>\n",
       "      <td>I'll Be There</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Let_It_Be_(song)</td>\n",
       "      <td>Let It Be</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/ABC_(song)</td>\n",
       "      <td>ABC</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Love_You_Save</td>\n",
       "      <td>The Love You Save</td>\n",
       "      <td>The Jackson 5</td>\n",
       "      <td>x</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    position  year                                         title.href                           title          artist lyrics  decade song_key\n",
       "1          2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...  (They Long to Be) Close to You  The Carpenters      x    1970   1970-2\n",
       "6          7  1970  https://en.wikipedia.org/wiki/I%27ll_Be_There_...                   I'll Be There   The Jackson 5      x    1970   1970-7\n",
       "8          9  1970     https://en.wikipedia.org/wiki/Let_It_Be_(song)                       Let It Be     The Beatles      x    1970   1970-9\n",
       "14        15  1970           https://en.wikipedia.org/wiki/ABC_(song)                             ABC   The Jackson 5      x    1970  1970-15\n",
       "15        16  1970    https://en.wikipedia.org/wiki/The_Love_You_Save               The Love You Save   The Jackson 5      x    1970  1970-16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing lyrics have a value of 'x', these will need to be back-filled in the Harvest section\n",
    "missing_lyricsdf = lyrics_pd_df[lyrics_pd_df['lyrics'] == 'x']\n",
    "print \"lyrics == 'x' shape --> \", missing_lyricsdf.shape\n",
    "print \"check lyrics len < 10 --> \", lyrics_pd_df[lyrics_pd_df['lyrics'].str.len() <= 10].shape\n",
    "missing_lyricsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join Provided Artist Info with Song \n",
    "* **TODO**: Use artist info provided to have a joined dataframe, reference HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Harvest Missing Data\n",
    "* **TODO**: consider missing lyrics from `missing_lyrics` \n",
    "* **TODO**: consider additional years, prior to 1970 and 2015 \n",
    "* **TODO**: harvest artist info for additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join Harvested Artist Info with Song \n",
    "* **TODO**: Use artist info harvested to have a joined dataframe, reference HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Build Decade Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000, 4499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1970: [0, 999],\n",
       " 1980: [1000, 1999],\n",
       " 1990: [2000, 2999],\n",
       " 2000: [3000, 3999],\n",
       " 2010: [4000, 4499]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary holding indexes for decades, useful for filtering in corpus, vocabs, etc.\n",
    "def buildDecadeDict(df=lyrics_pd_df):\n",
    "    \"\"\"\n",
    "    assumes df is sorted by year.\n",
    "    Note: saved value as list to be consistent with json persistence.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    dyear = None\n",
    "    didx = 0\n",
    "    idx = -1\n",
    "    for row in lyrics_pd_df.iterrows():\n",
    "        idx +=1    \n",
    "        year = row[1].year\n",
    "        # initial conditions\n",
    "        if not dyear:\n",
    "            dyear = year\n",
    "        # year change    \n",
    "        elif year - year%10 != dyear:\n",
    "            # add the last year to the dictionary\n",
    "            d[dyear] = [didx,idx-1]           \n",
    "            dyear = year\n",
    "            didx = idx\n",
    "    # handle the last entry in the loop\n",
    "    d[dyear] = [didx,idx]\n",
    "    print d[dyear]\n",
    "    \n",
    "    return d\n",
    "decade_dict = buildDecadeDict()\n",
    "decade_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Pandas Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save lyrics_pd_df\n",
    "lyrics_pd_df.to_csv(\"../../data/conditioned/lyrics_pd_df.csv\",index=False) #note: ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save decade dict\n",
    "with open('../../data/conditioned/decade-dict.json', 'w') as fp:\n",
    "    json.dump(decade_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Manipulate With Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|  1970|  1970-2|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|  1970|  1970-3|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer need the pandas version, clear it out\n",
    "del lyrics_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|  1970|  1970-2|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|  1970|  1970-3|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many songs do we have? 4500\n"
     ]
    }
   ],
   "source": [
    "#We cache the data to make sure it is only read once from disk\n",
    "lyricsdf.cache()\n",
    "print \"How many songs do we have?\", lyricsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the schema? root\n",
      " |-- position: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- title.href: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- decade: long (nullable = true)\n",
      " |-- song_key: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"What is the schema?\", lyricsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sample Lyrics (or Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial sampling to take from each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether or not to sample lyrics, and how many to sample per year\n",
    "sample_lyrics = False\n",
    "PER_YEAR_SAMPLES=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "def randomSubSampleLyrics(sparkdf,take=PER_YEAR_SAMPLES):    \n",
    "    # generate spark pairs as a tuple\n",
    "    br_pairs = sparkdf.map(lambda r: (r.year, r.song_key))\n",
    "    \n",
    "    # group by key for a list of reviews per business and collect\n",
    "    br_grouped = br_pairs.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "        \n",
    "    #sample after collect\n",
    "    br_sample = [np.random.choice(v, size=take, replace=False) for k,v in br_grouped]    \n",
    "    \n",
    "    #flatten into a list\n",
    "    return list(itertools.chain.from_iterable(br_sample))\n",
    "    \n",
    "small_song_keys = randomSubSampleLyrics(lyricsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\n"
     ]
    }
   ],
   "source": [
    "if sample_lyrics:\n",
    "    print \"How many small_song_keys? \", len(small_song_keys)\n",
    "    small_song_keys[:5]\n",
    "else:\n",
    "    print \"No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Fri, 20 Nov 2015 16:17:16\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#(your code here)\n",
    "if sample_lyrics:\n",
    "    ldf=lyricsdf[lyricsdf.song_key.isin(small_song_keys)]#creates new dataframe\n",
    "else:\n",
    "    ldf=lyricsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[position: bigint, year: bigint, title.href: string, title: string, artist: string, lyrics: string, decade: bigint, song_key: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "ldf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lyrics are in ldf?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many lyrics are in ldf? \", ldf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Test of parse...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The/DT/B-NP/O/the world/NN/I-NP/O/world is/VBZ/B-VP/O/be the/DT/B-NP/O/the craziest/JJ/I-NP/O/craziest place/NN/I-NP/O/place ././O/O/.\\nI/PRP/B-NP/O/i am/VBP/B-VP/O/be working/VBG/I-VP/O/work hard/RB/B-ADVP/O/hard ././O/O/.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick Test of parse...\"\n",
    "parse(\"The world is the craziest place. I am working hard.\", tokenize=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of get_parts ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[u'patio', u'job'], [u'lunch', u'egg']], [[u'perfect'], [u'good', u'great']])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick check of get_parts ...\"\n",
    "get_parts(\"Have had many other items and just love the food. The patio...job was and...perfect. Lunch is good, and the only egg is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run Get Parts on Provided Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "lyric_parts = ldf.map(lambda r : get_parts(r.lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'feeling',\n",
       "    u'tear',\n",
       "    u'eye',\n",
       "    u'time',\n",
       "    u'friend',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'street',\n",
       "    u'evening',\n",
       "    u'comfort',\n",
       "    u'darkness',\n",
       "    u'pain',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'time',\n",
       "    u'dream',\n",
       "    u'way',\n",
       "    u'friend',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'mind',\n",
       "    u'bridge',\n",
       "    u'water',\n",
       "    u'mind']],\n",
       "  [[u'weary',\n",
       "    u'small',\n",
       "    u'rough',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled',\n",
       "    u'troubled']]),\n",
       " ([], [])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyric_parts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Fri, 20 Nov 2015 16:17:18\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 13 ms, total: 139 ms\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parseout=lyric_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vocab\n",
    "###Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many parseout entries?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many parseout entries? \", len(parseout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten parseout to create initial noun rdd\n",
    "nounrdd=sc.parallelize([ele[0] for ele in parseout]).flatMap(lambda l: l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'feeling',\n",
       "  u'tear',\n",
       "  u'eye',\n",
       "  u'time',\n",
       "  u'friend',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'street',\n",
       "  u'evening',\n",
       "  u'comfort',\n",
       "  u'darkness',\n",
       "  u'pain',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'time',\n",
       "  u'dream',\n",
       "  u'way',\n",
       "  u'friend',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'mind',\n",
       "  u'bridge',\n",
       "  u'water',\n",
       "  u'mind']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounrdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "nwordsrdd = (nounrdd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 3),\n",
       " (u'sleet', 6),\n",
       " (u'sleep', 77),\n",
       " (u'mansion', 10),\n",
       " (u'integrity', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nwordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'love', 4969),\n",
       " (u'baby', 3852),\n",
       " (u'time', 3801),\n",
       " (u'way', 2994),\n",
       " (u'girl', 2825),\n",
       " (u'night', 2406),\n",
       " (u'heart', 2006),\n",
       " (u'thing', 1996),\n",
       " (u'day', 1947),\n",
       " (u'life', 1806)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "nwordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[41] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect all the words and cache\n",
    "nounvocabtups = (nwordsrdd\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 0), (u'sleet', 1), (u'sleep', 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[44] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "nounvocab=nounvocabtups.collectAsMap()\n",
    "nounid2word=nounvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'jockin', u'woody', 2302)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `nounvocab['dance']`\n",
    "nounid2word[0], nounvocab.keys()[5], nounvocab[nounvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the noun vocabulary?  8757\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the noun vocabulary? \", len(nounvocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create initial adj rdd from parseout\n",
    "adjrdd=sc.parallelize([ele[1] for ele in parseout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'weary',\n",
       "   u'small',\n",
       "   u'rough',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled',\n",
       "   u'troubled']],\n",
       " [],\n",
       " [[u'american',\n",
       "   u'american',\n",
       "   u'hanging',\n",
       "   u'important',\n",
       "   u'old',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'warm',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'good',\n",
       "   u'good',\n",
       "   u'american',\n",
       "   u'american',\n",
       "   u'american']]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[46] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "awordsrdd = (adjrdd\n",
    "             .flatMap(lambda l: l)\n",
    "             .flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ooh-oh', 1),\n",
       " (u'good-night', 1),\n",
       " (u'suicidal', 2),\n",
       " (u'b-b-b-be', 1),\n",
       " (u'crucial', 5)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "awordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'good', 1934),\n",
       " (u'little', 1560),\n",
       " (u'bad', 858),\n",
       " (u'real', 842),\n",
       " (u'true', 705),\n",
       " (u'wrong', 703),\n",
       " (u'right', 661),\n",
       " (u'long', 632),\n",
       " (u'crazy', 626),\n",
       " (u'new', 620)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "awordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "awordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "adjvocabtups = (awordsrdd\n",
    "              .map(lambda (x,y): x)\n",
    "              .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ooh-oh', 0), (u'good-night', 1), (u'suicidal', 2)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[57] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "adjvocab=adjvocabtups.collectAsMap()\n",
    "adjid2word=adjvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'ooh-oh', u'dynamic', 32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `adjvocab['exotic']`\n",
    "adjid2word[0], adjvocab.keys()[5], adjvocab[adjvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the adjective vocabulary?  3486\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the adjective vocabulary? \", len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# CITATION - Use of counter for reduce within each word list from:\n",
    "# http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "##################################################################################################\n",
    "from collections import Counter\n",
    "\n",
    "# for each sentence, reduct into a list of tuple k,v where k=vocab index and v=count, \n",
    "# each word list is sorted by occurence\n",
    "documents = nounrdd.map(lambda words: Counter([nounvocab[word] for word in words]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(610, 6),\n",
       "  (3123, 6),\n",
       "  (8417, 2),\n",
       "  (5210, 2),\n",
       "  (232, 2),\n",
       "  (5211, 1),\n",
       "  (580, 1),\n",
       "  (840, 1),\n",
       "  (828, 1),\n",
       "  (8241, 1),\n",
       "  (6868, 1),\n",
       "  (1206, 1),\n",
       "  (7987, 1),\n",
       "  (7003, 1),\n",
       "  (3420, 1)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify output\n",
    "documents.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather spark results\n",
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Spark Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save lyricsdf / ldf\n",
    "ldf.toPandas().to_csv(\"../../data/conditioned/sample_or_not_lyricsdf.csv\",index=False,encoding='utf-8') #note: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun n-gram\n",
    "with open('../../data/conditioned/noun-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(nwordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adjective n-gram\n",
    "with open('../../data/conditioned/adj-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(awordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun vocab and id2word\n",
    "with open('../../data/conditioned/nounvocab.json', 'w') as fp:\n",
    "    json.dump(nounvocab, fp)\n",
    "    \n",
    "with open('../../data/conditioned/nounid2word.json', 'w') as fp:\n",
    "    json.dump(nounid2word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adj vocab and id2word\n",
    "with open('../../data/conditioned/adjvocab.json', 'w') as fp:\n",
    "    json.dump(adjvocab, fp)\n",
    "    \n",
    "with open('../../data/conditioned/adjid2word.json', 'w') as fp:\n",
    "    json.dump(adjid2word, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save corpus\n",
    "pickle.dump( corpus, open( \"../../data/conditioned/corpus.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Synonym Lookups\n",
    "Focus on WordNet python package within [nltk](http://www.nltk.org) via [textblob](https://textblob.readthedocs.org/en/dev/)\n",
    "The main idea is to lookup all words in the noun and adj vocab dictionaries and attempt to collapse down -- where possible -- to synonyms. The synonyms can be used for common_support also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n",
      "-0.341666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextBlob(\"La amenaza titular de The Blob siempre me ha parecido la película final\n",
       "monstruo: una, la masa insaciablemente hambriento ameba capaz de penetrar\n",
       "prácticamente cualquier salvaguardia, capaz de - como médico condenado escalofriantemente\n",
       "lo describe - \"asimilar carne en contacto.\n",
       "Comparaciones Snide a la gelatina ser condenados, es un concepto con el más\n",
       "devastadora de las posibles consecuencias, no muy diferente del escenario plaga gris\n",
       "propuesto por los teóricos tecnológicos temerosos de\n",
       "la inteligencia artificial ejecutar rampante.\")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of TextBlob\n",
    "\n",
    "# from https://textblob.readthedocs.org/en/dev/\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "blob.tags           # [('The', 'DT'), ('titular', 'JJ'),\n",
    "                    #  ('threat', 'NN'), ('of', 'IN'), ...]\n",
    "\n",
    "blob.noun_phrases   # WordList(['titular threat', 'blob',\n",
    "                    #            'ultimate movie monster',\n",
    "                    #            'amoeba-like mass', ...])\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)\n",
    "# 0.060\n",
    "# -0.341\n",
    "\n",
    "blob.translate(to=\"es\")  # 'La amenaza titular de The Blob...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octopus similarity to octopus -->  1.0\n",
      "octopus similarity to nautilus -->  0.333333333333\n",
      "octopus similarity to shrimp -->  0.111111111111\n",
      "octopus similarity to pearl -->  0.0666666666667\n"
     ]
    }
   ],
   "source": [
    "# test of Synset and path_similarity\n",
    "\n",
    "# from http://textblob.readthedocs.org/en/dev/quickstart.html#quickstart\n",
    "from textblob.wordnet import Synset\n",
    "octopus = Synset(\"octopus.n.02\")\n",
    "nautilus = Synset('paper_nautilus.n.01')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "pearl = Synset('pearl.n.01')\n",
    "\n",
    "print \"octopus similarity to octopus --> \",octopus.path_similarity(octopus)  # 1.0\n",
    "print \"octopus similarity to nautilus --> \",octopus.path_similarity(nautilus)  # 0.33\n",
    "print \"octopus similarity to shrimp --> \",octopus.path_similarity(shrimp)  # 0.11\n",
    "print \"octopus similarity to pearl --> \",octopus.path_similarity(pearl)  # 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vice / Virtue Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ensemble Approach\n",
    "See [combine all features into a single feature vector](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spark Word2Vec for similarity\n",
    "This approach is plausible but probably should be used in an ensemble vector approach, see [combine all features into a single feature vector](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.mllib.feature import Word2Vec #mllib works with RDD as-is\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(nounrdd)\n",
    "\n",
    "synonyms = model.findSynonyms('ho', 10)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Split out by decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
