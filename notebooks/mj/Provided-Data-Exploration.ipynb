{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Provided Data Exploration\n",
    "### Adapted from https://github.com/cs109-students/michaeljohns-2015hw/blob/hw5/hw5part1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Provided Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the provided lyrics\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/provided/all billboard top 100 songs from 1970-2014.csv\")  \n",
    "# lyrics_pd_df = pd.read_csv(\"../../data/provided/all billboard top 100 songs from 1970-2014.csv\", \n",
    "#                            names = ['<column 1>','<column 2>']) # if no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyrics_pd_df = lyrics_pd_df[['position','year','title.href','title','artist','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary feeling small When tears are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Woman_(...</td>\n",
       "      <td>American Woman</td>\n",
       "      <td>The Guess Who</td>\n",
       "      <td>American woman, stay away from me American wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Raindrops_Keep_F...</td>\n",
       "      <td>Raindrops Keep Fallin' on My Head</td>\n",
       "      <td>B.J. Thomas</td>\n",
       "      <td>Raindrops keep falling on my head Just like th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/War_(Edwin_Starr...</td>\n",
       "      <td>War</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>War huh Yeah! Absolutely uh-huh, uh-huh huh Ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position  year                                         title.href                              title               artist                                             lyrics\n",
       "0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...         Bridge over Troubled Water  Simon and Garfunkel  When you're weary feeling small When tears are...\n",
       "1         2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...     (They Long to Be) Close to You       The Carpenters                                                  x\n",
       "2         3  1970  https://en.wikipedia.org/wiki/American_Woman_(...                     American Woman        The Guess Who  American woman, stay away from me American wom...\n",
       "3         4  1970  https://en.wikipedia.org/wiki/Raindrops_Keep_F...  Raindrops Keep Fallin' on My Head          B.J. Thomas  Raindrops keep falling on my head Just like th...\n",
       "4         5  1970  https://en.wikipedia.org/wiki/War_(Edwin_Starr...                                War          Edwin Starr  War huh Yeah! Absolutely uh-huh, uh-huh huh Ye..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer need the pandas version, clear it out\n",
    "del lyrics_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\"\"\"\n",
    "add a `song_key` column by joining `year` and `position` for better identity \n",
    "\"\"\"\n",
    "##################################################################################################\n",
    "# CITATION - Concat usage within Spark Dataframe adapted from:\n",
    "# http://stackoverflow.com/questions/31450846/concatenate-columns-in-apache-spark-dataframe\n",
    "##################################################################################################\n",
    "lyricsdf = lyricsdf.withColumn('song_key',F.concat(F.col(\"year\"), F.lit(\"-\"), F.col(\"position\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+--------------------+-------------------+--------------------+--------+\n",
      "|position|year|          title.href|               title|             artist|              lyrics|song_key|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+--------+\n",
      "|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970-1|\n",
      "|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|                   x|  1970-2|\n",
      "|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|American woman, s...|  1970-3|\n",
      "+--------+----+--------------------+--------------------+-------------------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many songs do we have? 4500\n"
     ]
    }
   ],
   "source": [
    "#We cache the data to make sure it is only read once from disk\n",
    "lyricsdf.cache()\n",
    "print \"How many songs do we have?\", lyricsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the schema? root\n",
      " |-- position: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- title.href: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- song_key: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"What is the schema?\", lyricsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial sampling to take from each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether or not to sample lyrics, and how many to sample per year\n",
    "sample_lyrics = True\n",
    "PER_YEAR_SAMPLES=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "def randomSubSampleLyrics(sparkdf,take=PER_YEAR_SAMPLES):    \n",
    "    # generate spark pairs as a tuple\n",
    "    br_pairs = sparkdf.map(lambda r: (r.year, r.song_key))\n",
    "    \n",
    "    # group by key for a list of reviews per business and collect\n",
    "    br_grouped = br_pairs.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "        \n",
    "    #sample after collect\n",
    "    br_sample = [np.random.choice(v, size=take, replace=False) for k,v in br_grouped]    \n",
    "    \n",
    "    #flatten into a list\n",
    "    return list(itertools.chain.from_iterable(br_sample))\n",
    "    \n",
    "small_song_keys = randomSubSampleLyrics(lyricsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many small_song_keys?  450\n"
     ]
    }
   ],
   "source": [
    "if sample_lyrics:\n",
    "    print \"How many small_song_keys? \", len(small_song_keys)\n",
    "    small_song_keys[:5]\n",
    "else:\n",
    "    print \"No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sample Lyrics (or Not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Wed, 18 Nov 2015 13:44:33\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 138 ms, total: 138 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#(your code here)\n",
    "if sample_lyrics:\n",
    "    ldf=lyricsdf[lyricsdf.song_key.isin(small_song_keys)]#creates new dataframe\n",
    "else:\n",
    "    ldf=lyricsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[position: bigint, year: bigint, title.href: string, title: string, artist: string, lyrics: string, song_key: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "ldf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lyrics are in ldf?  450\n"
     ]
    }
   ],
   "source": [
    "print \"How many lyrics are in ldf? \", ldf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Test of parse...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The/DT/B-NP/O/the world/NN/I-NP/O/world is/VBZ/B-VP/O/be the/DT/B-NP/O/the craziest/JJ/I-NP/O/craziest place/NN/I-NP/O/place ././O/O/.\\nI/PRP/B-NP/O/i am/VBP/B-VP/O/be working/VBG/I-VP/O/work hard/RB/B-ADVP/O/hard ././O/O/.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick Test of parse...\"\n",
    "parse(\"The world is the craziest place. I am working hard.\", tokenize=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of get_parts ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[u'patio', u'job'], [u'lunch', u'egg']], [[u'perfect'], [u'good', u'great']])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick check of get_parts ...\"\n",
    "get_parts(\"Have had many other items and just love the food. The patio...job was and...perfect. Lunch is good, and the only egg is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run Get Parts on Provided Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "lyric_parts = ldf.map(lambda r : get_parts(r.lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'question'],\n",
       "   [u'party',\n",
       "    u'light',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'window',\n",
       "    u'air',\n",
       "    u'room',\n",
       "    u'smell',\n",
       "    u'perfume',\n",
       "    u'cigarette',\n",
       "    u'smoking',\n",
       "    u'half',\n",
       "    u'death',\n",
       "    u'window',\n",
       "    u'sucker',\n",
       "    u'breath',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'radio',\n",
       "    u'door',\n",
       "    u'girlfriend',\n",
       "    u'floor',\n",
       "    u'thing',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'way',\n",
       "    u'fun',\n",
       "    u'son']],\n",
       "  [[u'crazy'], [u'craziest', u'stale']]),\n",
       " ([[u'thing', u'time', u'thing'], [u'love', u'girl']],\n",
       "  [[u'little'], [u'short', u'right', u'wrong']])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyric_parts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Wed, 18 Nov 2015 13:44:35\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 ms, sys: 2.77 ms, total: 13.5 ms\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parseout=lyric_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vocab\n",
    "##Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many parseout entries?  450\n"
     ]
    }
   ],
   "source": [
    "print \"How many parseout entries? \", len(parseout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten parseout to create initial noun rdd\n",
    "nounrdd=sc.parallelize([ele[0] for ele in parseout]).flatMap(lambda l: l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'question']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounrdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[43] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect all the words and cache\n",
    "nounvocabtups = (nounrdd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shop', 0), (u'liar', 1), (u'dance', 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[50] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "nounvocab=nounvocabtups.collectAsMap()\n",
    "nounid2word=nounvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'shop', u'hate', 182)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `nounvocab['dance']`\n",
    "nounid2word[0], nounvocab.keys()[5], nounvocab[nounvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the noun vocabulary?  2361\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the noun vocabulary? \", len(nounvocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create initial adj rdd from parseout\n",
    "adjrdd=sc.parallelize([ele[1] for ele in parseout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'crazy'], [u'craziest', u'stale']],\n",
       " [[u'little'], [u'short', u'right', u'wrong']],\n",
       " [[u'warm'],\n",
       "  [u'rainy',\n",
       "   u'rainy',\n",
       "   u'distant',\n",
       "   u'sad',\n",
       "   u'rainy',\n",
       "   u'rainy',\n",
       "   u'hard',\n",
       "   u'rainy',\n",
       "   u'rainy',\n",
       "   u'lonely',\n",
       "   u'lonely'],\n",
       "  [u'rainin']]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[52] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "adjvocabtups = (adjrdd.flatMap(lambda l: l)\n",
    "              .flatMap(lambda word: word)\n",
    "              .map(lambda word: (word, 1))\n",
    "              .reduceByKey(lambda a, b: a + b)\n",
    "              .map(lambda (x,y): x)\n",
    "              .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'exotic', 0), (u'shot', 1), (u'lean', 2)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[60] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "adjvocab=adjvocabtups.collectAsMap()\n",
    "adjid2word=adjvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'exotic', u'two-three', 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `adjvocab['exotic']`\n",
    "adjid2word[0], adjvocab.keys()[5], adjvocab[adjvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the adjective vocabulary?  763\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the adjective vocabulary? \", len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# CITATION - Use of counter for reduce within each word list from:\n",
    "# http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "##################################################################################################\n",
    "from collections import Counter\n",
    "\n",
    "# for each sentence, reduct into a list of tuple k,v where k=vocab index and v=count, \n",
    "# each word list is sorted by occurence\n",
    "documents = nounrdd.map(lambda words: Counter([nounvocab[word] for word in words]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(539, 1)],\n",
       " [(1284, 13),\n",
       "  (407, 13),\n",
       "  (2281, 7),\n",
       "  (1559, 2),\n",
       "  (769, 1),\n",
       "  (386, 1),\n",
       "  (870, 1),\n",
       "  (742, 1),\n",
       "  (583, 1),\n",
       "  (1000, 1),\n",
       "  (697, 1),\n",
       "  (97, 1),\n",
       "  (974, 1),\n",
       "  (2097, 1),\n",
       "  (2222, 1),\n",
       "  (2169, 1),\n",
       "  (721, 1),\n",
       "  (572, 1),\n",
       "  (2215, 1),\n",
       "  (2334, 1),\n",
       "  (309, 1)],\n",
       " [(721, 2), (2359, 1)]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify output\n",
    "documents.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather spark results\n",
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
