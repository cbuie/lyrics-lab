{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vocab Consolidation\n",
    "### Adapted concepts from [HW1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw1/hw1.ipynb) and [HW5 Part1](https://github.com/cs109-students/michaeljohns-2015hw/blob/hw5/hw5part1.ipynb)\n",
    "\n",
    "**This notebook should be locally run by issuing `vagrant up` from project root, then locating the notebook at \"http:\\\\localhost:4545\". You may also need to issue `vagrant provision` to update any required resources.**\n",
    "\n",
    "The following artifacts will be established by manipulating the output of the processing pipeline for harvesting data, file [use-this-master-lyricsdf-extracted.csv](../../data/conditioned/use-this-master-lyricsdf-extracted.csv):\n",
    "* vocabs for noun and adj\n",
    "* n-gram for noun and adj\n",
    "* synonyms for noun and adj\n",
    "* hypernyms for noun and adj\n",
    "\n",
    "Other notes:\n",
    "* this notebook leverages and finalizes exploratory work in [Data-Exploration Notebook](Data-Exploration.ipynb).\n",
    "* outputs are anticipated to be combined in follow-on work for better latent factors, prediction, and recommendation processing (not reflected here)\n",
    "* in other notebooks that use the exact same contents as here, we will establish n-gram and vocab per decade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SET THE DECADE FOR PROCESS FILTERING\n",
    "## THIS WILL ALLOW SPECIAL PROCESSING\n",
    "decade = None # for no decade filtering, i.e. corpus-wide\n",
    "# decade = 1970\n",
    "# decade = 1980\n",
    "# decade = 1990\n",
    "# decade = 2000\n",
    "# decade = 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MLJ: Additional Extras\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Handle Directory for Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from https://justgagan.wordpress.com/2010/09/22/python-create-path-or-directories-if-not-exist/\n",
    "def assureDirExists(path):\n",
    "    d = os.path.dirname(path)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create requisite directory for processing\n",
    "root_out = \"\"\n",
    "if not decade:\n",
    "    root_out = \"../../data/conditioned/corpus_vocabs/\" #entire corpus\n",
    "else:\n",
    "    root_out = \"../../data/conditioned/decades/\"+str(decade)+\"/\" #single decade\n",
    "    \n",
    "assureDirExists(root_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.memory', u'8g'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'pyspark')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Finalized Conditioned Data Into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the lyrics from the approved \"master\" dataframe\n",
    "lyrics_pd_df = pd.read_csv(\"../../data/conditioned/use-this-master-lyricsdf-extracted.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FILTER BY DECADE IF SET\n",
    "if decade:\n",
    "    lyrics_pd_df = lyrics_pd_df[lyrics_pd_df['decade'] == decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>position</th>\n",
       "      <th>year</th>\n",
       "      <th>title.href</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>song_key</th>\n",
       "      <th>lyrics_url</th>\n",
       "      <th>lyrics_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridge_over_Trou...</td>\n",
       "      <td>Bridge over Troubled Water</td>\n",
       "      <td>Simon and Garfunkel</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-1</td>\n",
       "      <td>http://lyrics.wikia.com/Simon_And_Garfunkel:Br...</td>\n",
       "      <td>When you're weary. Feeling small. When tears a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/(They_Long_to_Be...</td>\n",
       "      <td>(They Long to Be) Close to You</td>\n",
       "      <td>The Carpenters</td>\n",
       "      <td>Why do birds suddenly appear. Everytime you ar...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-2</td>\n",
       "      <td>http://lyrics.wikia.com/Carpenters:%28They_Lon...</td>\n",
       "      <td>Why do birds suddenly appear. Everytime you ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Woman_(...</td>\n",
       "      <td>American Woman</td>\n",
       "      <td>The Guess Who</td>\n",
       "      <td>Mmm, da da da. Mmm, mmm, da da da. Mmm, mmm, d...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-3</td>\n",
       "      <td>http://lyrics.wikia.com/The_Guess_Who:American...</td>\n",
       "      <td>Mmm, da da da. Mmm, mmm, da da da. Mmm, mmm, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Raindrops_Keep_F...</td>\n",
       "      <td>Raindrops Keep Fallin' on My Head</td>\n",
       "      <td>B.J. Thomas</td>\n",
       "      <td>Raindrops are falling on my head. And just lik...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-4</td>\n",
       "      <td>http://lyrics.wikia.com/B.J._Thomas:Raindrops_...</td>\n",
       "      <td>Raindrops are falling on my head. And just lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/War_(Edwin_Starr...</td>\n",
       "      <td>War</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>War, huh, yeah. What is it good for? Absolutel...</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970-5</td>\n",
       "      <td>http://lyrics.wikia.com/Edwin_Starr:War</td>\n",
       "      <td>War, huh, yeah. What is it good for? Absolutel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  position  year                                         title.href                              title               artist                                             lyrics  decade song_key                                         lyrics_url                                    lyrics_abstract\n",
       "0      0         1  1970  https://en.wikipedia.org/wiki/Bridge_over_Trou...         Bridge over Troubled Water  Simon and Garfunkel  When you're weary. Feeling small. When tears a...    1970   1970-1  http://lyrics.wikia.com/Simon_And_Garfunkel:Br...  When you're weary. Feeling small. When tears a...\n",
       "1      1         2  1970  https://en.wikipedia.org/wiki/(They_Long_to_Be...     (They Long to Be) Close to You       The Carpenters  Why do birds suddenly appear. Everytime you ar...    1970   1970-2  http://lyrics.wikia.com/Carpenters:%28They_Lon...  Why do birds suddenly appear. Everytime you ar...\n",
       "2      2         3  1970  https://en.wikipedia.org/wiki/American_Woman_(...                     American Woman        The Guess Who  Mmm, da da da. Mmm, mmm, da da da. Mmm, mmm, d...    1970   1970-3  http://lyrics.wikia.com/The_Guess_Who:American...  Mmm, da da da. Mmm, mmm, da da da. Mmm, mmm, d...\n",
       "3      3         4  1970  https://en.wikipedia.org/wiki/Raindrops_Keep_F...  Raindrops Keep Fallin' on My Head          B.J. Thomas  Raindrops are falling on my head. And just lik...    1970   1970-4  http://lyrics.wikia.com/B.J._Thomas:Raindrops_...  Raindrops are falling on my head. And just lik...\n",
       "4      4         5  1970  https://en.wikipedia.org/wiki/War_(Edwin_Starr...                                War          Edwin Starr  War, huh, yeah. What is it good for? Absolutel...    1970   1970-5            http://lyrics.wikia.com/Edwin_Starr:War  War, huh, yeah. What is it good for? Absolutel..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Manipulate With Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from pandas to spark dataframe\n",
    "lyricsdf = sqlsc.createDataFrame(lyrics_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "|index|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|          lyrics_url|     lyrics_abstract|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "|    0|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|http://lyrics.wik...|When you're weary...|\n",
      "|    1|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|Why do birds sudd...|  1970|  1970-2|http://lyrics.wik...|Why do birds sudd...|\n",
      "|    2|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|Mmm, da da da. Mm...|  1970|  1970-3|http://lyrics.wik...|Mmm, da da da. Mm...|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "|index|position|year|          title.href|               title|             artist|              lyrics|decade|song_key|          lyrics_url|     lyrics_abstract|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "|    0|       1|1970|https://en.wikipe...|Bridge over Troub...|Simon and Garfunkel|When you're weary...|  1970|  1970-1|http://lyrics.wik...|When you're weary...|\n",
      "|    1|       2|1970|https://en.wikipe...|(They Long to Be)...|     The Carpenters|Why do birds sudd...|  1970|  1970-2|http://lyrics.wik...|Why do birds sudd...|\n",
      "|    2|       3|1970|https://en.wikipe...|      American Woman|      The Guess Who|Mmm, da da da. Mm...|  1970|  1970-3|http://lyrics.wik...|Mmm, da da da. Mm...|\n",
      "+-----+--------+----+--------------------+--------------------+-------------------+--------------------+------+--------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view output\n",
    "lyricsdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many songs do we have? 4500\n"
     ]
    }
   ],
   "source": [
    "#We cache the data to make sure it is only read once from disk\n",
    "lyricsdf.cache()\n",
    "print \"How many songs do we have?\", lyricsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the schema? root\n",
      " |-- index: long (nullable = true)\n",
      " |-- position: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- title.href: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- decade: long (nullable = true)\n",
      " |-- song_key: string (nullable = true)\n",
      " |-- lyrics_url: string (nullable = true)\n",
      " |-- lyrics_abstract: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"What is the schema?\", lyricsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sample Lyrics (or Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial sampling to take from each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether or not to sample lyrics, and how many to sample per year\n",
    "sample_lyrics = False\n",
    "PER_YEAR_SAMPLES=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "def randomSubSampleLyrics(sparkdf,take=PER_YEAR_SAMPLES):    \n",
    "    # generate spark pairs as a tuple\n",
    "    br_pairs = sparkdf.map(lambda r: (r.year, r.song_key))\n",
    "    \n",
    "    # group by key for a list of reviews per business and collect\n",
    "    br_grouped = br_pairs.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "        \n",
    "    #sample after collect\n",
    "    br_sample = [np.random.choice(v, size=take, replace=False) for k,v in br_grouped]    \n",
    "    \n",
    "    #flatten into a list\n",
    "    return list(itertools.chain.from_iterable(br_sample))\n",
    "    \n",
    "small_song_keys = randomSubSampleLyrics(lyricsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\n"
     ]
    }
   ],
   "source": [
    "if sample_lyrics:\n",
    "    print \"How many small_song_keys? \", len(small_song_keys)\n",
    "    small_song_keys[:5]\n",
    "else:\n",
    "    print \"No lyric sampling, full processing (change `sample_lyrics` value to `True` to sample)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Tue, 08 Dec 2015 03:53:26\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 5 µs, total: 16 µs\n",
      "Wall time: 22.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#(your code here)\n",
    "if sample_lyrics:\n",
    "    ldf=lyricsdf[lyricsdf.song_key.isin(small_song_keys)]#creates new dataframe\n",
    "else:\n",
    "    ldf=lyricsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, position: bigint, year: bigint, title.href: string, title: string, artist: string, lyrics: string, decade: bigint, song_key: string, lyrics_url: string, lyrics_abstract: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "ldf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lyrics are in ldf?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many lyrics are in ldf? \", ldf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Test of parse...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The/DT/B-NP/O/the world/NN/I-NP/O/world is/VBZ/B-VP/O/be the/DT/B-NP/O/the craziest/JJ/I-NP/O/craziest place/NN/I-NP/O/place ././O/O/.\\nI/PRP/B-NP/O/i am/VBP/B-VP/O/be working/VBG/I-VP/O/work hard/RB/B-ADVP/O/hard ././O/O/.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick Test of parse...\"\n",
    "parse(\"The world is the craziest place. I am working hard.\", tokenize=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of get_parts ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[u'patio', u'job'], [u'lunch', u'egg']], [[u'perfect'], [u'good', u'great']])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Quick check of get_parts ...\"\n",
    "get_parts(\"Have had many other items and just love the food. The patio...job was and...perfect. Lunch is good, and the only egg is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run Get Parts on Provided Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "lyric_parts = ldf.map(lambda r : get_parts(r.lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'time'],\n",
       "   [u'bridge', u'water'],\n",
       "   [u'bridge', u'water'],\n",
       "   [u'bridge', u'water'],\n",
       "   [u'bridge', u'water'],\n",
       "   [u'bridge', u'water'],\n",
       "   [u'bridge', u'water']],\n",
       "  [[u'rough'],\n",
       "   [u'troubled'],\n",
       "   [u'troubled'],\n",
       "   [u'troubled'],\n",
       "   [u'troubled'],\n",
       "   [u'troubled'],\n",
       "   [u'troubled']]),\n",
       " ([[u'dream'], [u'starlight', u'eye'], [u'dream'], [u'starlight', u'eye']],\n",
       "  [[u'true'], [u'blue'], [u'true'], [u'blue']])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "lyric_parts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Tue, 08 Dec 2015 03:53:27\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 34.4 ms, total: 146 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parseout=lyric_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vocab\n",
    "###Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many parseout entries?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many parseout entries? \", len(parseout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten parseout to create initial noun rdd\n",
    "nounrdd=sc.parallelize([ele[0] for ele in parseout]).flatMap(lambda l: l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'time'],\n",
       " [u'bridge', u'water'],\n",
       " [u'bridge', u'water'],\n",
       " [u'bridge', u'water'],\n",
       " [u'bridge', u'water']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "nwordsrdd = (nounrdd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 1),\n",
       " (u'slope', 1),\n",
       " (u'girl(oh', 1),\n",
       " (u'dance', 216),\n",
       " (u'pigeon', 3)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nwordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'love', 2390),\n",
       " (u'baby', 1665),\n",
       " (u'girl', 1583),\n",
       " (u'time', 1544),\n",
       " (u'thing', 1097),\n",
       " (u'night', 1003),\n",
       " (u'man', 918),\n",
       " (u'way', 881),\n",
       " (u'day', 830),\n",
       " (u'heart', 802)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "nwordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[41] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect all the words and cache\n",
    "nounvocabtups = (nwordsrdd\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'jockin', 0), (u'slope', 1), (u'girl(oh', 2)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "nounvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[44] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "nounvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "nounvocab=nounvocabtups.collectAsMap()\n",
    "nounid2word=nounvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'jockin', u'catch', 728)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `nounvocab['dance']`\n",
    "nounid2word[0], nounvocab.keys()[5], nounvocab[nounvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the noun vocabulary?  5144\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the noun vocabulary? \", len(nounvocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create initial adj rdd from parseout\n",
    "adjrdd=sc.parallelize([ele[1] for ele in parseout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'rough'],\n",
       "  [u'troubled'],\n",
       "  [u'troubled'],\n",
       "  [u'troubled'],\n",
       "  [u'troubled'],\n",
       "  [u'troubled'],\n",
       "  [u'troubled']],\n",
       " [[u'true'], [u'blue'], [u'true'], [u'blue']],\n",
       " [[u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'important'],\n",
       "  [u'old'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'coloured'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'american'],\n",
       "  [u'coloured'],\n",
       "  [u'american'],\n",
       "  [u'leave'],\n",
       "  [u'american'],\n",
       "  [u'american']]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[46] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# straight reduce for overall word counts\n",
    "awordsrdd = (adjrdd\n",
    "             .flatMap(lambda l: l)\n",
    "             .flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'suicidal', 2),\n",
       " (u'hooked', 21),\n",
       " (u'resist', 1),\n",
       " (u'dynamic', 3),\n",
       " (u'cocky', 2)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "awordsrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'little', 1838),\n",
       " (u'good', 1727),\n",
       " (u'real', 946),\n",
       " (u'bad', 770),\n",
       " (u'new', 764),\n",
       " (u'big', 678),\n",
       " (u'true', 649),\n",
       " (u'sweet', 635),\n",
       " (u'ooh', 607),\n",
       " (u'long', 579)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n, based on values, sorted descending\n",
    "awordsrdd.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "awordsrdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(your code here)\n",
    "adjvocabtups = (awordsrdd\n",
    "              .map(lambda (x,y): x)\n",
    "              .zipWithIndex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'suicidal', 0), (u'hooked', 1), (u'resist', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view output\n",
    "adjvocabtups.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[57] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache results\n",
    "adjvocabtups.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect results\n",
    "adjvocab=adjvocabtups.collectAsMap()\n",
    "adjid2word=adjvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'suicidal', u'suspenseful', 1696)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since sampling may be used, avoiding more common usage, e.g. `adjvocab['exotic']`\n",
    "adjid2word[0], adjvocab.keys()[5], adjvocab[adjvocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How big is the adjective vocabulary?  3379\n"
     ]
    }
   ],
   "source": [
    "print \"How big is the adjective vocabulary? \", len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# CITATION - Use of counter for reduce within each word list from:\n",
    "# http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "##################################################################################################\n",
    "from collections import Counter\n",
    "\n",
    "# for each sentence, reduct into a list of tuple k,v where k=vocab index and v=count, \n",
    "# each word list is sorted by occurence\n",
    "documents = nounrdd.map(lambda words: Counter([nounvocab[word] for word in words]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5139, 1)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify output\n",
    "documents.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather spark results\n",
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Spark Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part of Speech Nouns / Adjectives (Original Lyrics Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncollect = sc.parallelize([ele[0] for ele in parseout]).collect()\n",
    "acollect = sc.parallelize([ele[1] for ele in parseout]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many noun rows?  4500\n",
      "How many adjective rows?  4500\n"
     ]
    }
   ],
   "source": [
    "print \"How many noun rows? \", len(ncollect)\n",
    "print \"How many adjective rows? \", len(acollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'time'], [u'bridge', u'water'], [u'bridge', u'water'], [u'bridge', u'water'], [u'bridge', u'water'], [u'bridge', u'water'], [u'bridge', u'water']], [[u'dream'], [u'starlight', u'eye'], [u'dream'], [u'starlight', u'eye']], [[u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman', u'mess', u'mind'], [u'woman'], [u'woman', u'mama'], [u'thing'], [u'time', u'growin'], [u'woman'], [u'woman'], [u'woman', u'mama'], [u'light'], [u'woman'], [u'woman'], [u'woman'], [u'light'], [u'woman', u'mama'], [u'ya', u'woman'], [u'woman'], [u'shit']]]\n"
     ]
    }
   ],
   "source": [
    "print ncollect[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'rough'], [u'troubled'], [u'troubled'], [u'troubled'], [u'troubled'], [u'troubled'], [u'troubled']], [[u'true'], [u'blue'], [u'true'], [u'blue']], [[u'american'], [u'american'], [u'american'], [u'american'], [u'american'], [u'american'], [u'american'], [u'american'], [u'american'], [u'important'], [u'old'], [u'american'], [u'american'], [u'american'], [u'coloured'], [u'american'], [u'american'], [u'american'], [u'coloured'], [u'american'], [u'leave'], [u'american'], [u'american']]]\n"
     ]
    }
   ],
   "source": [
    "print acollect[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save ncollect\n",
    "with open(root_out+'noun_collect.json', 'w') as fp:\n",
    "    json.dump(ncollect, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save acollect\n",
    "with open(root_out+'adj_collect.json', 'w') as fp:\n",
    "    json.dump(acollect, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Unique words per lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word Reduction per document\n",
    "def buildWordReduction(collected):\n",
    "    ngram_reduced = []\n",
    "    for r in collected:\n",
    "        v = []\n",
    "        for rr in r:\n",
    "            for i in rr:\n",
    "                if not i in v:\n",
    "                    v.append(i)\n",
    "        ngram_reduced.append(v)\n",
    "    return ngram_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nreduction = buildWordReduction(ncollect)\n",
    "areduction = buildWordReduction(acollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'woman',\n",
       " u'mess',\n",
       " u'mind',\n",
       " u'mama',\n",
       " u'thing',\n",
       " u'time',\n",
       " u'growin',\n",
       " u'light',\n",
       " u'ya',\n",
       " u'shit']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nreduction[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun word reduction\n",
    "with open(root_out+'noun-word-reduction.json', 'w') as fp:\n",
    "    json.dump(nreduction, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adj word reduction\n",
    "with open(root_out+'adj-word-reduction.json', 'w') as fp:\n",
    "    json.dump(areduction, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###N-Gram Specific\n",
    "**Want Raw n-gram for total words, then reduced n-gram for 1x per document max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun n-gram (raw)\n",
    "with open(root_out+'noun-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(nwordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adjective n-gram (raw)\n",
    "with open(root_out+'adj-n-gram.json', 'w') as fp:\n",
    "    json.dump(dict(awordsrdd.collect()), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build from nreduction and areduction to get actual counts.\n",
    "def buildNgramReduced(reduction):\n",
    "    return (sc.parallelize(reduction)\n",
    "          .flatMap(lambda word: word)\n",
    "          .map(lambda word: (word, 1))\n",
    "          .reduceByKey(lambda a, b: a + b)\n",
    "       ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_ngram_reduced = buildNgramReduced(nreduction)\n",
    "a_ngram_reduced = buildNgramReduced(areduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save reduced noun n-gram\n",
    "with open(root_out+'noun_n-gram_reduced.json', 'w') as fp:\n",
    "    json.dump(n_ngram_reduced, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save reduced adj n-gram\n",
    "with open(root_out+'adj_n-gram_reduced.json', 'w') as fp:\n",
    "    json.dump(a_ngram_reduced, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Vocab, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun vocab and id2word\n",
    "with open(root_out+'nounvocab.json', 'w') as fp:\n",
    "    json.dump(nounvocab, fp)\n",
    "    \n",
    "with open(root_out+'nounid2word.json', 'w') as fp:\n",
    "    json.dump(nounid2word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save adj vocab and id2word\n",
    "with open(root_out+'adjvocab.json', 'w') as fp:\n",
    "    json.dump(adjvocab, fp)\n",
    "    \n",
    "with open(root_out+'adjid2word.json', 'w') as fp:\n",
    "    json.dump(adjid2word, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save corpus\n",
    "pickle.dump( corpus, open( root_out+'corpus.p', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Synonym Lookups\n",
    "Focus on WordNet python package within [nltk](http://www.nltk.org) via [textblob](https://textblob.readthedocs.org/en/dev/)\n",
    "The main idea is to lookup all words in the noun and adj vocab dictionaries and attempt to collapse down -- where possible -- to synonyms. The synonyms can be used for common_support also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.wordnet import Synset\n",
    "from textblob.wordnet import NOUN\n",
    "from textblob.wordnet import ADJ\n",
    "\n",
    "SIM_THRESHOLD = 1.0 # Only act on values at/above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## COMMON METHODS FOR SYNSETS\n",
    "def synsetStr(syn):\n",
    "    \"\"\"\n",
    "    attempt to parse the string from a Synset, e.g. Synset('dog.n.01') would return 'dog'\n",
    "    return String or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return syn.name().split('.')[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def flattenSynsetValues(syn_dict, skip_invalid=True, replace_invalid=None):\n",
    "    \"\"\"\n",
    "    flatten synset values in dictionary using params\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for k,v in syn_dict.iteritems():\n",
    "        if v:\n",
    "            d[k] = synsetStr(v)\n",
    "        elif not skip_invalid:\n",
    "            d[k] = replace_invalid\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CORE FUNCTIONS FOR BUILDING SIMILARITY MATRIX\n",
    "\n",
    "def posToSingle(pos):\n",
    "    \"\"\"\n",
    "    Keep up with which pos values are implemented.\n",
    "    \"\"\"\n",
    "    if pos == NOUN:\n",
    "        return \"n\"\n",
    "    elif pos == ADJ:\n",
    "        return \"a\"\n",
    "    return None # essentially, else clause\n",
    "\n",
    "\n",
    "def cachedSynsetOrBuild(idx, syns, p, id_lookup):\n",
    "    \"\"\"\n",
    "    Build Synset for given `idx`, using the `id_lookup`.\n",
    "    Facilitate O(n) computational complexity by caching results.\n",
    "    \n",
    "    --- Input ---\n",
    "    idx: id to build and cache\n",
    "    syns: existing dictionary of synsets, with k: id, v: Synset or None\n",
    "    p: String pos value in the form needed for Synset generation, see `posToSingle`\n",
    "    id_lookup: dictionary for noun / adj to build n x n matrix of similarity.\n",
    "    \n",
    "    --- Return ---\n",
    "    Synset or None\n",
    "    \"\"\"\n",
    "    if idx in syns:\n",
    "        return syns[idx] \n",
    "        \n",
    "    # focus on `.01` only\n",
    "    try:                      \n",
    "        syn = Synset(\"{}.{}.01\".format(id_lookup[idx],p))\n",
    "        syns[idx] = syn\n",
    "        return syn\n",
    "    except Exception:\n",
    "        syns[idx] = None\n",
    "        return None\n",
    "\n",
    "def similarityMatrix(id2word, pos, take_n=None):\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    Build matrix of synsets for given id2word dictionary.    \n",
    "    Optionally, only build a similarity matrix for the first n values.\n",
    "    \n",
    "    --- Input ---    \n",
    "    id2word: dictionary for noun / adj to build n x n matrix of similarity.\n",
    "    pos: WordNet position, `NOUN` or `ADJ` imported based on needs\n",
    "    take_n: whether take the first n values for testing, default=None\n",
    "    \n",
    "    --- Return ---\n",
    "    return a tuple, t where\n",
    "    t[0]: n x n matrix with raw similarity score or zero\n",
    "    t[1]: dictionary of synsets with k: id, v: Synset or None\n",
    "    ##############################################################    \n",
    "    \"\"\"    \n",
    "    syns = {} # obtain O(n)\n",
    "    p = posToSingle(pos)\n",
    "    \n",
    "    # determine n\n",
    "    n = len(id2word)\n",
    "    if take_n:\n",
    "        n = take_n\n",
    "    \n",
    "    # n x n matrix, initialized with zeros \n",
    "    matrix = np.zeros((n,n))\n",
    "    \n",
    "    # populate\n",
    "    ns = range(n)\n",
    "    for i in ns:  \n",
    "        isyn = cachedSynsetOrBuild(i,syns,p,id2word)       \n",
    "        for j in ns:\n",
    "            # find j in synset\n",
    "            jsyn = None\n",
    "            if isyn:\n",
    "                jsyn = cachedSynsetOrBuild(j,syns,p,id2word) # no reason unless isyn is ok\n",
    "        \n",
    "            # update matrix with path_similarity between i and j words\n",
    "            if isyn and jsyn:            \n",
    "                ps = isyn.path_similarity(jsyn)            \n",
    "                if ps:\n",
    "                    matrix[i][j] = ps\n",
    "            \n",
    "    return matrix, syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS FOR EVALUATING SIMILARITY MATRIX RESULTS\n",
    "\n",
    "def getSimilarityPairs(matrix, print_n=None, id_lookup=None, sim_threshold=SIM_THRESHOLD): \n",
    "    \"\"\"\n",
    "    print non zero similarities, ignoring diagonals.\n",
    "    Optionally, show only first n non zeros then return.\n",
    "    Optionally, lookup ids with words.\n",
    "    Optionally, only evaluate values at/above a threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    ns = range(len(matrix))      \n",
    "    c = 0\n",
    "    for i in ns:\n",
    "        for j in ns:\n",
    "            v = matrix[i][j] \n",
    "            \n",
    "            # handle sim_threshold\n",
    "            met_threshold = True\n",
    "            if sim_threshold and v < sim_threshold:\n",
    "                met_threshold = False\n",
    "            elif not v:\n",
    "                met_threshold = False\n",
    "                    \n",
    "            if (i != j) and met_threshold:                \n",
    "                if not print_n or c < print_n:\n",
    "                    c += 1\n",
    "                    s_i = i\n",
    "                    s_j = j\n",
    "                    if id_lookup:\n",
    "                        s_i = id_lookup[i]\n",
    "                        s_j = id_lookup[j]\n",
    "                    if print_n:    \n",
    "                        print \"{},{} --> {}\".format(s_i,s_j,v)\n",
    "                    pairs.append((s_i,s_j))\n",
    "                elif print_n:\n",
    "                    return pairs\n",
    "    return pairs\n",
    "                \n",
    "def countSimilarityPairs(matrix, sim_threshold=SIM_THRESHOLD):\n",
    "    \"\"\"\n",
    "    count non zero similarities, ignoring diagonals.\n",
    "    Optionally, only evaluate values at/above a threshold.    \n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    ns = range(len(matrix))         \n",
    "    for i in ns:\n",
    "        for j in ns:\n",
    "            v = matrix[i][j]\n",
    "            \n",
    "            # handle sim_threshold\n",
    "            met_threshold = True\n",
    "            if sim_threshold and v < sim_threshold:\n",
    "                met_threshold = False\n",
    "            elif not v:\n",
    "                met_threshold = False\n",
    "            \n",
    "            if (i != j) and met_threshold:                \n",
    "                c += 1                    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Tue, 08 Dec 2015 03:54:57\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 29.7 ms, total: 26 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build adj similarity matrix\n",
    "asimatrix, asyns = similarityMatrix(adjid2word, ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count non-zero similarities for adjectivies at/above SIM_THRESHOLD, ignoring diagonal\n",
    "countSimilarityPairs(asimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crimson,ruby --> 1.0\n",
      "crimson,cherry --> 1.0\n",
      "crimson,scarlet --> 1.0\n",
      "crimson,red --> 1.0\n",
      "magic,magical --> 1.0\n",
      "aflame,ablaze --> 1.0\n",
      "small,little --> 1.0\n",
      "7th,seventh --> 1.0\n",
      "blue,bluish --> 1.0\n",
      "unsure,shy --> 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check adj similarity results, are they any good?\n",
    "getSimilarityPairs(asimatrix, print_n=10, id_lookup=adjid2word)\n",
    "\n",
    "# build the actual (to be dumped) variables <-- NOTE: Hypernyms will be built from here!\n",
    "asimpairs_words = getSimilarityPairs(asimatrix, id_lookup=adjid2word)\n",
    "asimpairs_ids = getSimilarityPairs(asimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(asimpairs_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution start --> Tue, 08 Dec 2015 03:55:35\n"
     ]
    }
   ],
   "source": [
    "print \"execution start --> {}\".format(time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 43s, sys: 7.78 s, total: 15min 50s\n",
      "Wall time: 15min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build noun similarity matrix (can take 30+ minutes!!!)\n",
    "nsimatrix, nsyns = similarityMatrix(nounid2word, NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count non-zero similarities for nouns at/above SIM_THRESHOLD, ignoring diagonal\n",
    "countSimilarityPairs(nsimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep,slumber --> 1.0\n",
      "prick,motherfucker --> 1.0\n",
      "prick,bastard --> 1.0\n",
      "prick,asshole --> 1.0\n",
      "chatter,yack --> 1.0\n",
      "cavity,pit --> 1.0\n",
      "topic,subject --> 1.0\n",
      "tush,ass --> 1.0\n",
      "tush,derriere --> 1.0\n",
      "tush,fanny --> 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check noun similarity results, are they any good?\n",
    "getSimilarityPairs(nsimatrix, print_n = 10, id_lookup=nounid2word)\n",
    "\n",
    "# build the actual (to be dumped) variables <-- NOTE: Hypernyms will be built from here!\n",
    "nsimpairs_words = getSimilarityPairs(nsimatrix, id_lookup=nounid2word)\n",
    "nsimpairs_ids = getSimilarityPairs(nsimatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nsimpairs_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Save Synonym work\n",
    "####Similarity Matrix and Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save asimatrix\n",
    "pickle.dump( asimatrix, open(root_out+'asimatrix.p', \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten and save asyns\n",
    "with open(root_out+'asyns.json', 'w') as fp:\n",
    "    json.dump(flattenSynsetValues(asyns), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save nsimatrix\n",
    "pickle.dump( nsimatrix, open(root_out+'nsimatrix.p', \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten and save nsyns\n",
    "with open(root_out+'nsyns.json', 'w') as fp:\n",
    "    json.dump(flattenSynsetValues(nsyns), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Similarity Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(root_out+'asimpairs_ids.json', 'w') as fp:\n",
    "    json.dump(asimpairs_ids, fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(root_out+'asimpairs_words.json', 'w') as fp:\n",
    "    json.dump(asimpairs_words, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(root_out+'nsimpairs_ids.json', 'w') as fp:\n",
    "    json.dump(nsimpairs_words, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(root_out+'nsimpairs_words.json', 'w') as fp:\n",
    "    json.dump(nsimpairs_words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Hypernyms\n",
    "find the lowest common [hypernym](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy) between similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('carnivore.n.01')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick Test\n",
    "Synset('dog.n.01').lowest_common_hypernyms(Synset('cat.n.01'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## CORE FUNCTIONS FOR BUILDING HYPERNYM -- THIS USES SIMATRIX\n",
    "\n",
    "# def makeOrderedTuple(idx1, idx2):\n",
    "#     if idx1 > idx2:\n",
    "#         return (idx2,idx1) \n",
    "#     return (idx1,idx2) \n",
    "\n",
    "# def cachedHypernymOrBuild(idx1, idx2, syn_lookup, hypes, hype_as_str=True):\n",
    "#     \"\"\"\n",
    "#     Build Hypernym for given `idxtuple`, using the `syns_lookup`.\n",
    "#     Facilitate O(n) computational complexity by caching results\n",
    "#     Will internally manage hypernym keys as ordered tuple.\n",
    "    \n",
    "#     --- Input ---\n",
    "#     idx: tuple of id to build and cache\n",
    "#     syn_lookup: existing dictionary of synsets, with k: id, v: Synset or None    \n",
    "#     hypes: dictionary for hypernyms with k: ordered tuple, v: hypernym.\n",
    "#     hype_as_str: optional build map with string values, default = True\n",
    "#     --- Return ---\n",
    "#     a hypernym Synset or None\n",
    "#     \"\"\"\n",
    "#     ituple = makeOrderedTuple(idx1,idx2)    \n",
    "#     if ituple in hypes: \n",
    "#         return hypes[ituple] \n",
    "    \n",
    "#     try:    \n",
    "#         s1 = syn_lookup[ituple[0]]\n",
    "#         s2 = syn_lookup[ituple[1]]\n",
    "#         h = s1.lowest_common_hypernyms(s2)[0]\n",
    "        \n",
    "#         if hype_as_str:\n",
    "#             h = synsetStr(h)\n",
    "            \n",
    "#         hypes[ituple] = h\n",
    "#         return h\n",
    "#     except Exception:\n",
    "#         hypes[ituple] = None\n",
    "#         return None\n",
    "\n",
    "# def lowestCommonHypernyms(simatrix, syn_lookup, sim_threshold=SIM_THRESHOLD, hype_as_str=True):\n",
    "#     \"\"\"\n",
    "#     Build a matrix with hypernym where found.\n",
    "#     Optionally, only evaluate values at/above a threshold.\n",
    "    \n",
    "#     --- Input ---\n",
    "#     simatrix: tuple of id to build and cache\n",
    "#     syn_lookup: existing dictionary of synsets, with k: id, v: Synset or None    \n",
    "#     sim_threshold: optional threshold to use for establishing hypernyms, default = SIM_THRESHOLD\n",
    "#     hype_as_str: optional build map with string values, default = True\n",
    "    \n",
    "#     --- Return ---\n",
    "#     dictionary for hypernyms with k: ordered tuple, v: Synset.    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     hypes = {} # dictionary to build up.\n",
    "    \n",
    "#     n = len(simatrix)\n",
    "#     ns = range(n)          \n",
    "#     for i in ns:\n",
    "#         for j in ns:\n",
    "#             v = simatrix[i][j] \n",
    "            \n",
    "#             # handle sim_threshold\n",
    "#             met_threshold = True\n",
    "#             if sim_threshold and v < sim_threshold:\n",
    "#                 met_threshold = False\n",
    "#             elif not v:\n",
    "#                 met_threshold = False\n",
    "                    \n",
    "#             if (i != j) and met_threshold:                                \n",
    "#                 cachedHypernymOrBuild(i,j, syn_lookup, hypes, hype_as_str)\n",
    "                \n",
    "#     return hypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CORE FUNCTIONS FOR BUILDING HYPERNYM -- THIS USES SIMPAIR\n",
    "\n",
    "def makeOrderedTuple(idx1, idx2):\n",
    "    if idx1 > idx2:\n",
    "        return (idx2,idx1) \n",
    "    return (idx1,idx2) \n",
    "\n",
    "def lowestCommonHypernyms(simpair_words, syn_pos, hype_as_str=True):\n",
    "    \"\"\"\n",
    "    Build a dict with hypernym where found.\n",
    "    \n",
    "    --- Input ---\n",
    "    simpair_words: tuple of words to build and cache\n",
    "    p: part\n",
    "    hype_as_str: optional build map with string values, default = True\n",
    "    \n",
    "    --- Return ---\n",
    "    dictionary for hypernyms with k: ordered tuple, v: Synset | String .    \n",
    "    \"\"\"\n",
    "    \n",
    "    hypes = {} # dictionary to build up.\n",
    "    \n",
    "    for ts in simpair_words:          \n",
    "        ituple = makeOrderedTuple(ts[0],ts[1])    \n",
    "        if ituple not in hypes: \n",
    "            try:                   \n",
    "                s1 = Synset(\"{}.{}.01\".format(ituple[0],syn_pos))\n",
    "                s2 = Synset(\"{}.{}.01\".format(ituple[1],syn_pos))\n",
    "                h = s1.lowest_common_hypernyms(s2)[0]\n",
    "                \n",
    "                if hype_as_str:\n",
    "                    h = synsetStr(h)\n",
    "                    \n",
    "                hypes[ituple] = h\n",
    "                \n",
    "            except Exception:\n",
    "                hypes[ituple] = None\n",
    "                \n",
    "    return hypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS FOR EVALUATING HYPERNYMS\n",
    "\n",
    "def countHypernyms(hypes, count_valid=True, count_invalid=True):\n",
    "    \"\"\"\n",
    "    Count  hypernyms, ignoring None\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    for k,v in hypes.iteritems():\n",
    "        if count_valid and v:\n",
    "            c += 1\n",
    "        elif count_invalid and not v:\n",
    "            c += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adjective Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find adj hypernyms, defaulting to only the string value\n",
    "ahypes = lowestCommonHypernyms(asimpairs_words, ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many adj hypernyms?  167\n",
      "how many valid adj hypernyms?  167\n",
      "how many invalid adj hypernyms?  0\n",
      "example key: (u'everyday', u'mundane'), value: everyday\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"how many adj hypernyms? \", countHypernyms(ahypes)\n",
    "print \"how many valid adj hypernyms? \", countHypernyms(ahypes, count_valid=True, count_invalid=False)\n",
    "print \"how many invalid adj hypernyms? \", countHypernyms(ahypes, count_valid=False, count_invalid=True)\n",
    "print \"example key: {}, value: {}\".format(ahypes.keys()[0],ahypes[ahypes.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'18th', u'eighteenth'): u'eighteenth',\n",
       " (u'5th', u'fifth'): u'fifth',\n",
       " (u'6th', u'sixth'): u'sixth',\n",
       " (u'7th', u'seventh'): u'seventh',\n",
       " (u'8th', u'eighth'): u'eighth',\n",
       " (u'ablaze', u'aflame'): u'ablaze',\n",
       " (u'ageless', u'eternal'): u'ageless',\n",
       " (u'ageless', u'everlasting'): u'ageless',\n",
       " (u'ageless', u'perpetual'): u'ageless',\n",
       " (u'all-night', u'overnight'): u'nightlong',\n",
       " (u'amazing', u'astonishing'): u'amazing',\n",
       " (u'apparent', u'evident'): u'apparent',\n",
       " (u'apparent', u'plain'): u'apparent',\n",
       " (u'average', u'mean'): u'average',\n",
       " (u'bare', u'naked'): u'bare',\n",
       " (u'bare', u'nude'): u'bare',\n",
       " (u'barren', u'desolate'): u'bare',\n",
       " (u'big', u'large'): u'large',\n",
       " (u'bigger', u'larger'): u'bigger',\n",
       " (u'bitty', u'itty-bitty'): u'bitty',\n",
       " (u'bitty', u'wee'): u'bitty',\n",
       " (u'blamed', u'damned'): u'blasted',\n",
       " (u'blond', u'blonde'): u'blond',\n",
       " (u'blue', u'bluish'): u'blue',\n",
       " (u'broad', u'wide'): u'wide',\n",
       " (u'bronzed', u'tanned'): u'bronzed',\n",
       " (u'bushy', u'shaggy'): u'bushy',\n",
       " (u'calm', u'serene'): u'calm',\n",
       " (u'casual', u'nonchalant'): u'casual',\n",
       " (u'central', u'key'): u'cardinal',\n",
       " (u'cheery', u'gay'): u'cheery',\n",
       " (u'cheery', u'sunny'): u'cheery',\n",
       " (u'cheesy', u'crummy'): u'bum',\n",
       " (u'cherry', u'crimson'): u'red',\n",
       " (u'cherry', u'red'): u'red',\n",
       " (u'cherry', u'ruby'): u'red',\n",
       " (u'cherry', u'scarlet'): u'red',\n",
       " (u'chief', u'main'): u'chief',\n",
       " (u'chopped', u'shredded'): u'chopped',\n",
       " (u'chubby', u'plump'): u'chubby',\n",
       " (u'chunky', u'lumpy'): u'chunky',\n",
       " (u'classy', u'swish'): u'classy',\n",
       " (u'coincidental', u'simultaneous'): u'coincident',\n",
       " (u'colored', u'coloured'): u'colored',\n",
       " (u'colossal', u'stupendous'): u'colossal',\n",
       " (u'comical', u'funny'): u'amusing',\n",
       " (u'conceited', u'swollen'): u'conceited',\n",
       " (u'conceited', u'vain'): u'conceited',\n",
       " (u'crack', u'super'): u'ace',\n",
       " (u'crazed', u'deranged'): u'crazed',\n",
       " (u'crimson', u'red'): u'red',\n",
       " (u'crimson', u'ruby'): u'red',\n",
       " (u'crimson', u'scarlet'): u'red',\n",
       " (u'crisp', u'sharp'): u'crisp',\n",
       " (u'cruel', u'savage'): u'barbarous',\n",
       " (u'cruel', u'vicious'): u'barbarous',\n",
       " (u'cunning', u'cute'): u'cunning',\n",
       " (u'curious', u'peculiar'): u'curious',\n",
       " (u'cutting', u'stinging'): u'cutting',\n",
       " (u'deep-fried', u'fried'): u'fried',\n",
       " (u'deluxe', u'princely'): u'deluxe',\n",
       " (u'deserving', u'worth'): u'deserving',\n",
       " (u'devilish', u'diabolical'): u'devilish',\n",
       " (u'difficult', u'hard'): u'difficult',\n",
       " (u'dizzy', u'giddy'): u'dizzy',\n",
       " (u'dizzy', u'woozy'): u'dizzy',\n",
       " (u'edgy', u'uptight'): u'edgy',\n",
       " (u'elderly', u'older'): u'aged',\n",
       " (u'enamored', u'infatuated'): u'enamored',\n",
       " (u'enormous', u'tremendous'): u'enormous',\n",
       " (u'entire', u'total'): u'entire',\n",
       " (u'ephemeral', u'passing'): u'ephemeral',\n",
       " (u'eternal', u'everlasting'): u'ageless',\n",
       " (u'eternal', u'perpetual'): u'ageless',\n",
       " (u'everlasting', u'perpetual'): u'ageless',\n",
       " (u'everyday', u'mundane'): u'everyday',\n",
       " (u'everyday', u'routine'): u'everyday',\n",
       " (u'evident', u'plain'): u'apparent',\n",
       " (u'exclusive', u'sole'): u'exclusive',\n",
       " (u'extreme', u'utmost'): u'extreme',\n",
       " (u'fake', u'phoney'): u'bogus',\n",
       " (u'fake', u'phony'): u'bogus',\n",
       " (u'far-out', u'quirky'): u'far-out',\n",
       " (u'farthest', u'furthest'): u'farthermost',\n",
       " (u'favorite', u'favourite'): u'favorite',\n",
       " (u'ferocious', u'fierce'): u'ferocious',\n",
       " (u'ferocious', u'furious'): u'ferocious',\n",
       " (u'fierce', u'furious'): u'ferocious',\n",
       " (u'fine', u'ok'): u'all_right',\n",
       " (u'fine', u'okay'): u'all_right',\n",
       " (u'firm', u'steadfast'): u'firm',\n",
       " (u'flaming', u'fucking'): u'bally',\n",
       " (u'flash', u'flashy'): u'brassy',\n",
       " (u'foul', u'yucky'): u'disgusting',\n",
       " (u'frightening', u'terrible'): u'awful',\n",
       " (u'frosty', u'icy'): u'frigid',\n",
       " (u'fuddled', u'smashed'): u'besotted',\n",
       " (u'gay', u'sunny'): u'cheery',\n",
       " (u'giddy', u'woozy'): u'dizzy',\n",
       " (u'gifted', u'talented'): u'talented',\n",
       " (u'gilded', u'golden'): u'aureate',\n",
       " (u'good-looking', u'handsome'): u'fine-looking',\n",
       " (u'goofy', u'silly'): u'cockamamie',\n",
       " (u'goofy', u'wacky'): u'cockamamie',\n",
       " (u'grateful', u'thankful'): u'grateful',\n",
       " (u'gray', u'grey'): u'grey',\n",
       " (u'grecian', u'greek'): u'greek',\n",
       " (u'grim', u'relentless'): u'grim',\n",
       " (u'groovy', u'nifty'): u'bang-up',\n",
       " (u'groovy', u'peachy'): u'bang-up',\n",
       " (u'groovy', u'smashing'): u'bang-up',\n",
       " (u'haired', u'hairy'): u'hairy',\n",
       " (u'hazy', u'misty'): u'brumous',\n",
       " (u'honest', u'honorable'): u'honest',\n",
       " (u'horny', u'randy'): u'aroused',\n",
       " (u'huge', u'vast'): u'huge',\n",
       " (u'humble', u'lowly'): u'humble',\n",
       " (u'hurt', u'wounded'): u'hurt',\n",
       " (u'icky', u'lousy'): u'icky',\n",
       " (u'icky', u'rotten'): u'icky',\n",
       " (u'icky', u'shitty'): u'icky',\n",
       " (u'ill', u'sick'): u'ill',\n",
       " (u'immaculate', u'spotless'): u'immaculate',\n",
       " (u'incredible', u'unbelievable'): u'incredible',\n",
       " (u'infamous', u'notorious'): u'ill-famed',\n",
       " (u'inscrutable', u'mysterious'): u'cryptic',\n",
       " (u'instant', u'instantaneous'): u'instantaneous',\n",
       " (u'itty-bitty', u'wee'): u'bitty',\n",
       " (u'jam-packed', u'packed'): u'jammed',\n",
       " (u'jolly', u'merry'): u'gay',\n",
       " (u'laden', u'loaded'): u'laden',\n",
       " (u'lasting', u'permanent'): u'permanent',\n",
       " (u'little', u'small'): u'small',\n",
       " (u'lone', u'lonely'): u'alone',\n",
       " (u'lousy', u'rotten'): u'icky',\n",
       " (u'lousy', u'shitty'): u'icky',\n",
       " (u'magic', u'magical'): u'charming',\n",
       " (u'marvelous', u'wonderful'): u'fantastic',\n",
       " (u'muddy', u'soggy'): u'boggy',\n",
       " (u'mundane', u'routine'): u'everyday',\n",
       " (u'naked', u'nude'): u'bare',\n",
       " (u'nauseous', u'sickening'): u'nauseating',\n",
       " (u'neglected', u'unheeded'): u'ignored',\n",
       " (u'nifty', u'peachy'): u'bang-up',\n",
       " (u'nifty', u'smashing'): u'bang-up',\n",
       " (u'noble', u'stately'): u'baronial',\n",
       " (u'null', u'void'): u'null',\n",
       " (u'ok', u'okay'): u'all_right',\n",
       " (u'particular', u'special'): u'particular',\n",
       " (u'peachy', u'smashing'): u'bang-up',\n",
       " (u'petite', u'tiny'): u'bantam',\n",
       " (u'phoney', u'phony'): u'bogus',\n",
       " (u'radical', u'ultra'): u'extremist',\n",
       " (u'red', u'ruby'): u'red',\n",
       " (u'red', u'scarlet'): u'red',\n",
       " (u'religious', u'spiritual'): u'religious',\n",
       " (u'rotten', u'shitty'): u'icky',\n",
       " (u'ruby', u'scarlet'): u'red',\n",
       " (u'savage', u'vicious'): u'barbarous',\n",
       " (u'shy', u'unsure'): u'diffident',\n",
       " (u'silly', u'wacky'): u'cockamamie',\n",
       " (u'slender', u'slim'): u'slender',\n",
       " (u'snot-nosed', u'snotty'): u'bigheaded',\n",
       " (u'swollen', u'vain'): u'conceited',\n",
       " (u'teen', u'teenage'): u'adolescent',\n",
       " (u'uncouth', u'vulgar'): u'coarse',\n",
       " (u'uncut', u'untrimmed'): u'untrimmed'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Noun Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find noun hypernyms\n",
    "nhypes = lowestCommonHypernyms(nsimpairs_words, NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many noun hypernyms?  293\n",
      "how many valid noun hypernyms?  293\n",
      "how many invalid noun hypernyms?  0\n",
      "example key: (u'material', u'stuff'), value: material\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"how many noun hypernyms? \", countHypernyms(nhypes)\n",
    "print \"how many valid noun hypernyms? \", countHypernyms(nhypes, count_valid=True, count_invalid=False)\n",
    "print \"how many invalid noun hypernyms? \", countHypernyms(nhypes, count_valid=False, count_invalid=True)\n",
    "print \"example key: {}, value: {}\".format(nhypes.keys()[0],nhypes[nhypes.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'adult', u'grownup'): u'adult',\n",
       " (u'affair', u'matter'): u'matter',\n",
       " (u'aim', u'intent'): u'purpose',\n",
       " (u'aim', u'intention'): u'purpose',\n",
       " (u'airplane', u'plane'): u'airplane',\n",
       " (u'anguish', u'torture'): u'anguish',\n",
       " (u'animal', u'beast'): u'animal',\n",
       " (u'animal', u'creature'): u'animal',\n",
       " (u'answer', u'reply'): u'answer',\n",
       " (u'arena', u'domain'): u'sphere',\n",
       " (u'ass', u'derriere'): u'buttocks',\n",
       " (u'ass', u'fanny'): u'buttocks',\n",
       " (u'ass', u'tush'): u'buttocks',\n",
       " (u'asshole', u'bastard'): u'asshole',\n",
       " (u'asshole', u'motherfucker'): u'asshole',\n",
       " (u'asshole', u'prick'): u'asshole',\n",
       " (u'athlete', u'jock'): u'athlete',\n",
       " (u'attempt', u'effort'): u'attempt',\n",
       " (u'attempt', u'try'): u'attempt',\n",
       " (u'automobile', u'car'): u'car',\n",
       " (u'autumn', u'fall'): u'fall',\n",
       " (u'babe', u'baby'): u'baby',\n",
       " (u'baggage', u'luggage'): u'baggage',\n",
       " (u'bait', u'come-on'): u'bait',\n",
       " (u'bandana', u'bandanna'): u'bandanna',\n",
       " (u'bang', u'smash'): u'knock',\n",
       " (u'bar', u'saloon'): u'barroom',\n",
       " (u'barkeeper', u'bartender'): u'bartender',\n",
       " (u'basement', u'cellar'): u'basement',\n",
       " (u'bastard', u'motherfucker'): u'asshole',\n",
       " (u'bastard', u'prick'): u'asshole',\n",
       " (u'battle', u'fight'): u'battle',\n",
       " (u'beast', u'creature'): u'animal',\n",
       " (u'blockhead', u'dumbass'): u'dunce',\n",
       " (u'bloke', u'fella'): u'chap',\n",
       " (u'bloke', u'fellow'): u'chap',\n",
       " (u'bloke', u'lad'): u'chap',\n",
       " (u'bolt', u'thunderbolt'): u'thunderbolt',\n",
       " (u'boogie', u'boogie-woogie'): u'boogie',\n",
       " (u'boom', u'roar'): u'boom',\n",
       " (u'boom', u'roaring'): u'boom',\n",
       " (u'boom', u'thunder'): u'boom',\n",
       " (u'booty', u'loot'): u'loot',\n",
       " (u'booze', u'liquor'): u'liquor',\n",
       " (u'bowel', u'gut'): u'intestine',\n",
       " (u'buddy', u'pal'): u'buddy',\n",
       " (u'bulge', u'hump'): u'bulge',\n",
       " (u'bullet', u'slug'): u'bullet',\n",
       " (u'bum', u'puke'): u'rotter',\n",
       " (u'bum', u'skunk'): u'rotter',\n",
       " (u'buster', u'dude'): u'fellow',\n",
       " (u'calamity', u'catastrophe'): u'calamity',\n",
       " (u'calamity', u'tragedy'): u'calamity',\n",
       " (u'canary', u'snitch'): u'fink',\n",
       " (u'captive', u'prisoner'): u'prisoner',\n",
       " (u'career', u'vocation'): u'career',\n",
       " (u'carpet', u'rug'): u'rug',\n",
       " (u'catastrophe', u'tragedy'): u'calamity',\n",
       " (u'cavity', u'pit'): u'pit',\n",
       " (u'center', u'middle'): u'center',\n",
       " (u'chance', u'opportunity'): u'opportunity',\n",
       " (u'chatter', u'yack'): u'yak',\n",
       " (u'check', u'cheque'): u'check',\n",
       " (u'cheep', u'peep'): u'cheep',\n",
       " (u'cheerio', u'good-bye'): u'adieu',\n",
       " (u'cheerio', u'goodbye'): u'adieu',\n",
       " (u'child', u'kid'): u'child',\n",
       " (u'chip', u'scrap'): u'bit',\n",
       " (u'chocolate', u'cocoa'): u'cocoa',\n",
       " (u'choice', u'pick'): u'choice',\n",
       " (u'chump', u'sucker'): u'chump',\n",
       " (u'chunk', u'lump'): u'ball',\n",
       " (u'clash', u'crash'): u'clang',\n",
       " (u'cloth', u'fabric'): u'fabric',\n",
       " (u'clutch', u'grip'): u'clasp',\n",
       " (u'clutch', u'hold'): u'clasp',\n",
       " (u'coast', u'seashore'): u'seashore',\n",
       " (u'come', u'semen'): u'semen',\n",
       " (u'complaint', u'ill'): u'ailment',\n",
       " (u'component', u'element'): u'component',\n",
       " (u'conceit', u'vanity'): u'amour_propre',\n",
       " (u'couch', u'lounge'): u'sofa',\n",
       " (u'couch', u'sofa'): u'sofa',\n",
       " (u'country', u'nation'): u'state',\n",
       " (u'crap', u'poop'): u'crap',\n",
       " (u'crap', u'shit'): u'crap',\n",
       " (u'craze', u'fad'): u'fad',\n",
       " (u'criminal', u'crook'): u'criminal',\n",
       " (u'criminal', u'felon'): u'criminal',\n",
       " (u'crook', u'felon'): u'criminal',\n",
       " (u'crop', u'harvest'): u'crop',\n",
       " (u'curse', u'swearing'): u'curse',\n",
       " (u'dad', u'dada'): u'dad',\n",
       " (u'dad', u'daddy'): u'dad',\n",
       " (u'dad', u'pa'): u'dad',\n",
       " (u'dad', u'papa'): u'dad',\n",
       " (u'dad', u'pop'): u'dad',\n",
       " (u'dada', u'daddy'): u'dad',\n",
       " (u'dada', u'pa'): u'dad',\n",
       " (u'dada', u'papa'): u'dad',\n",
       " (u'dada', u'pop'): u'dad',\n",
       " (u'daddy', u'pa'): u'dad',\n",
       " (u'daddy', u'papa'): u'dad',\n",
       " (u'daddy', u'pop'): u'dad',\n",
       " (u'dark', u'darkness'): u'dark',\n",
       " (u'dawn', u'sunrise'): u'dawn',\n",
       " (u'daydream', u'reverie'): u'reverie',\n",
       " (u'daze', u'shock'): u'daze',\n",
       " (u'degree', u'level'): u'degree',\n",
       " (u'derriere', u'fanny'): u'buttocks',\n",
       " (u'derriere', u'tush'): u'buttocks',\n",
       " (u'destiny', u'fate'): u'destiny',\n",
       " (u'dice', u'die'): u'die',\n",
       " (u'doctor', u'physician'): u'doctor',\n",
       " (u'drama', u'play'): u'play',\n",
       " (u'dribble', u'drip'): u'drip',\n",
       " (u'dribble', u'trickle'): u'drip',\n",
       " (u'drip', u'trickle'): u'drip',\n",
       " (u'drunk', u'wino'): u'drunkard',\n",
       " (u'dusk', u'twilight'): u'twilight',\n",
       " (u'eating', u'feeding'): u'eating',\n",
       " (u'ecstasy', u'rapture'): u'ecstasy',\n",
       " (u'effect', u'result'): u'consequence',\n",
       " (u'effort', u'try'): u'attempt',\n",
       " (u'eternity', u'infinity'): u'eternity',\n",
       " (u'expression', u'look'): u'expression',\n",
       " (u'faith', u'religion'): u'religion',\n",
       " (u'family', u'menage'): u'family',\n",
       " (u'fanny', u'tush'): u'buttocks',\n",
       " (u'fashion', u'manner'): u'manner',\n",
       " (u'fashion', u'mode'): u'manner',\n",
       " (u'fashion', u'style'): u'manner',\n",
       " (u'fashion', u'way'): u'manner',\n",
       " (u'fault', u'mistake'): u'mistake',\n",
       " (u'fella', u'fellow'): u'chap',\n",
       " (u'fella', u'lad'): u'chap',\n",
       " (u'fellow', u'lad'): u'chap',\n",
       " (u'fiddle', u'violin'): u'violin',\n",
       " (u'filet', u'fillet'): u'fillet',\n",
       " (u'film', u'movie'): u'movie',\n",
       " (u'film', u'pic'): u'movie',\n",
       " (u'filth', u'skank'): u'filth',\n",
       " (u'flavor', u'flavour'): u'spirit',\n",
       " (u'flicker', u'spark'): u'flicker',\n",
       " (u'freak', u'monstrosity'): u'freak',\n",
       " (u'fume', u'smoke'): u'smoke',\n",
       " (u'girl', u'miss'): u'girl',\n",
       " (u'girl', u'missy'): u'girl',\n",
       " (u'good-bye', u'goodbye'): u'adieu',\n",
       " (u'grand', u'thousand'): u'thousand',\n",
       " (u'granddad', u'grandpa'): u'grandfather',\n",
       " (u'grandmother', u'granny'): u'grandma',\n",
       " (u'grief', u'heartache'): u'grief',\n",
       " (u'grief', u'heartbreak'): u'grief',\n",
       " (u'grin', u'smile'): u'smile',\n",
       " (u'grip', u'hold'): u'clasp',\n",
       " (u'guarantee', u'warranty'): u'guarantee',\n",
       " (u'guardian', u'protector'): u'defender',\n",
       " (u'guy', u'hombre'): u'guy',\n",
       " (u'hall', u'hallway'): u'hallway',\n",
       " (u'hang', u'knack'): u'bent',\n",
       " (u'harm', u'hurt'): u'injury',\n",
       " (u'hate', u'hatred'): u'hate',\n",
       " (u'hazard', u'risk'): u'hazard',\n",
       " (u'headache', u'worry'): u'concern',\n",
       " (u'heap', u'pile'): u'pile',\n",
       " (u'heartache', u'heartbreak'): u'grief',\n",
       " (u'heaven', u'paradise'): u'eden',\n",
       " (u'hideaway', u'hideout'): u'hideout',\n",
       " (u'holiday', u'vacation'): u'vacation',\n",
       " (u'honkey', u'honky'): u'whitey',\n",
       " (u'hood', u'punk'): u'hood',\n",
       " (u'hood', u'thug'): u'hood',\n",
       " (u'hoopla', u'hype'): u'ballyhoo',\n",
       " (u'hush', u'stillness'): u'hush',\n",
       " (u'idea', u'thought'): u'idea',\n",
       " (u'impression', u'notion'): u'impression',\n",
       " (u'individual', u'person'): u'person',\n",
       " (u'individual', u'somebody'): u'person',\n",
       " (u'info', u'information'): u'information',\n",
       " (u'inside', u'interior'): u'inside',\n",
       " (u'intent', u'intention'): u'purpose',\n",
       " (u'internet', u'net'): u'internet',\n",
       " (u'kind', u'sort'): u'kind',\n",
       " (u'kingdom', u'realm'): u'kingdom',\n",
       " (u'laugh', u'laughter'): u'laugh',\n",
       " (u'limo', u'limousine'): u'limousine',\n",
       " (u'lollipop', u'popsicle'): u'ice_lolly',\n",
       " (u'longing', u'yearning'): u'longing',\n",
       " (u'lookout', u'picket'): u'lookout',\n",
       " (u'lookout', u'scout'): u'lookout',\n",
       " (u'lounge', u'sofa'): u'sofa',\n",
       " (u'lunch', u'luncheon'): u'lunch',\n",
       " (u'lurch', u'stumble'): u'lurch',\n",
       " (u'mac', u'mack'): u'macintosh',\n",
       " (u'mag', u'magazine'): u'magazine',\n",
       " (u'mama', u'mamma'): u'ma',\n",
       " (u'mama', u'mom'): u'ma',\n",
       " (u'mama', u'momma'): u'ma',\n",
       " (u'mama', u'mommy'): u'ma',\n",
       " (u'mamma', u'mom'): u'ma',\n",
       " (u'mamma', u'momma'): u'ma',\n",
       " (u'mamma', u'mommy'): u'ma',\n",
       " (u'manner', u'mode'): u'manner',\n",
       " (u'manner', u'style'): u'manner',\n",
       " (u'manner', u'way'): u'manner',\n",
       " (u'mark', u'score'): u'mark',\n",
       " (u'marriage', u'matrimony'): u'marriage',\n",
       " (u'material', u'stuff'): u'material',\n",
       " (u'measure', u'step'): u'measure',\n",
       " (u'melody', u'tune'): u'tune',\n",
       " (u'menace', u'threat'): u'menace',\n",
       " (u'microphone', u'mike'): u'microphone',\n",
       " (u'millimeter', u'mm'): u'millimeter',\n",
       " (u'mini', u'miniskirt'): u'miniskirt',\n",
       " (u'miss', u'missy'): u'girl',\n",
       " (u'mode', u'style'): u'manner',\n",
       " (u'mode', u'way'): u'manner',\n",
       " (u'mold', u'stamp'): u'cast',\n",
       " (u'mom', u'momma'): u'ma',\n",
       " (u'mom', u'mommy'): u'ma',\n",
       " (u'momma', u'mommy'): u'ma',\n",
       " (u'moonlight', u'moonshine'): u'moonlight',\n",
       " (u'morn', u'morning'): u'morning',\n",
       " (u'motherfucker', u'prick'): u'asshole',\n",
       " (u'motivation', u'motive'): u'motivation',\n",
       " (u'movie', u'pic'): u'movie',\n",
       " (u'neighborhood', u'neighbourhood'): u'vicinity',\n",
       " (u'nigga', u'nigger'): u'nigger',\n",
       " (u'pa', u'papa'): u'dad',\n",
       " (u'pa', u'pop'): u'dad',\n",
       " (u'package', u'packet'): u'package',\n",
       " (u'palette', u'pallet'): u'palette',\n",
       " (u'papa', u'pop'): u'dad',\n",
       " (u'past', u'yesteryear'): u'past',\n",
       " (u'pay', u'salary'): u'wage',\n",
       " (u'pay', u'wage'): u'wage',\n",
       " (u'pedestal', u'stand'): u'base',\n",
       " (u'pee', u'piss'): u'urine',\n",
       " (u'pee', u'urine'): u'urine',\n",
       " (u'perfume', u'scent'): u'aroma',\n",
       " (u'person', u'somebody'): u'person',\n",
       " (u'perspective', u'view'): u'position',\n",
       " (u'phase', u'stage'): u'phase',\n",
       " (u'phone', u'telephone'): u'telephone',\n",
       " (u'photo', u'photograph'): u'photograph',\n",
       " (u'picket', u'scout'): u'lookout',\n",
       " (u'piss', u'urine'): u'urine',\n",
       " (u'place', u'spot'): u'topographic_point',\n",
       " (u'plan', u'program'): u'plan',\n",
       " (u'poop', u'shit'): u'crap',\n",
       " (u'porn', u'porno'): u'pornography',\n",
       " (u'prance', u'strut'): u'strut',\n",
       " (u'puke', u'skunk'): u'rotter',\n",
       " (u'pun', u'wordplay'): u'pun',\n",
       " (u'punk', u'thug'): u'hood',\n",
       " (u'roar', u'roaring'): u'boom',\n",
       " (u'roar', u'thunder'): u'boom',\n",
       " (u'roaring', u'thunder'): u'boom',\n",
       " (u'rock', u'stone'): u'rock',\n",
       " (u'rover', u'wanderer'): u'wanderer',\n",
       " (u'rumor', u'rumour'): u'rumor',\n",
       " (u'salary', u'wage'): u'wage',\n",
       " (u'savior', u'saviour'): u'jesus',\n",
       " (u'scheme', u'strategy'): u'scheme',\n",
       " (u'scream', u'shriek'): u'scream',\n",
       " (u'sec', u'second'): u'second',\n",
       " (u'shake', u'shingle'): u'shingle',\n",
       " (u'shooting', u'shot'): u'shooting',\n",
       " (u'shop', u'store'): u'shop',\n",
       " (u'shout', u'yell'): u'cry',\n",
       " (u'sis', u'sister'): u'sister',\n",
       " (u'sleep', u'slumber'): u'sleep',\n",
       " (u'speaker', u'talker'): u'speaker',\n",
       " (u'spell', u'trance'): u'enchantment',\n",
       " (u'spring', u'springtime'): u'spring',\n",
       " (u'story', u'tale'): u'narrative',\n",
       " (u'style', u'way'): u'manner',\n",
       " (u'subject', u'topic'): u'subject',\n",
       " (u'suburb', u'suburbia'): u'suburb',\n",
       " (u'summer', u'summertime'): u'summer',\n",
       " (u'sundown', u'sunset'): u'sunset',\n",
       " (u'sunlight', u'sunshine'): u'sunlight',\n",
       " (u'superstar', u'whiz'): u'ace',\n",
       " (u'superstar', u'wizard'): u'ace',\n",
       " (u'sweetheart', u'sweetie'): u'sweetheart',\n",
       " (u'tit', u'titty'): u'breast',\n",
       " (u'touch', u'touching'): u'touch',\n",
       " (u'universe', u'world'): u'universe',\n",
       " (u'waist', u'waistline'): u'waist',\n",
       " (u'walk', u'walking'): u'walk',\n",
       " (u'whiz', u'wizard'): u'ace',\n",
       " (u'zero', u'zip'): u'nothing'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save adj hypernyms\n",
    "pickle.dump( ahypes, open(root_out+'ahypes.p', \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save noun hypernyms\n",
    "pickle.dump( nhypes, open(root_out+'nhypes.p', \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New: do some conversion for a json file\n",
    "\n",
    "def saveHypesAsJson(hypes,json_name,root_out=root_out):\n",
    "    h = {}\n",
    "    hkeys = [] #hypernym keys\n",
    "    \n",
    "    for ts,v in hypes.iteritems():\n",
    "        if not v in hkeys:\n",
    "            hkeys.append(v)\n",
    "            \n",
    "    for ts,v in hypes.iteritems():\n",
    "        if v in h:\n",
    "            s = h[v]\n",
    "            if ts[0] not in s:\n",
    "                s.append(ts[0])\n",
    "            if ts[1] not in s:\n",
    "                s.append(ts[1])\n",
    "        else:\n",
    "            h[v] = []\n",
    "            h[v].append(ts[0])\n",
    "            h[v].append(ts[1])\n",
    "    \n",
    "    # save h\n",
    "    with open(root_out+ json_name + '.json', 'w') as fp:\n",
    "        json.dump(h, fp)\n",
    "        \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ace': [u'superstar', u'whiz', u'wizard'],\n",
       " u'adieu': [u'cheerio', u'good-bye', u'goodbye'],\n",
       " u'adult': [u'adult', u'grownup'],\n",
       " u'ailment': [u'complaint', u'ill'],\n",
       " u'airplane': [u'airplane', u'plane'],\n",
       " u'amour_propre': [u'conceit', u'vanity'],\n",
       " u'anguish': [u'anguish', u'torture'],\n",
       " u'animal': [u'animal', u'creature', u'beast'],\n",
       " u'answer': [u'answer', u'reply'],\n",
       " u'aroma': [u'perfume', u'scent'],\n",
       " u'asshole': [u'motherfucker', u'prick', u'asshole', u'bastard'],\n",
       " u'athlete': [u'athlete', u'jock'],\n",
       " u'attempt': [u'attempt', u'effort', u'try'],\n",
       " u'baby': [u'babe', u'baby'],\n",
       " u'baggage': [u'baggage', u'luggage'],\n",
       " u'bait': [u'bait', u'come-on'],\n",
       " u'ball': [u'chunk', u'lump'],\n",
       " u'ballyhoo': [u'hoopla', u'hype'],\n",
       " u'bandanna': [u'bandana', u'bandanna'],\n",
       " u'barroom': [u'bar', u'saloon'],\n",
       " u'bartender': [u'barkeeper', u'bartender'],\n",
       " u'base': [u'pedestal', u'stand'],\n",
       " u'basement': [u'basement', u'cellar'],\n",
       " u'battle': [u'battle', u'fight'],\n",
       " u'bent': [u'hang', u'knack'],\n",
       " u'bit': [u'chip', u'scrap'],\n",
       " u'boogie': [u'boogie', u'boogie-woogie'],\n",
       " u'boom': [u'boom', u'roar', u'roaring', u'thunder'],\n",
       " u'breast': [u'tit', u'titty'],\n",
       " u'buddy': [u'buddy', u'pal'],\n",
       " u'bulge': [u'bulge', u'hump'],\n",
       " u'bullet': [u'bullet', u'slug'],\n",
       " u'buttocks': [u'derriere', u'fanny', u'ass', u'tush'],\n",
       " u'calamity': [u'calamity', u'catastrophe', u'tragedy'],\n",
       " u'car': [u'automobile', u'car'],\n",
       " u'career': [u'career', u'vocation'],\n",
       " u'cast': [u'mold', u'stamp'],\n",
       " u'center': [u'center', u'middle'],\n",
       " u'chap': [u'bloke', u'fella', u'lad', u'fellow'],\n",
       " u'check': [u'check', u'cheque'],\n",
       " u'cheep': [u'cheep', u'peep'],\n",
       " u'child': [u'child', u'kid'],\n",
       " u'choice': [u'choice', u'pick'],\n",
       " u'chump': [u'chump', u'sucker'],\n",
       " u'clang': [u'clash', u'crash'],\n",
       " u'clasp': [u'clutch', u'grip', u'hold'],\n",
       " u'cocoa': [u'chocolate', u'cocoa'],\n",
       " u'component': [u'component', u'element'],\n",
       " u'concern': [u'headache', u'worry'],\n",
       " u'consequence': [u'effect', u'result'],\n",
       " u'crap': [u'poop', u'shit', u'crap'],\n",
       " u'criminal': [u'criminal', u'crook', u'felon'],\n",
       " u'crop': [u'crop', u'harvest'],\n",
       " u'cry': [u'shout', u'yell'],\n",
       " u'curse': [u'curse', u'swearing'],\n",
       " u'dad': [u'daddy', u'papa', u'dad', u'dada', u'pop', u'pa'],\n",
       " u'dark': [u'dark', u'darkness'],\n",
       " u'dawn': [u'dawn', u'sunrise'],\n",
       " u'daze': [u'daze', u'shock'],\n",
       " u'defender': [u'guardian', u'protector'],\n",
       " u'degree': [u'degree', u'level'],\n",
       " u'destiny': [u'destiny', u'fate'],\n",
       " u'die': [u'dice', u'die'],\n",
       " u'doctor': [u'doctor', u'physician'],\n",
       " u'drip': [u'dribble', u'drip', u'trickle'],\n",
       " u'drunkard': [u'drunk', u'wino'],\n",
       " u'dunce': [u'blockhead', u'dumbass'],\n",
       " u'eating': [u'eating', u'feeding'],\n",
       " u'ecstasy': [u'ecstasy', u'rapture'],\n",
       " u'eden': [u'heaven', u'paradise'],\n",
       " u'enchantment': [u'spell', u'trance'],\n",
       " u'eternity': [u'eternity', u'infinity'],\n",
       " u'expression': [u'expression', u'look'],\n",
       " u'fabric': [u'cloth', u'fabric'],\n",
       " u'fad': [u'craze', u'fad'],\n",
       " u'fall': [u'autumn', u'fall'],\n",
       " u'family': [u'family', u'menage'],\n",
       " u'fellow': [u'buster', u'dude'],\n",
       " u'fillet': [u'filet', u'fillet'],\n",
       " u'filth': [u'filth', u'skank'],\n",
       " u'fink': [u'canary', u'snitch'],\n",
       " u'flicker': [u'flicker', u'spark'],\n",
       " u'freak': [u'freak', u'monstrosity'],\n",
       " u'girl': [u'girl', u'missy', u'miss'],\n",
       " u'grandfather': [u'granddad', u'grandpa'],\n",
       " u'grandma': [u'grandmother', u'granny'],\n",
       " u'grief': [u'heartache', u'heartbreak', u'grief'],\n",
       " u'guarantee': [u'guarantee', u'warranty'],\n",
       " u'guy': [u'guy', u'hombre'],\n",
       " u'hallway': [u'hall', u'hallway'],\n",
       " u'hate': [u'hate', u'hatred'],\n",
       " u'hazard': [u'hazard', u'risk'],\n",
       " u'hideout': [u'hideaway', u'hideout'],\n",
       " u'hood': [u'punk', u'thug', u'hood'],\n",
       " u'hush': [u'hush', u'stillness'],\n",
       " u'ice_lolly': [u'lollipop', u'popsicle'],\n",
       " u'idea': [u'idea', u'thought'],\n",
       " u'impression': [u'impression', u'notion'],\n",
       " u'information': [u'info', u'information'],\n",
       " u'injury': [u'harm', u'hurt'],\n",
       " u'inside': [u'inside', u'interior'],\n",
       " u'internet': [u'internet', u'net'],\n",
       " u'intestine': [u'bowel', u'gut'],\n",
       " u'jesus': [u'savior', u'saviour'],\n",
       " u'kind': [u'kind', u'sort'],\n",
       " u'kingdom': [u'kingdom', u'realm'],\n",
       " u'knock': [u'bang', u'smash'],\n",
       " u'laugh': [u'laugh', u'laughter'],\n",
       " u'limousine': [u'limo', u'limousine'],\n",
       " u'liquor': [u'booze', u'liquor'],\n",
       " u'longing': [u'longing', u'yearning'],\n",
       " u'lookout': [u'lookout', u'scout', u'picket'],\n",
       " u'loot': [u'booty', u'loot'],\n",
       " u'lunch': [u'lunch', u'luncheon'],\n",
       " u'lurch': [u'lurch', u'stumble'],\n",
       " u'ma': [u'momma', u'mommy', u'mamma', u'mom', u'mama'],\n",
       " u'macintosh': [u'mac', u'mack'],\n",
       " u'magazine': [u'mag', u'magazine'],\n",
       " u'manner': [u'mode', u'style', u'fashion', u'manner', u'way'],\n",
       " u'mark': [u'mark', u'score'],\n",
       " u'marriage': [u'marriage', u'matrimony'],\n",
       " u'material': [u'material', u'stuff'],\n",
       " u'matter': [u'affair', u'matter'],\n",
       " u'measure': [u'measure', u'step'],\n",
       " u'menace': [u'menace', u'threat'],\n",
       " u'microphone': [u'microphone', u'mike'],\n",
       " u'millimeter': [u'millimeter', u'mm'],\n",
       " u'miniskirt': [u'mini', u'miniskirt'],\n",
       " u'mistake': [u'fault', u'mistake'],\n",
       " u'moonlight': [u'moonlight', u'moonshine'],\n",
       " u'morning': [u'morn', u'morning'],\n",
       " u'motivation': [u'motivation', u'motive'],\n",
       " u'movie': [u'film', u'pic', u'movie'],\n",
       " u'narrative': [u'story', u'tale'],\n",
       " u'nigger': [u'nigga', u'nigger'],\n",
       " u'nothing': [u'zero', u'zip'],\n",
       " u'opportunity': [u'chance', u'opportunity'],\n",
       " u'package': [u'package', u'packet'],\n",
       " u'palette': [u'palette', u'pallet'],\n",
       " u'past': [u'past', u'yesteryear'],\n",
       " u'person': [u'person', u'somebody', u'individual'],\n",
       " u'phase': [u'phase', u'stage'],\n",
       " u'photograph': [u'photo', u'photograph'],\n",
       " u'pile': [u'heap', u'pile'],\n",
       " u'pit': [u'cavity', u'pit'],\n",
       " u'plan': [u'plan', u'program'],\n",
       " u'play': [u'drama', u'play'],\n",
       " u'pornography': [u'porn', u'porno'],\n",
       " u'position': [u'perspective', u'view'],\n",
       " u'prisoner': [u'captive', u'prisoner'],\n",
       " u'pun': [u'pun', u'wordplay'],\n",
       " u'purpose': [u'aim', u'intent', u'intention'],\n",
       " u'religion': [u'faith', u'religion'],\n",
       " u'reverie': [u'daydream', u'reverie'],\n",
       " u'rock': [u'rock', u'stone'],\n",
       " u'rotter': [u'bum', u'skunk', u'puke'],\n",
       " u'rug': [u'carpet', u'rug'],\n",
       " u'rumor': [u'rumor', u'rumour'],\n",
       " u'scheme': [u'scheme', u'strategy'],\n",
       " u'scream': [u'scream', u'shriek'],\n",
       " u'seashore': [u'coast', u'seashore'],\n",
       " u'second': [u'sec', u'second'],\n",
       " u'semen': [u'come', u'semen'],\n",
       " u'shingle': [u'shake', u'shingle'],\n",
       " u'shooting': [u'shooting', u'shot'],\n",
       " u'shop': [u'shop', u'store'],\n",
       " u'sister': [u'sis', u'sister'],\n",
       " u'sleep': [u'sleep', u'slumber'],\n",
       " u'smile': [u'grin', u'smile'],\n",
       " u'smoke': [u'fume', u'smoke'],\n",
       " u'sofa': [u'lounge', u'sofa', u'couch'],\n",
       " u'speaker': [u'speaker', u'talker'],\n",
       " u'sphere': [u'arena', u'domain'],\n",
       " u'spirit': [u'flavor', u'flavour'],\n",
       " u'spring': [u'spring', u'springtime'],\n",
       " u'state': [u'country', u'nation'],\n",
       " u'strut': [u'prance', u'strut'],\n",
       " u'subject': [u'subject', u'topic'],\n",
       " u'suburb': [u'suburb', u'suburbia'],\n",
       " u'summer': [u'summer', u'summertime'],\n",
       " u'sunlight': [u'sunlight', u'sunshine'],\n",
       " u'sunset': [u'sundown', u'sunset'],\n",
       " u'sweetheart': [u'sweetheart', u'sweetie'],\n",
       " u'telephone': [u'phone', u'telephone'],\n",
       " u'thousand': [u'grand', u'thousand'],\n",
       " u'thunderbolt': [u'bolt', u'thunderbolt'],\n",
       " u'topographic_point': [u'place', u'spot'],\n",
       " u'touch': [u'touch', u'touching'],\n",
       " u'tune': [u'melody', u'tune'],\n",
       " u'twilight': [u'dusk', u'twilight'],\n",
       " u'universe': [u'universe', u'world'],\n",
       " u'urine': [u'pee', u'urine', u'piss'],\n",
       " u'vacation': [u'holiday', u'vacation'],\n",
       " u'vicinity': [u'neighborhood', u'neighbourhood'],\n",
       " u'violin': [u'fiddle', u'violin'],\n",
       " u'wage': [u'pay', u'wage', u'salary'],\n",
       " u'waist': [u'waist', u'waistline'],\n",
       " u'walk': [u'walk', u'walking'],\n",
       " u'wanderer': [u'rover', u'wanderer'],\n",
       " u'whitey': [u'honkey', u'honky'],\n",
       " u'yak': [u'chatter', u'yack']}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "njhypes = saveHypesAsJson(nhypes,'noun_hype_syns_words')\n",
    "njhypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ablaze': [u'ablaze', u'aflame'],\n",
       " u'ace': [u'crack', u'super'],\n",
       " u'adolescent': [u'teen', u'teenage'],\n",
       " u'aged': [u'elderly', u'older'],\n",
       " u'ageless': [u'ageless', u'perpetual', u'everlasting', u'eternal'],\n",
       " u'all_right': [u'fine', u'okay', u'ok'],\n",
       " u'alone': [u'lone', u'lonely'],\n",
       " u'amazing': [u'amazing', u'astonishing'],\n",
       " u'amusing': [u'comical', u'funny'],\n",
       " u'apparent': [u'apparent', u'plain', u'evident'],\n",
       " u'aroused': [u'horny', u'randy'],\n",
       " u'aureate': [u'gilded', u'golden'],\n",
       " u'average': [u'average', u'mean'],\n",
       " u'awful': [u'frightening', u'terrible'],\n",
       " u'bally': [u'flaming', u'fucking'],\n",
       " u'bang-up': [u'groovy', u'smashing', u'peachy', u'nifty'],\n",
       " u'bantam': [u'petite', u'tiny'],\n",
       " u'barbarous': [u'cruel', u'savage', u'vicious'],\n",
       " u'bare': [u'bare', u'naked', u'nude', u'barren', u'desolate'],\n",
       " u'baronial': [u'noble', u'stately'],\n",
       " u'besotted': [u'fuddled', u'smashed'],\n",
       " u'bigger': [u'bigger', u'larger'],\n",
       " u'bigheaded': [u'snot-nosed', u'snotty'],\n",
       " u'bitty': [u'bitty', u'itty-bitty', u'wee'],\n",
       " u'blasted': [u'blamed', u'damned'],\n",
       " u'blond': [u'blond', u'blonde'],\n",
       " u'blue': [u'blue', u'bluish'],\n",
       " u'boggy': [u'muddy', u'soggy'],\n",
       " u'bogus': [u'fake', u'phony', u'phoney'],\n",
       " u'brassy': [u'flash', u'flashy'],\n",
       " u'bronzed': [u'bronzed', u'tanned'],\n",
       " u'brumous': [u'hazy', u'misty'],\n",
       " u'bum': [u'cheesy', u'crummy'],\n",
       " u'bushy': [u'bushy', u'shaggy'],\n",
       " u'calm': [u'calm', u'serene'],\n",
       " u'cardinal': [u'central', u'key'],\n",
       " u'casual': [u'casual', u'nonchalant'],\n",
       " u'charming': [u'magic', u'magical'],\n",
       " u'cheery': [u'gay', u'sunny', u'cheery'],\n",
       " u'chief': [u'chief', u'main'],\n",
       " u'chopped': [u'chopped', u'shredded'],\n",
       " u'chubby': [u'chubby', u'plump'],\n",
       " u'chunky': [u'chunky', u'lumpy'],\n",
       " u'classy': [u'classy', u'swish'],\n",
       " u'coarse': [u'uncouth', u'vulgar'],\n",
       " u'cockamamie': [u'goofy', u'silly', u'wacky'],\n",
       " u'coincident': [u'coincidental', u'simultaneous'],\n",
       " u'colored': [u'colored', u'coloured'],\n",
       " u'colossal': [u'colossal', u'stupendous'],\n",
       " u'conceited': [u'conceited', u'vain', u'swollen'],\n",
       " u'crazed': [u'crazed', u'deranged'],\n",
       " u'crisp': [u'crisp', u'sharp'],\n",
       " u'cryptic': [u'inscrutable', u'mysterious'],\n",
       " u'cunning': [u'cunning', u'cute'],\n",
       " u'curious': [u'curious', u'peculiar'],\n",
       " u'cutting': [u'cutting', u'stinging'],\n",
       " u'deluxe': [u'deluxe', u'princely'],\n",
       " u'deserving': [u'deserving', u'worth'],\n",
       " u'devilish': [u'devilish', u'diabolical'],\n",
       " u'difficult': [u'difficult', u'hard'],\n",
       " u'diffident': [u'shy', u'unsure'],\n",
       " u'disgusting': [u'foul', u'yucky'],\n",
       " u'dizzy': [u'dizzy', u'giddy', u'woozy'],\n",
       " u'edgy': [u'edgy', u'uptight'],\n",
       " u'eighteenth': [u'18th', u'eighteenth'],\n",
       " u'eighth': [u'8th', u'eighth'],\n",
       " u'enamored': [u'enamored', u'infatuated'],\n",
       " u'enormous': [u'enormous', u'tremendous'],\n",
       " u'entire': [u'entire', u'total'],\n",
       " u'ephemeral': [u'ephemeral', u'passing'],\n",
       " u'everyday': [u'everyday', u'mundane', u'routine'],\n",
       " u'exclusive': [u'exclusive', u'sole'],\n",
       " u'extreme': [u'extreme', u'utmost'],\n",
       " u'extremist': [u'radical', u'ultra'],\n",
       " u'fantastic': [u'marvelous', u'wonderful'],\n",
       " u'far-out': [u'far-out', u'quirky'],\n",
       " u'farthermost': [u'farthest', u'furthest'],\n",
       " u'favorite': [u'favorite', u'favourite'],\n",
       " u'ferocious': [u'ferocious', u'furious', u'fierce'],\n",
       " u'fifth': [u'5th', u'fifth'],\n",
       " u'fine-looking': [u'good-looking', u'handsome'],\n",
       " u'firm': [u'firm', u'steadfast'],\n",
       " u'fried': [u'deep-fried', u'fried'],\n",
       " u'frigid': [u'frosty', u'icy'],\n",
       " u'gay': [u'jolly', u'merry'],\n",
       " u'grateful': [u'grateful', u'thankful'],\n",
       " u'greek': [u'grecian', u'greek'],\n",
       " u'grey': [u'gray', u'grey'],\n",
       " u'grim': [u'grim', u'relentless'],\n",
       " u'hairy': [u'haired', u'hairy'],\n",
       " u'honest': [u'honest', u'honorable'],\n",
       " u'huge': [u'huge', u'vast'],\n",
       " u'humble': [u'humble', u'lowly'],\n",
       " u'hurt': [u'hurt', u'wounded'],\n",
       " u'icky': [u'icky', u'shitty', u'lousy', u'rotten'],\n",
       " u'ignored': [u'neglected', u'unheeded'],\n",
       " u'ill': [u'ill', u'sick'],\n",
       " u'ill-famed': [u'infamous', u'notorious'],\n",
       " u'immaculate': [u'immaculate', u'spotless'],\n",
       " u'incredible': [u'incredible', u'unbelievable'],\n",
       " u'instantaneous': [u'instant', u'instantaneous'],\n",
       " u'jammed': [u'jam-packed', u'packed'],\n",
       " u'laden': [u'laden', u'loaded'],\n",
       " u'large': [u'big', u'large'],\n",
       " u'nauseating': [u'nauseous', u'sickening'],\n",
       " u'nightlong': [u'all-night', u'overnight'],\n",
       " u'null': [u'null', u'void'],\n",
       " u'particular': [u'particular', u'special'],\n",
       " u'permanent': [u'lasting', u'permanent'],\n",
       " u'red': [u'red', u'ruby', u'scarlet', u'crimson', u'cherry'],\n",
       " u'religious': [u'religious', u'spiritual'],\n",
       " u'seventh': [u'7th', u'seventh'],\n",
       " u'sixth': [u'6th', u'sixth'],\n",
       " u'slender': [u'slender', u'slim'],\n",
       " u'small': [u'little', u'small'],\n",
       " u'talented': [u'gifted', u'talented'],\n",
       " u'untrimmed': [u'uncut', u'untrimmed'],\n",
       " u'wide': [u'broad', u'wide']}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajhypes = saveHypesAsJson(ahypes,'adj_hype_syns_words')\n",
    "ajhypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compare Synonym and Hypernym Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareWordLists(alist,blist):\n",
    "    same = []\n",
    "    ina = []\n",
    "    inb = []\n",
    "    \n",
    "    for a in alist:\n",
    "        if a in blist:\n",
    "            same.append(a)\n",
    "        else:\n",
    "            ina.append(a)\n",
    "    \n",
    "    for b in blist:\n",
    "        if b not in same:\n",
    "            inb.append(b)\n",
    "    return sorted(same), sorted(ina), sorted(inb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FULL noun syn versus syn-hype words...\n",
      "\tHow many are same?  0\n",
      "\tHow many are only in syn?  3580\n",
      "\tHow many are only in hype?  201\n",
      "\n",
      "For FULL adj syn versus syn-hype words...\n",
      "\tHow many are same?  0\n",
      "\tHow many are only in syn?  1707\n",
      "\tHow many are only in hype?  118\n"
     ]
    }
   ],
   "source": [
    "tncomp = compareWordLists(flattenSynsetValues(nsyns).values(),njhypes.values())\n",
    "print \"For FULL noun syn versus syn-hype words...\"\n",
    "print \"\\tHow many are same? \", len(tncomp[0])\n",
    "print \"\\tHow many are only in syn? \", len(tncomp[1])\n",
    "print \"\\tHow many are only in hype? \", len(tncomp[2])\n",
    "print\n",
    "tacomp = compareWordLists(flattenSynsetValues(asyns).values(),ajhypes.values())\n",
    "print \"For FULL adj syn versus syn-hype words...\"\n",
    "print \"\\tHow many are same? \", len(tacomp[0])\n",
    "print \"\\tHow many are only in syn? \", len(tacomp[1])\n",
    "print \"\\tHow many are only in hype? \", len(tacomp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenListOfLists(alist):\n",
    "    v = []\n",
    "    for a in alist:\n",
    "        for x in a:\n",
    "            v.append(x)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ONLY hypernym relevant nouns, syn versus hype words...\n",
      "\tHow many are same?  153\n",
      "\tHow many are only in syn?  287\n",
      "\tHow many are only in hype?  48\n",
      "\n",
      "For ONLY hypernym relevant adj, syn versus hype words...\n",
      "\tHow many are same?  78\n",
      "\tHow many are only in syn?  181\n",
      "\tHow many are only in hype?  40\n"
     ]
    }
   ],
   "source": [
    "incomp = compareWordLists(flattenListOfLists(njhypes.values()),njhypes.keys())\n",
    "print \"For ONLY hypernym relevant nouns, syn versus hype words...\"\n",
    "print \"\\tHow many are same? \", len(incomp[0])\n",
    "print \"\\tHow many are only in syn? \", len(incomp[1])\n",
    "print \"\\tHow many are only in hype? \", len(incomp[2])\n",
    "print\n",
    "iacomp = compareWordLists(flattenListOfLists(ajhypes.values()),ajhypes.keys())\n",
    "print \"For ONLY hypernym relevant adj, syn versus hype words...\"\n",
    "print \"\\tHow many are same? \", len(iacomp[0])\n",
    "print \"\\tHow many are only in syn? \", len(iacomp[1])\n",
    "print \"\\tHow many are only in hype? \", len(iacomp[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
