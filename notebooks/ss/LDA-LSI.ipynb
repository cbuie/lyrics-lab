{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Trendy or Timeless?\n",
    "\n",
    "\n",
    "Popular music is on the forefront of what the people consider new and cool, but it can also reflect shared values that are more persistent, even timeless. Leaving aside the ancient question of whether art imitates life or life imitates art, are there some concepts so firmly rooted in collective human belief systems that they remain constant across time? While a fully generalizable answer to that question may not be possible, some insights can be obtained from analysis of the content of the lyrics of popular music, and comparing them across time. \n",
    "\n",
    "This inquiry is primarily exploratory, but by proceeding at multiple levels of abstraction it is hoped that some conclusions can be drawn. More concrete metrics, such as word count, song length, and lexical diversity (non-repetitiveness) help establish a heuristic baseline. As one might expect, the word \"love\" appears very frequently in song lyrics, but not always in a positive context. It may be possible to learn more by deeper examination.  Another potentially useful metric of the sentiment content of music lyrics is obtained by treating the artist as a proxy (literally the embodiment) for the music's semantic content, and tracing the persistence versus evanescence of individual artists or bands over time.  And of course, some more modern text mining techniques, Latent Semantic Analysis (LSI) Latent Dirichlet Analysis (LDA), will be used to extract models of what the songs are about.\n",
    "\n",
    "All of these inquiries use the same data set, a sample consisting of the lyrics for songs listed in the Billboard Top 100 for each year from 1970 through 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load additional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate spark: (note this will only work after vagrant is up):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nimport os\\nimport findspark\\nfindspark.init()\\nprint findspark.find()\\nimport pyspark\\nconf = (pyspark.SparkConf()\\n    .setMaster(\\'local[4]\\')\\n    .setAppName(\\'pyspark\\')\\n    .set(\"spark.executor.memory\", \"2g\"))\\nsc = pyspark.SparkContext(conf=conf)\\nsc._conf.getAll()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local[4]')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc._conf.getAll()\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nimport sys\\nrdd = sc.parallelize(xrange(10),10)\\nrdd.map(lambda x: sys.version).collect()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open everything that's been saved in /data/conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open everything that's been saved in /data/conditioned\n",
    "\n",
    "df=pd.read_csv(\"../../data/conditioned/use-this-master-lyricsdf-extracted.csv\")\n",
    "dfg=pd.read_csv(\"../../data/conditioned/master-lyricsdf-genre_inner.csv\")\n",
    "\n",
    "with open(\"../../data/conditioned/noun-n-gram.json\") as json_file:\n",
    "    noungram = json.load(json_file)\n",
    "with open(\"../../data/conditioned/nounvocab.json\") as json_file:\n",
    "    nounvocab = json.load(json_file)\n",
    "with open(\"../../data/conditioned/nounid2word.json\") as json_file:\n",
    "    nounid2word = json.load(json_file)\n",
    "with open(\"../../data/conditioned/adj-n-gram.json\") as json_file:\n",
    "    adjgram = json.load(json_file)\n",
    "with open(\"../../data/conditioned/adjvocab.json\") as json_file:\n",
    "    adjvocab = json.load(json_file)\n",
    "with open(\"../../data/conditioned/adjid2word.json\") as json_file:\n",
    "    adjvocab = json.load(json_file)\n",
    "with open(\"../../data/conditioned/decade-dict.json\") as json_file:\n",
    "    decade_dict = json.load(json_file)\n",
    "    \n",
    "\n",
    "f = open(\"../../data/conditioned/ahypes.p\",'r')  \n",
    "ahypes = pickle.load(f)  \n",
    "f = open(\"../../data/conditioned/nhypes.p\",'r')  \n",
    "nhypes = pickle.load(f)\n",
    "f = open(\"../../data/conditioned/corpus.p\",'r')  \n",
    "corpus = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 11), (2946, 316))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decades=df.decade.unique()\n",
    "df.shape, dfg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Data Cleaning- should be able to eliminate with new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sss\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:1825: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4341, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate \" We don't currently have a license\"\n",
    "counter=0\n",
    "#eliminate rows with null lyrics\n",
    "dfc = df[pd.isnull(df.lyrics)==False]\n",
    "\n",
    "\n",
    "for row in dfc.iterrows():\n",
    "    if row[1][6].startswith(\"We don't currently have a license\"):\n",
    "        dfc.iloc[row[1][1],6]=\"Instrumental\"\n",
    "    counter+=1\n",
    "    if counter >3000:\n",
    "        break\n",
    "\n",
    "#eliminate instrumentals\n",
    "dfc = dfc[df.lyrics!=\"Instrumental\"]\n",
    "# eliminate \" \tWe don't currently have a license\"\n",
    "\n",
    "dfc.shape\n",
    "#dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised Machine Learning  \n",
    "\n",
    "Fortunately, most of the hard work in making LSA and LDA models operational is accomplished by the `gensim` library for Python, which resulted from the [Phd Dissertation of Radim Hurek](http://radimrehurek.com/phd_rehurek.pdf). Hurek provides an exelent accessible dicussion of the logic behind textual anal;ysis, beginning with the statistical semantics hypothesis:\n",
    "\n",
    "> Statistical patterns of human word usage can be used to figure out what people mean. \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "The first use of gensim LSI and LDA is to extract topics across the entire data set, spanning years 1970-2014. The number of topic nodes can be changed by adjusting the `numtopics` variable; the value of `showtopics` determines how many of the extracted topics will be displayed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because the index numbers were in string form in the dict\n",
    "id2w=dict()\n",
    "run = dict()\n",
    "for k in nounid2word:\n",
    "    id2w[int(k)]=nounid2word[k]   \n",
    "   \n",
    "    # this sets the parameters for all the runs to follow.\n",
    "ntopics=80 # topics for LDA\n",
    "nfeatures = 300   #features for LSI\n",
    "nwords=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function makes reading the output easier\n",
    "# dectops is the master dict of topics keyed by decade\n",
    "# remmeber decade 1000 is the result of running across all years\n",
    "# howmany is the number of topics to display\n",
    "# years is a list of the years to display, defaults to whole set\n",
    "def printreadable(dectops,howmany,years=decades):\n",
    "    for dec in years:\n",
    "        howmany = min(howmany,ntopics)\n",
    "        print \"Top \"+str(howmany)+\" of \"+str(ntopics)+\" Topics for decade \"+str(dec)+\":\"\n",
    "        for i in range(0,howmany):\n",
    "            print \"\\n- - - -Topic \"+str(i)+\"- - - - \"\n",
    "            for j in range(0,howmany):\n",
    "                print dectops[dec][i][j][1], dectops[dec][i][j][0]\n",
    "        print '-----------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing (LSI) Analysis \n",
    "\n",
    "Latent Semantic Indexing, also called Latent Semantic Analysis, was first described in Deerwester, et al, (1990)  [Indexing by Latent Semantic Analysis](http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf). According to the [gensim documentation](https://radimrehurek.com/gensim/tut2.html), \"target dimensionality of 200-500 is recommended as a 'golden standard.'\"  Accordingly, for LSI 300 features are selected. We first use LSI to extract topic features across the entire data set, then do the same for the LDA model, which is an extension of LSI. Then the sample is partitioned according to decade, and both processes are applied to each partition separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeatures = 300\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=corpus, id2word=id2w, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_decade_topics = {}\n",
    "\n",
    "tlist = lsi.print_topics(nwords)\n",
    "\n",
    "i=0\n",
    "lsidict={}\n",
    "for t in tlist:\n",
    "    u = t.split(' + ')\n",
    "    topictuple= [v.split('*') for v in u]\n",
    "    lsidict[i]= topictuple\n",
    "    i +=1\n",
    "\n",
    "lsi_decade_topics[1000]=lsidict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 of 80 Topics for decade 1000:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"hitta\" 0.990\n",
      "\"day\" 0.054\n",
      "\"shit\" 0.046\n",
      "\"ride\" 0.044\n",
      "\"finger\" 0.042\n",
      "\"trigger\" 0.042\n",
      "\"motherfucking\" 0.042\n",
      "\"bitch\" 0.034\n",
      "\"love\" 0.032\n",
      "\"drink\" 0.021\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"love\" 0.807\n",
      "\"baby\" 0.508\n",
      "\"girl\" 0.169\n",
      "\"time\" 0.133\n",
      "\"thing\" 0.087\n",
      "\"night\" 0.074\n",
      "\"cheep\" 0.072\n",
      "\"momma\" 0.061\n",
      "\"way\" 0.055\n",
      "\"man\" 0.047\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"baby\" 0.727\n",
      "\"love\" -0.576\n",
      "\"girl\" 0.264\n",
      "\"cheep\" 0.144\n",
      "\"momma\" 0.122\n",
      "\"man\" 0.069\n",
      "\"bird\" 0.062\n",
      "\"night\" 0.061\n",
      "\"thing\" 0.055\n",
      "\"time\" 0.046\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"girl\" 0.900\n",
      "\"baby\" -0.339\n",
      "\"man\" 0.126\n",
      "\"thing\" 0.112\n",
      "\"cheep\" -0.088\n",
      "\"momma\" -0.072\n",
      "\"pop\" 0.070\n",
      "\"time\" 0.068\n",
      "\"lock\" 0.067\n",
      "\"way\" 0.056\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"time\" -0.937\n",
      "\"thing\" -0.192\n",
      "\"girl\" 0.154\n",
      "\"touch\" -0.133\n",
      "\"baby\" 0.099\n",
      "\"love\" 0.098\n",
      "\"hand\" -0.074\n",
      "\"night\" -0.049\n",
      "\"life\" -0.038\n",
      "\"applause\" -0.038\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"thing\" -0.644\n",
      "\"touch\" -0.574\n",
      "\"hand\" -0.306\n",
      "\"time\" 0.282\n",
      "\"applause\" -0.161\n",
      "\"girl\" 0.121\n",
      "\"make\" -0.114\n",
      "\"loud\" -0.107\n",
      "\"man\" -0.057\n",
      "\"way\" -0.052\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"night\" 0.727\n",
      "\"cheep\" 0.419\n",
      "\"momma\" 0.359\n",
      "\"baby\" -0.239\n",
      "\"bird\" 0.183\n",
      "\"day\" 0.141\n",
      "\"poppa\" 0.123\n",
      "\"song\" 0.109\n",
      "\"morning\" 0.080\n",
      "\"singing\" 0.071\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"thing\" -0.523\n",
      "\"touch\" 0.497\n",
      "\"man\" -0.489\n",
      "\"hand\" 0.277\n",
      "\"day\" -0.159\n",
      "\"applause\" 0.136\n",
      "\"girl\" 0.131\n",
      "\"life\" -0.123\n",
      "\"cheep\" 0.117\n",
      "\"momma\" 0.098\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"man\" -0.694\n",
      "\"thing\" 0.468\n",
      "\"touch\" -0.262\n",
      "\"night\" -0.196\n",
      "\"way\" -0.191\n",
      "\"hand\" -0.170\n",
      "\"cheep\" 0.150\n",
      "\"momma\" 0.123\n",
      "\"day\" -0.122\n",
      "\"boy\" -0.116\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"night\" 0.481\n",
      "\"man\" -0.441\n",
      "\"cheep\" -0.418\n",
      "\"momma\" -0.353\n",
      "\"day\" 0.345\n",
      "\"bird\" -0.182\n",
      "\"way\" 0.176\n",
      "\"poppa\" -0.123\n",
      "\"life\" 0.118\n",
      "\"baby\" 0.117\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printreadable(lsi_decade_topics,10,[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:80 Words: 12\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Topics:\"+str(ntopics)+\" Words: \"+str(nwords)\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2w, num_topics=ntopics, update_every=1, chunksize=100, passes=3)\n",
    "topicsobject= lda.print_topics(num_topics=ntopics,num_words=nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_decade_topics={}  # this dict will hold decade results, also this one for convenience\n",
    "\n",
    "topicsdict={}\n",
    "topicnumber=0\n",
    "for t in topicsobject:  #store them all even if only some will be displayed\n",
    "    topictuple= [d.split('*') for d in t.split(' + ')]\n",
    "    topicsdict[topicnumber] = topictuple\n",
    "    topicnumber += 1\n",
    "\n",
    "lda_decade_topics[1000] = topicsdict   # all-year results stored as \"decade\" 1000   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 of 80 Topics for decade 1000:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "heart 0.340\n",
      "kind 0.111\n",
      "smile 0.096\n",
      "hair 0.078\n",
      "train 0.047\n",
      "door 0.043\n",
      "ice 0.038\n",
      "morning 0.031\n",
      "bang 0.028\n",
      "ticket 0.024\n",
      "sea 0.017\n",
      "bucket 0.017\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "seat 0.223\n",
      "jean 0.203\n",
      "inside 0.119\n",
      "couple 0.070\n",
      "suit 0.055\n",
      "fool 0.051\n",
      "queen 0.036\n",
      "da 0.036\n",
      "la 0.030\n",
      "lung 0.013\n",
      "hitta 0.012\n",
      "bench 0.011\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "man 0.386\n",
      "shot 0.204\n",
      "fight 0.041\n",
      "picture 0.039\n",
      "king 0.037\n",
      "think 0.036\n",
      "tune 0.030\n",
      "castle 0.027\n",
      "mix 0.017\n",
      "note 0.014\n",
      "square 0.011\n",
      "needle 0.011\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "lover 0.121\n",
      "notch 0.092\n",
      "lie 0.063\n",
      "filth 0.063\n",
      "answer 0.061\n",
      "shawty 0.059\n",
      "la-laaa 0.057\n",
      "shine 0.050\n",
      "bite 0.043\n",
      "panamera 0.040\n",
      "belt 0.035\n",
      "alcohol 0.028\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "hitta 0.238\n",
      "finger 0.180\n",
      "competition 0.045\n",
      "church 0.039\n",
      "tobacco 0.038\n",
      "shotgun 0.036\n",
      "league 0.034\n",
      "doll 0.032\n",
      "difference 0.031\n",
      "record 0.028\n",
      "mommy 0.022\n",
      "interlock 0.015\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "tear 0.265\n",
      "thought 0.154\n",
      "nothin 0.117\n",
      "shoulder 0.102\n",
      "honey 0.071\n",
      "contigo 0.035\n",
      "aisle 0.022\n",
      "cryin 0.014\n",
      "mask 0.014\n",
      "outcome 0.008\n",
      "companion 0.005\n",
      "etiquette 0.004\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "hoe 0.239\n",
      "cup 0.131\n",
      "flame 0.130\n",
      "air 0.087\n",
      "beer 0.068\n",
      "tongue 0.044\n",
      "lipstick 0.043\n",
      "wake 0.039\n",
      "wrist 0.037\n",
      "season 0.025\n",
      "mascara 0.025\n",
      "bus 0.018\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "thing 0.861\n",
      "lot 0.041\n",
      "dealer 0.010\n",
      "sure 0.006\n",
      "yah 0.005\n",
      "mhmm 0.005\n",
      "applause 0.003\n",
      "plain 0.003\n",
      "hitta 0.002\n",
      "neighbor 0.002\n",
      "mmhm 0.001\n",
      "bing 0.001\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "view 0.087\n",
      "little 0.083\n",
      "pound 0.075\n",
      "week 0.061\n",
      "gram 0.061\n",
      "boot 0.056\n",
      "point 0.056\n",
      "treasure 0.054\n",
      "hill 0.032\n",
      "tellin 0.026\n",
      "string 0.021\n",
      "bow 0.020\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "standing 0.183\n",
      "truck 0.171\n",
      "news 0.066\n",
      "model 0.048\n",
      "mile 0.043\n",
      "fame 0.041\n",
      "flavor 0.037\n",
      "building 0.034\n",
      "tower 0.031\n",
      "revenge 0.026\n",
      "buck 0.024\n",
      "dawn 0.018\n",
      "\n",
      "- - - -Topic 10- - - - \n",
      "problem 0.341\n",
      "room 0.153\n",
      "beast 0.067\n",
      "figure 0.064\n",
      "sex 0.047\n",
      "band 0.038\n",
      "finding 0.031\n",
      "hitta 0.025\n",
      "step 0.016\n",
      "closer 0.015\n",
      "gown 0.014\n",
      "climax 0.013\n",
      "\n",
      "- - - -Topic 11- - - - \n",
      "feel 0.119\n",
      "sugar 0.101\n",
      "dimensin 0.058\n",
      "partition 0.056\n",
      "faker 0.052\n",
      "bomb 0.039\n",
      "streetlight 0.036\n",
      "en 0.036\n",
      "future 0.032\n",
      "broken 0.029\n",
      "liking 0.026\n",
      "lottery 0.026\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printreadable(lda_decade_topics,nwords,[1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for bow in corpus[2:4400:440]:\n",
    "#    print bow\n",
    "#    print lda.get_document_topics(bow)\n",
    "#    print \" \".join([id2w[e[0]] for e in bow])\n",
    "#    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Breaking It Down by Decade\n",
    "\n",
    "The corpus for each decade has been separately generated and saved in a separate subdirectory using the name corpus each time. To analyze by decade, we therefore use identical code operating on different working directories.\n",
    "\n",
    "\n",
    "### LSA by Decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 6031\n",
      "1980 6447\n",
      "1990 7795\n",
      "2000 10398\n",
      "2010 4801\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "decades = [\"1970\",\"1980\",\"1990\",\"2000\",\"2010\"]\n",
    "for dec in decades:\n",
    "    filename=\"../../data/conditioned/decades/\"+dec+\"/corpus\"+dec+\".p\"\n",
    "    f = open(filename,'r')  \n",
    "    corpus = pickle.load(f)\n",
    "    print dec,len(corpus)\n",
    "    lsi = gensim.models.lsimodel.LsiModel(corpus=corpus, id2word=id2w, num_topics=ntopics)\n",
    "    tlist = lsi.print_topics(nwords)\n",
    "    i=0\n",
    "    lsidict={}\n",
    "    for t in tlist:\n",
    "        u = t.split(' + ')\n",
    "        topictuple= [v.split('*') for v in u]\n",
    "        lsidict[i]= topictuple\n",
    "        i +=1\n",
    "    lsi_decade_topics[int(dec)]=lsidict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 of 80 Topics for decade 1970:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"schoolgirl\" 0.634\n",
      "\"glare\" 0.529\n",
      "\"passin\" 0.382\n",
      "\"monsoon\" 0.271\n",
      "\"goner\" 0.185\n",
      "\"hoss\" 0.123\n",
      "\"downtown\" 0.115\n",
      "\"groove\" 0.107\n",
      "\"intellectualism\" 0.107\n",
      "\"icky\" 0.106\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"doobie\" 0.864\n",
      "\"dancey\" 0.504\n",
      "\"prejudice\" 0.009\n",
      "\"true\" -0.000\n",
      "\"declinin\" 0.000\n",
      "\"pinky\" 0.000\n",
      "\"airwave\" 0.000\n",
      "\"baby-baby-baby\" -0.000\n",
      "\"finisher\" 0.000\n",
      "\"salary\" 0.000\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"sound\" -0.981\n",
      "\"split\" -0.082\n",
      "\"passin\" -0.076\n",
      "\"duct\" -0.050\n",
      "\"tip\" -0.048\n",
      "\"schoolgirl\" 0.044\n",
      "\"freek\" -0.040\n",
      "\"ridin\" -0.038\n",
      "\"glare\" 0.037\n",
      "\"judge\" -0.036\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"duct\" 0.815\n",
      "\"passin\" 0.474\n",
      "\"macaroni\" 0.151\n",
      "\"schoolgirl\" -0.139\n",
      "\"glare\" -0.116\n",
      "\"sound\" -0.105\n",
      "\"operating\" 0.096\n",
      "\"necklace\" 0.073\n",
      "\"coco\" 0.071\n",
      "\"monsoon\" -0.063\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"passin\" 0.746\n",
      "\"duct\" -0.547\n",
      "\"schoolgirl\" -0.207\n",
      "\"glare\" -0.173\n",
      "\"operating\" 0.121\n",
      "\"monsoon\" -0.095\n",
      "\"cologne\" 0.086\n",
      "\"split\" 0.084\n",
      "\"goner\" -0.060\n",
      "\"sound\" -0.059\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"split\" -0.974\n",
      "\"passin\" 0.106\n",
      "\"stewardess\" -0.101\n",
      "\"mutha-faker\" -0.097\n",
      "\"sound\" 0.073\n",
      "\"hoss\" -0.060\n",
      "\"bicycle\" -0.041\n",
      "\"downtown\" -0.034\n",
      "\"herrre\" -0.033\n",
      "\"necklace\" -0.032\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"ridin\" -0.992\n",
      "\"pit\" -0.075\n",
      "\"baseball\" -0.055\n",
      "\"sound\" 0.038\n",
      "\"knick-a-boxer\" -0.032\n",
      "\"cologne\" -0.030\n",
      "\"hero\" -0.029\n",
      "\"battle\" -0.027\n",
      "\"tale\" -0.019\n",
      "\"slave\" -0.012\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"necklace\" -0.875\n",
      "\"cologne\" -0.362\n",
      "\"hoss\" -0.217\n",
      "\"nuttin\" -0.075\n",
      "\"bicycle\" -0.075\n",
      "\"passin\" 0.071\n",
      "\"duct\" 0.067\n",
      "\"reminisce\" -0.066\n",
      "\"operating\" -0.064\n",
      "\"split\" 0.060\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"cologne\" 0.886\n",
      "\"necklace\" -0.392\n",
      "\"bicycle\" 0.147\n",
      "\"passin\" -0.098\n",
      "\"operating\" 0.080\n",
      "\"reminisce\" 0.045\n",
      "\"macaroni\" 0.040\n",
      "\"hotline\" 0.037\n",
      "\"prioritizin\" 0.033\n",
      "\"ridin\" -0.028\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"nuttin\" 0.978\n",
      "\"necklace\" -0.099\n",
      "\"reminisce\" 0.072\n",
      "\"susie\" 0.071\n",
      "\"downtown\" 0.065\n",
      "\"hoss\" 0.063\n",
      "\"bicycle\" 0.062\n",
      "\"cologne\" -0.046\n",
      "\"operating\" -0.032\n",
      "\"passin\" -0.025\n",
      "-----------------------------------------------\n",
      "\n",
      "Top 10 of 80 Topics for decade 1980:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"plasma\" 0.941\n",
      "\"shoot\" 0.302\n",
      "\"buddha\" 0.086\n",
      "\"kiddy\" 0.052\n",
      "\"subway\" 0.047\n",
      "\"cuff\" 0.043\n",
      "\"bury\" 0.041\n",
      "\"verdict\" 0.039\n",
      "\"mam\" 0.033\n",
      "\"picnic\" 0.026\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"subway\" -0.983\n",
      "\"shoot\" -0.121\n",
      "\"plasma\" 0.095\n",
      "\"squaw\" -0.046\n",
      "\"iceberg\" -0.038\n",
      "\"gentleman\" -0.037\n",
      "\"cuff\" -0.026\n",
      "\"perseverance\" -0.025\n",
      "\"buddha\" -0.021\n",
      "\"sheet\" -0.020\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"shoot\" 0.903\n",
      "\"plasma\" -0.313\n",
      "\"buddha\" 0.183\n",
      "\"subway\" -0.148\n",
      "\"kiddy\" 0.067\n",
      "\"perseverance\" 0.062\n",
      "\"bury\" 0.057\n",
      "\"cuff\" 0.053\n",
      "\"mam\" -0.050\n",
      "\"verdict\" 0.050\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"mam\" 0.959\n",
      "\"dip\" 0.120\n",
      "\"perseverance\" 0.116\n",
      "\"squaw\" 0.106\n",
      "\"pup\" 0.082\n",
      "\"redbone\" 0.079\n",
      "\"buddha\" 0.059\n",
      "\"macaroni\" 0.051\n",
      "\"foreigner\" 0.051\n",
      "\"plasma\" -0.050\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"squaw\" 0.943\n",
      "\"perseverance\" 0.185\n",
      "\"mam\" -0.148\n",
      "\"establishment\" 0.101\n",
      "\"impatient\" 0.092\n",
      "\"trainer\" 0.071\n",
      "\"foreigner\" 0.065\n",
      "\"pup\" 0.062\n",
      "\"palm\" 0.059\n",
      "\"metal\" 0.056\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"perseverance\" 0.936\n",
      "\"squaw\" -0.219\n",
      "\"macaroni\" 0.127\n",
      "\"mam\" -0.107\n",
      "\"foreigner\" 0.101\n",
      "\"gentleman\" 0.101\n",
      "\"shoot\" -0.088\n",
      "\"satin\" 0.035\n",
      "\"redbone\" 0.035\n",
      "\"fractal\" 0.034\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"sheet\" -0.952\n",
      "\"gentleman\" -0.247\n",
      "\"redbone\" -0.077\n",
      "\"perseverance\" 0.073\n",
      "\"pup\" -0.070\n",
      "\"foreigner\" -0.052\n",
      "\"mam\" 0.051\n",
      "\"macaroni\" -0.045\n",
      "\"metal\" -0.035\n",
      "\"subway\" 0.035\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"buddha\" -0.916\n",
      "\"shoot\" 0.222\n",
      "\"eruption\" -0.198\n",
      "\"redbone\" 0.107\n",
      "\"kiddy\" -0.095\n",
      "\"bury\" -0.091\n",
      "\"cuff\" -0.090\n",
      "\"verdict\" -0.071\n",
      "\"perseverance\" 0.065\n",
      "\"metal\" -0.053\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"gentleman\" -0.602\n",
      "\"foreigner\" -0.473\n",
      "\"redbone\" -0.383\n",
      "\"macaroni\" -0.367\n",
      "\"sheet\" 0.251\n",
      "\"perseverance\" 0.203\n",
      "\"peer\" -0.068\n",
      "\"squaw\" 0.056\n",
      "\"mam\" 0.049\n",
      "\"buddha\" -0.047\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"redbone\" -0.842\n",
      "\"gentleman\" 0.474\n",
      "\"foreigner\" 0.148\n",
      "\"macaroni\" -0.106\n",
      "\"buddha\" -0.088\n",
      "\"mam\" 0.064\n",
      "\"sheet\" -0.061\n",
      "\"shoot\" 0.054\n",
      "\"dress\" 0.035\n",
      "\"pledge\" -0.034\n",
      "-----------------------------------------------\n",
      "\n",
      "Top 10 of 80 Topics for decade 1990:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"dip\" -0.886\n",
      "\"lon\" -0.332\n",
      "\"noose\" -0.231\n",
      "\"asleep\" -0.084\n",
      "\"baseline\" -0.081\n",
      "\"pickin\" -0.069\n",
      "\"perspective\" -0.063\n",
      "\"60\" -0.059\n",
      "\"heartache\" -0.056\n",
      "\"knick-a-boxer\" -0.043\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"lon\" 0.804\n",
      "\"dip\" -0.434\n",
      "\"noose\" 0.271\n",
      "\"perspective\" 0.107\n",
      "\"baseline\" 0.101\n",
      "\"guitar\" 0.099\n",
      "\"asleep\" 0.094\n",
      "\"fiance\" 0.077\n",
      "\"pickin\" 0.063\n",
      "\"prize\" 0.063\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"noose\" -0.880\n",
      "\"lon\" 0.392\n",
      "\"pickin\" -0.154\n",
      "\"dip\" 0.122\n",
      "\"60\" -0.073\n",
      "\"heartache\" -0.068\n",
      "\"bong\" -0.045\n",
      "\"wannabe\" -0.039\n",
      "\"goal\" -0.039\n",
      "\"perspective\" -0.038\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"pickin\" -0.970\n",
      "\"noose\" 0.172\n",
      "\"gasp\" -0.110\n",
      "\"baseline\" 0.060\n",
      "\"guitar\" 0.059\n",
      "\"perspective\" 0.032\n",
      "\"bargain\" -0.029\n",
      "\"motherfucking\" -0.025\n",
      "\"phillie\" 0.025\n",
      "\"asleep\" 0.024\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"guitar\" -0.602\n",
      "\"baseline\" -0.436\n",
      "\"asleep\" -0.378\n",
      "\"lon\" 0.247\n",
      "\"gasp\" -0.229\n",
      "\"perspective\" -0.164\n",
      "\"noose\" 0.155\n",
      "\"motherfucking\" -0.130\n",
      "\"jack\" -0.112\n",
      "\"prize\" -0.109\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"gasp\" 0.768\n",
      "\"broken\" 0.346\n",
      "\"grate\" 0.332\n",
      "\"phillie\" 0.268\n",
      "\"guitar\" -0.167\n",
      "\"baseline\" -0.148\n",
      "\"pickin\" -0.114\n",
      "\"perspective\" -0.074\n",
      "\"gender\" 0.062\n",
      "\"gypsy\" 0.060\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"broken\" -0.629\n",
      "\"asleep\" -0.492\n",
      "\"gasp\" 0.452\n",
      "\"guitar\" 0.245\n",
      "\"phillie\" -0.185\n",
      "\"jack\" -0.159\n",
      "\"gypsy\" -0.086\n",
      "\"motherfucking\" -0.075\n",
      "\"baseline\" 0.069\n",
      "\"grate\" -0.065\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"phillie\" -0.820\n",
      "\"asleep\" 0.299\n",
      "\"guitar\" -0.271\n",
      "\"gasp\" 0.236\n",
      "\"field\" -0.143\n",
      "\"grate\" -0.135\n",
      "\"trick-or-treat\" -0.129\n",
      "\"ternos\" -0.100\n",
      "\"jack\" 0.094\n",
      "\"baseline\" -0.094\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"broken\" 0.629\n",
      "\"asleep\" -0.548\n",
      "\"phillie\" -0.318\n",
      "\"guitar\" 0.306\n",
      "\"jack\" -0.200\n",
      "\"motherfucking\" -0.115\n",
      "\"gasp\" -0.106\n",
      "\"gypsy\" 0.097\n",
      "\"baseline\" 0.085\n",
      "\"grate\" 0.078\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"grate\" 0.902\n",
      "\"broken\" -0.207\n",
      "\"phillie\" -0.198\n",
      "\"gasp\" -0.192\n",
      "\"motherfucking\" -0.126\n",
      "\"baseline\" 0.119\n",
      "\"guitar\" -0.103\n",
      "\"battle\" 0.070\n",
      "\"perspective\" 0.058\n",
      "\"jack\" -0.057\n",
      "-----------------------------------------------\n",
      "\n",
      "Top 10 of 80 Topics for decade 2000:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"pourin\" 0.838\n",
      "\"shelf\" 0.232\n",
      "\"mountain\" 0.222\n",
      "\"diesel\" 0.220\n",
      "\"lon\" 0.200\n",
      "\"spirit\" 0.168\n",
      "\"yacht\" 0.111\n",
      "\"description\" 0.106\n",
      "\"fine\" 0.075\n",
      "\"slappin\" 0.073\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"prejudice\" -0.665\n",
      "\"spirit\" 0.498\n",
      "\"shelf\" 0.262\n",
      "\"diesel\" 0.232\n",
      "\"pourin\" -0.197\n",
      "\"keen\" 0.145\n",
      "\"yacht\" -0.141\n",
      "\"mountain\" -0.134\n",
      "\"modification\" 0.118\n",
      "\"slappin\" 0.083\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"prejudice\" 0.669\n",
      "\"spirit\" 0.439\n",
      "\"pourin\" -0.345\n",
      "\"lon\" 0.198\n",
      "\"modification\" 0.193\n",
      "\"shelf\" 0.183\n",
      "\"plea\" 0.160\n",
      "\"diesel\" 0.160\n",
      "\"keen\" 0.141\n",
      "\"mannie\" 0.089\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"mountain\" -0.639\n",
      "\"lon\" -0.441\n",
      "\"yacht\" -0.313\n",
      "\"pourin\" 0.276\n",
      "\"modification\" -0.275\n",
      "\"prejudice\" 0.223\n",
      "\"spirit\" 0.152\n",
      "\"hang\" -0.149\n",
      "\"plea\" -0.130\n",
      "\"policy\" -0.062\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"modification\" 0.834\n",
      "\"mountain\" -0.367\n",
      "\"spirit\" -0.215\n",
      "\"hang\" 0.178\n",
      "\"plea\" 0.153\n",
      "\"pourin\" 0.137\n",
      "\"yacht\" -0.100\n",
      "\"lon\" -0.080\n",
      "\"keen\" -0.072\n",
      "\"shelf\" -0.057\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"lon\" -0.707\n",
      "\"mountain\" 0.424\n",
      "\"spirit\" 0.381\n",
      "\"modification\" 0.228\n",
      "\"keen\" 0.173\n",
      "\"yacht\" 0.149\n",
      "\"shelf\" -0.141\n",
      "\"diesel\" -0.129\n",
      "\"description\" -0.071\n",
      "\"slappin\" -0.050\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"plea\" 0.845\n",
      "\"modification\" -0.260\n",
      "\"yacht\" 0.239\n",
      "\"mountain\" -0.210\n",
      "\"prejudice\" -0.163\n",
      "\"hang\" 0.131\n",
      "\"hillside\" 0.120\n",
      "\"mannie\" 0.112\n",
      "\"spirit\" 0.099\n",
      "\"lon\" -0.080\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"hillside\" -0.743\n",
      "\"hang\" 0.555\n",
      "\"spirit\" 0.142\n",
      "\"description\" -0.125\n",
      "\"mountain\" -0.116\n",
      "\"shelf\" -0.115\n",
      "\"plea\" -0.112\n",
      "\"modification\" -0.102\n",
      "\"diesel\" -0.101\n",
      "\"yacht\" 0.089\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"hillside\" 0.657\n",
      "\"hang\" 0.610\n",
      "\"plea\" -0.284\n",
      "\"spirit\" 0.139\n",
      "\"shelf\" -0.114\n",
      "\"description\" -0.108\n",
      "\"diesel\" -0.100\n",
      "\"lon\" 0.099\n",
      "\"modification\" -0.094\n",
      "\"keen\" 0.089\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"hang\" -0.476\n",
      "\"lon\" 0.363\n",
      "\"description\" -0.346\n",
      "\"spirit\" 0.319\n",
      "\"shelf\" -0.298\n",
      "\"yacht\" 0.272\n",
      "\"diesel\" -0.257\n",
      "\"mountain\" -0.221\n",
      "\"keen\" 0.171\n",
      "\"modification\" 0.124\n",
      "-----------------------------------------------\n",
      "\n",
      "Top 10 of 80 Topics for decade 2010:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "\"serenity\" 0.991\n",
      "\"spinnin\" 0.043\n",
      "\"iceberg\" 0.043\n",
      "\"susie\" 0.042\n",
      "\"redhead\" 0.042\n",
      "\"heater\" 0.042\n",
      "\"tommorow\" 0.042\n",
      "\"bubble\" 0.033\n",
      "\"excuse\" 0.021\n",
      "\"possession\" 0.021\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "\"wrist\" -0.815\n",
      "\"whack\" -0.388\n",
      "\"x6\" -0.242\n",
      "\"mam\" -0.241\n",
      "\"thurr\" -0.160\n",
      "\"jail\" -0.159\n",
      "\"order\" -0.054\n",
      "\"snatchin\" -0.053\n",
      "\"terror\" -0.053\n",
      "\"odl\" -0.053\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "\"delivery\" 0.996\n",
      "\"ooh\" 0.041\n",
      "\"rescue\" 0.038\n",
      "\"tumor\" 0.026\n",
      "\"puedo\" 0.025\n",
      "\"volume\" 0.021\n",
      "\"x6\" 0.020\n",
      "\"intellectualism\" 0.018\n",
      "\"wrist\" -0.014\n",
      "\"doo-doo-mmm\" 0.013\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "\"puedo\" 0.980\n",
      "\"poupon\" 0.165\n",
      "\"seis\" 0.039\n",
      "\"yep\" 0.035\n",
      "\"trick\" 0.030\n",
      "\"delivery\" -0.029\n",
      "\"bubble\" 0.028\n",
      "\"ooh\" 0.021\n",
      "\"rescue\" 0.021\n",
      "\"misuse\" 0.021\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "\"chiefin\" -0.493\n",
      "\"bubble\" -0.370\n",
      "\"x6\" -0.301\n",
      "\"trick\" -0.271\n",
      "\"spinnin\" -0.261\n",
      "\"ooh\" -0.218\n",
      "\"tamale\" -0.193\n",
      "\"fragment\" -0.161\n",
      "\"streak\" -0.147\n",
      "\"track\" -0.141\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "\"ooh\" -0.690\n",
      "\"episode\" -0.594\n",
      "\"shh\" -0.158\n",
      "\"schoolgirl\" -0.148\n",
      "\"chiefin\" 0.142\n",
      "\"t-t-towel\" -0.097\n",
      "\"beep\" -0.083\n",
      "\"spinnin\" 0.077\n",
      "\"rescue\" -0.071\n",
      "\"trick\" 0.071\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "\"episode\" -0.768\n",
      "\"ooh\" 0.581\n",
      "\"schoolgirl\" 0.121\n",
      "\"rescue\" 0.079\n",
      "\"grammar\" -0.075\n",
      "\"beep\" -0.070\n",
      "\"chiefin\" -0.070\n",
      "\"t-t-towel\" -0.064\n",
      "\"hush\" 0.057\n",
      "\"shh\" -0.048\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "\"intellectualism\" 0.901\n",
      "\"rescue\" 0.206\n",
      "\"slave\" 0.193\n",
      "\"x6\" 0.174\n",
      "\"bubble\" -0.117\n",
      "\"chiefin\" -0.114\n",
      "\"ooh\" -0.098\n",
      "\"blaz-on\" 0.086\n",
      "\"trick\" -0.063\n",
      "\"spinnin\" -0.059\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "\"rescue\" -0.952\n",
      "\"intellectualism\" 0.202\n",
      "\"drivin\" -0.107\n",
      "\"ooh\" 0.090\n",
      "\"x6\" 0.086\n",
      "\"baby-baby-baby\" -0.084\n",
      "\"ask\" -0.052\n",
      "\"willy\" -0.045\n",
      "\"slave\" 0.040\n",
      "\"bicycle\" -0.039\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "\"bubble\" -0.872\n",
      "\"x6\" 0.276\n",
      "\"chiefin\" 0.197\n",
      "\"trick\" 0.143\n",
      "\"blaz-on\" 0.109\n",
      "\"intellectualism\" -0.106\n",
      "\"spinnin\" 0.084\n",
      "\"word\" -0.065\n",
      "\"ooh\" 0.064\n",
      "\"tamale\" 0.064\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printreadable(lsi_decade_topics,10)\n",
    "#lsi_decade_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LDA Over the Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 6031\n",
      "1980 6447\n",
      "1990 7795\n",
      "2000 10398\n",
      "2010 4801\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topicsobjects={}\n",
    "for dec in decades:\n",
    "    filename=\"../../data/conditioned/decades/\"+dec+\"/corpus\"+dec+\".p\"\n",
    "    f = open(filename,'r')  \n",
    "    corpus = pickle.load(f)\n",
    "    print dec,len(corpus)\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2w, num_topics=10, update_every=1, chunksize=100, passes=3)\n",
    "    topicsobjects[dec]=lda.print_topics(num_topics=ntopics,num_words=nwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the dict decadetopics was declared back when lsa was done on all years\n",
    "# it holds lsa for all years under key 1000.\n",
    "for dec in topicsobjects:\n",
    "    topicsdict={}\n",
    "    topicnumber=0\n",
    "    for t in topicsobjects[dec]:\n",
    "        topictuple= [d.split('*') for d in t.split(' + ')]\n",
    "        \n",
    "        topicsdict[topicnumber] = topictuple\n",
    "        topicnumber += 1\n",
    "    lda_decade_topics[int(dec)]=topicsdict\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 of 80 Topics for decade 1970:\n",
      "\n",
      "- - - -Topic 0- - - - \n",
      "operating 0.050\n",
      "hurtin 0.049\n",
      "jockin 0.037\n",
      "ahehe 0.031\n",
      "herrre 0.026\n",
      "physician 0.026\n",
      "grape 0.015\n",
      "blue 0.015\n",
      "wig 0.014\n",
      "heh 0.013\n",
      "fighter 0.013\n",
      "x11 0.013\n",
      "\n",
      "- - - -Topic 1- - - - \n",
      "nuttin 0.108\n",
      "smokescreen 0.063\n",
      "preacher 0.051\n",
      "reminisce 0.050\n",
      "share 0.030\n",
      "addition 0.020\n",
      "excitement 0.019\n",
      "forgiveness 0.019\n",
      "box 0.018\n",
      "wound 0.018\n",
      "hi-fi 0.016\n",
      "tombstone 0.012\n",
      "\n",
      "- - - -Topic 2- - - - \n",
      "macaroni 0.091\n",
      "amarretta 0.080\n",
      "wrapping 0.061\n",
      "ba-da 0.039\n",
      "trim 0.038\n",
      "buffalo 0.031\n",
      "veil 0.023\n",
      "gully 0.022\n",
      "trick-or-treat 0.021\n",
      "master 0.014\n",
      "baby-baby-baby 0.014\n",
      "guzzling 0.012\n",
      "\n",
      "- - - -Topic 3- - - - \n",
      "split 0.315\n",
      "necklace 0.091\n",
      "duct 0.084\n",
      "watch 0.026\n",
      "skeezer 0.018\n",
      "traffic 0.017\n",
      "windowpane 0.011\n",
      "upset 0.011\n",
      "windmill 0.009\n",
      "rockilla 0.008\n",
      "thinking 0.007\n",
      "meant 0.007\n",
      "\n",
      "- - - -Topic 4- - - - \n",
      "sound 0.385\n",
      "cologne 0.092\n",
      "hoss 0.082\n",
      "club 0.023\n",
      "hassle 0.022\n",
      "smyth 0.012\n",
      "picnic 0.011\n",
      "foreigner 0.008\n",
      "groovy 0.008\n",
      "a-rappin 0.007\n",
      "sweeties 0.007\n",
      "finisher 0.007\n",
      "\n",
      "- - - -Topic 5- - - - \n",
      "doobie 0.065\n",
      "radio 0.046\n",
      "dancey 0.041\n",
      "video 0.040\n",
      "hit 0.029\n",
      "slave 0.021\n",
      "eveeeeeeeeeeent 0.020\n",
      "skip 0.020\n",
      "spin 0.020\n",
      "floor-oor 0.020\n",
      "sweetness 0.016\n",
      "hotline 0.016\n",
      "\n",
      "- - - -Topic 6- - - - \n",
      "freek 0.120\n",
      "ridin 0.093\n",
      "knick-a-boxer 0.046\n",
      "shrug 0.037\n",
      "woa 0.023\n",
      "rope 0.022\n",
      "scheming 0.020\n",
      "huevos 0.017\n",
      "fortune 0.016\n",
      "harder 0.013\n",
      "tickle 0.012\n",
      "stewardess 0.009\n",
      "\n",
      "- - - -Topic 7- - - - \n",
      "passin 0.246\n",
      "tale 0.092\n",
      "carb 0.060\n",
      "choosin 0.044\n",
      "pon 0.040\n",
      "downtown 0.031\n",
      "kiffy 0.025\n",
      "intellectualism 0.021\n",
      "estilo 0.017\n",
      "black 0.016\n",
      "scam 0.010\n",
      "peer 0.010\n",
      "\n",
      "- - - -Topic 8- - - - \n",
      "cuff 0.073\n",
      "slippin 0.052\n",
      "karat 0.033\n",
      "kush 0.028\n",
      "nightshift 0.024\n",
      "wonderland 0.022\n",
      "past 0.021\n",
      "broncin 0.018\n",
      "selfs 0.017\n",
      "motha 0.016\n",
      "twirlin 0.015\n",
      "ya 0.015\n",
      "\n",
      "- - - -Topic 9- - - - \n",
      "susie 0.192\n",
      "bicycle 0.052\n",
      "stabber 0.050\n",
      "burger 0.043\n",
      "ankle 0.040\n",
      "steppin 0.029\n",
      "man,poor 0.020\n",
      "body 0.017\n",
      "leprechaun 0.016\n",
      "thurr 0.012\n",
      "excel 0.008\n",
      "episode 0.008\n",
      "\n",
      "- - - -Topic 10- - - - \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b89d34ff82a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprintreadable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_decade_topics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-f5b826422f4b>\u001b[0m in \u001b[0;36mprintreadable\u001b[1;34m(dectops, howmany, years)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\n- - - -Topic \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"- - - - \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhowmany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[1;32mprint\u001b[0m \u001b[0mdectops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdectops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m'-----------------------------------------------\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "\n",
    "printreadable(lda_decade_topics,nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the topic result sets are saved for processing and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#access example, LDA topic 3  for 1970:\n",
    "print lda_decade_topics[1970][3]\n",
    "# LSI is structured identically, of course results are different:\n",
    "print \"\\n\" ,lsi_decade_topics[1970][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"lda_decade_topics.json\",\"w\") as fd:\n",
    "    json.dump(lda_decade_topics, fd)\n",
    "    \n",
    "with open(\"lsi_decade_topics.json\",\"w\") as fd:\n",
    "    json.dump(lsi_decade_topics, fd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
